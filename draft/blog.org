#+hugo_base_dir: ../
#+hugo_selection: ./
#+hugo_auto_set_lastmod: t
#+options: author:nil

* 2019
** blogのお引越し                                                      :hugo:
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2019\" \"2019-10\")
:EXPORT_FILE_NAME: 201910030745
:EXPORT_DATE: 2019-10-03
:END:

これまでメインブログをはてなブログ、日々のメモをorgmodeからhugoとかやってたけど、
そもそもの更新頻度が低いのにバラバラになってるとさらに頻度が低くなってしまう。

どっちかに統一しようかと考え、最近の動向から長年連れそってきたはてなブログのほうをやめて、
こちらに持ってくるようにします。
昔のはてなブログの記事も、今でも有用なものはこっちに持ってこようかしら。
** emacs on WSLでSuper/Hyperキーを使う                            :emacs:wsl:
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2019\" \"2019-10\")
:EXPORT_FILE_NAME: 201910040445
:EXPORT_DATE: 2019-10-04
:END:
*** はじめに
Emacs勉強会(に限った話ではないが)だとMacユーザが多いので、私が普段使っている
Control/Meta以外に、Superキー/Hyperキーを割りあててる人もそこそこいる。
以前、Windowsの場合はwinキーがsuperキーになると聞いた覚えもあったのだけど、
うまくいかなかったのでWSLで使ってないキーにSuper/Hyperを割りあててデビューしようと調べてみた。
*** 環境
- WSL
- WSL上で動くEmacs
- X410(Xサーバ)

上記前提なので、 =Emacs for windows= とか、 =mingw= 上のemacsの話ではない。
これであれば、他の手段[fn:modifier]が使えるはず。
 
なお、Xサーバはやはり無償で利用できる =xming= とか =vcxsrv= とかが利用者多いと思うが、
emacsとの相性がよくない[fn:emacs-on-wsl]ので、有償のものを使っている。
=X410= は、ストアアプリとして購入でき、しょっちゅう割引きセールしているので、そのタイミングで買うのが吉。


*** 目指したゴール
無変換キーをSuperキー、変換キーをHyperキーとして使えるようにする。

本来は冒頭の通り、WindowsキーがSuperキーなのが普通だと思うのだけど、
なんかトラップされてうまくうごかなかったのと、Win+○のキーバインドも
結構使うので、それを潰してしまうのも惜しかったので、そのようにした。

前置きおわり。

*** Superキー/Hyperキーを設定する

**** 手段その1 AutoHotKeyを使う

     多分これが一番早いと思います。
     サンプルも多いはず。

     自分は、なんとなくWindowsのユーザランドでキーいじるやつが嫌だったので採用しなかった。
     (CapsとCtrlの入れかえで、結局レジストリいじらないとうまくいかなかったりというのに以前ぶつかったりしたので)

**** 手段その2 xmodmapを使う
=xmodmap= はlinuxというか、Xの世界でキーマップ変更するためのもの。
調べてみると最近は別のがあるっぽいけど、xmodmapでうまく動いたので気にしない。
     
     そうそうキーコードなんて変わらないと思うので、同じ設定で同じように動くはず。
     
***** 手順1. キーコードを特定する
       =xev= というコマンドを使うと、Xのアプリが起動して、そこでもろもろのイベントを標準出力に出して確認することができる。
       これを使うと、変換と無変換のキーコードは以下のとおり、変換が =129= 、無変換が =131= であることがわかる。

#+begin_verse
KeyRelease event, serial 33, synthetic NO, window 0x800001,
    root 0xf4, subw 0x0, time 426541921, (67,97), root:(179,232),
    state 0x0, keycode 131 (keysym 0xff22, Muhenkan), same_screen YES,
    XLookupString gives 0 bytes:
    XFilterEvent returns: False
    
KeyPress event, serial 33, synthetic NO, window 0x800001,
    root 0xf4, subw 0x0, time 426596187, (843,330), root:(955,465),
    state 0x0, keycode 129 (keysym 0xff23, Henkan_Mode), same_screen YES,
    XLookupString gives 0 bytes:
    XmbLookupString gives 0 bytes:
    XFilterEvent returns: False
#+end_verse

***** 手順2. xmodmapの動作確認する
      キーコードがわかったら、=xmodmap= コマンドで一時的にキーシムを書きかえて動作確認してみる。
      
#+begin_example
# xmodmap -e "keycode 131 = Super_L Super_L"
# xmodmap -e "keycode 129 = Hyper_L Hyper_L"
#+end_example

実行後、再度xevを使って期待通り変わっていることを確認する。

#+begin_verse
KeyPress event, serial 33, synthetic NO, window 0x800001,
    root 0xf4, subw 0x0, time 426819890, (125,123), root:(263,284),
    state 0x0, keycode 131 (keysym 0xffeb, Super_L), same_screen YES,
    XKeysymToKeycode returns keycode: 115
    XLookupString gives 0 bytes:
    XmbLookupString gives 0 bytes:
    XFilterEvent returns: False
#+end_verse

先程はキーコード131が無変換だったのが、Superに変わっていることがわかる。


***** 手順3. xmodmapの設定を作成する
       調べると、 ~xmodmap -pke~ を実行して、必要なところだけ書きかえましょう。というのが出てくるのだが、実際のところ必要な設定だけ書けばよかったので、いきなり =.Xmodmap= ファイルを作成する。
       WSLで手持ちのXサーバ使う分には別にファイル名は何でもいいと思うのだが、ここは慣例に従っておく。(なお、 =startx= コマンドでXを起動するときは、雛形で =.Xmodmap=を読み込むのでファイル名重要)

       自分の設定は、こんなかんじ。

       https://github.com/grugrut/dotfiles/blob/master/.Xmodmap

#+begin_src
clear  mod3
clear  mod4
!<muhenkan>
keycode 129 = Hyper_L Hyper_L Hyper_L Hyper_L
!<henkan>
keycode 131 = Super_L Super_L Super_L Super_L
add    mod3 = Hyper_L
add    mod4 = Super_L Super_R
#+end_src
       デフォルトの状態だと、SuperキーとHyperキーが同じ修飾キーとしてあつかわれていて、Hyperキー単体でうまくうけとれないので使われていないmod3にHyperキーを割当ておいた。

       
***** 手順4. 自動で適用されるようにする
       =xmodmap ~/.Xmodmap= とコマンド実行すればよいのだけど、注意点が一つ。
       xmodmapはXサーバに対して設定をおこなうコマンドなので、Xサーバが起動していない状態ではうまく動かない。
       LinuxやBSD使ってるときにもxmodmap使ってたけど、当時は常にxorg-serverが起動してたので今回はじめてそのこと知った。

       ついでなので、OSログイン時にXサーバを起動すべく適当なbatを作成した。
       https://github.com/grugrut/dotfiles/blob/master/_windows/startx.bat

#+begin_src bat
start /b x410.exe

ubuntu.exe run "DISPLAY=127.0.0.1:0.0 xmodmap ~/.Xmodmap"
#+end_src

       x410.exeが使っているXサーバ。バックグラウンド実行させたいので、 =/b= オプションをつけてる。
       そして、 ~ubuntu.exe run~  をすることでwslでコマンド実行できる。

これを Windowsの =shell:startup= に配置することでスタートアップ時に自動実行することができる。
       ただし、直接おくよりもショートカットを配置することをおすすめする。直接配置すると、実行時にコマンドプロンプトが一瞬表示されてうっとうしいが、ショートカットであれば最小化して実行することができるので気にならないからだ。

       
****  まとめ
     xmodmapを使うことで、他のアプリには影響なくwslのX使うアプリだけにいろいろ手をいれられることが確認できた。他にもその手の機能で便利なのありそう。

[fn:modifier] たぶん =w32-lwindow-modifier= とかが使えるはず
[fn:emacs-on-wsl] https://speakerdeck.com/grugrut/emacs-on-wsldefalsekun-rigoto
** DONE skk-azikで「っ」が誤爆しやすい問題                        :emacs:skk:
   CLOSED: [2019-10-22 Tue 22:05]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2019\" \"2019-10\")
:EXPORT_FILE_NAME: 201910222100
:END:

skk48に名乗りを上げたとおり、普段は =skk= を使っているのだけれども、
私は単なるskkではなく、 =azik= を使っている。

azikは簡単に言うと、日本語ローマ字入力(qwertyを想定)に特化したskkの拡張で、
以下のような便利なマッピングになっている。
- 長音(ー)は小指が遠いので「:」で入力できる
- 促音も二回重ねるのが面倒なので「;」で入力できる
- 日本語は「子音+an(晩餐はb + an, s + anとか)」、「子音+in(新品はs + in, p + in)」のように =子音 + 母音 + n= からなる語が多く含まれるので、「子音 + zで子音+az」「子音 + kで子音+in」といった感じに、少ないキータイプで入力することが可能になる。(なので、 =az= =ik= で =azik= )
- さらに拡張で「 =ds= で =です= 」「 =ms= で =ます= 」のように、さらに簡易にするマッピングもされている(自分はあまりこれは使ってない)

詳細については、公式サイトを見てほしい。
[[http://hp.vector.co.jp/authors/VA002116/azik/azikinfo.html]]

ところで、われらがemacsの =ddskk= にもazik用の設定が搭載されているのだが、
なぜかそのマッピングの中に =tU= が、 =っ= にわりあてられている。
そのため、よく入力中に、意図せず =っ= が入力されてしまう問題が発生していた。

例えば「疲れた」や「積む」のような「つ」から始まる感じを入力しようとして、「▽っかれた」のように頭が =つ= ではなく =っ= になってしまう人がいたら同じ症状だと思う。おそらく意識せず =Tu= と打とうとして、 =TU= とか =tU= と入力しているはず。

いろいろ試して以下の設定で改善することが確認できた。
私も長年、そもそも何がおきているかわからずに困っていたのだけれど、もし同様に困っている人いたら参考になれば幸いである。

#+begin_src lisp
(leaf ddskk
  :straight t
  :bind
  (("C-x C-j" . skk-mode)
   ("C-x j"   . skk-mode))
  :init
  (defvar dired-bind-jump nil)  ; dired-xがC-xC-jを奪うので対処しておく
  :custom
  (skk-use-azik . t)                     ; AZIKを使用する
  (skk-azik-keyboard-type . 'jp106)      ;
  :hook
  (skk-azik-load-hook . my/skk-azik-disable-tU)
  :preface
  (defun my/skk-azik-disable-tU ()
    "ddskkのazikモードが`tU'を`つ'として扱うのを抑制する."
    (setq skk-rule-tree (skk-compile-rule-list
                         skk-rom-kana-base-rule-list
                         (skk-del-alist "tU" skk-rom-kana-rule-list)))))
#+end_src
*** 内容の解説

基本的にドキュメントを読む限り、 =skk-rom-kana-rule-list= にユーザ独自の設定は入れるので、
そこから消せばよいはずなのだが、再コンパイルしないとだめだったのでそのようにしている。
ちなみに、 =skk-del-alist= は =skk-rom-kana-rule-list= から不要なのを削除するための便利な関数である。追加したい場合は普通に =append= すればよい。

参考: [[http://mail.ring.gr.jp/skk/200106/msg00009.html]]

また、普通なら =leaf= なり =use-package= なりの =:config= ブロックに設定すればよいのだけど、
ロード後の処理の影響からかazikの設定に上書きされてしまっているように見えたので、
skk-azikの中で最後に呼ばれる =skk-azik-load-hook= を使って、自前のルール修正関数を呼ぶようにしている。

printデバッグしてみたら、 =:config= がそもそも呼ばれてなかったようにも見えたので
もうすこし上手いやりかたがあるのかもしれない。

** DONE leaf-expandでleafのデバッグをする            :emacs:smartparens:leaf:
   CLOSED: [2019-10-14 Mon 22:18]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2019\" \"2019-10\")
:EXPORT_FILE_NAME: 201910141028
:END:

設定しているsmartparensが期待通りに動かずに困っていた。
具体的には、lispを編集するモード(emacs-lisp-modeとか)のときに、「'(シングルクオート)」がダブルクオートとかと同じく「''」となってしまいかえって面倒なことに。

beforeの設定がこんな感じ。
最近は、 =use-package= のかわりに =leaf= を使っているが、use-packageでもだいたい同じだと思う。

#+begin_src lisp
(leaf smartparens
  :straight t
  :require t
  :diminish smartparens-mode
  :config
  (leaf smartparens-config
    :require t
    :after smartparens
    :hook
    (prog-mode-hook . smartparens-mode)))
#+end_src

smartparensの設定は、 =(require 'smartparens-config)= が楽だし確実、というのを見て、たしかにそのように設定してるんだけどなあ。。。って感じだった。

いろいろ見た結果、期待通りにrequireできてないんじゃないの？って結論に至るのだけど、そういったときのデバッグって大変ですよね。

そんな時に便利なのが、 =M-x leaf-expand= で、これはleafマクロで書かれた箇所を展開するとどうなるかがその場でわかる。
それにより、この展開後が

#+begin_src lisp
(prog1 'smartparens-config
  (autoload #'smartparens-mode "smartparens-config" nil t)
  (eval-after-load 'smartparens
    '(progn
       (add-hook 'prog-mode-hook #'smartparens-mode)
       (eval-after-load 'smartparens-config
         '(progn
            (require 'smartparens-config))))))
#+end_src

であることがわかり、これってちゃんとrequireされないよね、ということがわかった。

ちなみに解決後の設定は以下な感じ。

#+begin_src lisp
(leaf smartparens
  :straight t
  :require smartparens-config
  :diminish smartparens-mode
  :hook
  (prog-mode-hook . turn-on-smartparens-mode)
  :config
  (show-smartparens-global-mode t))
#+end_src

これだと展開後の姿も、以下のようになり、とても綺麗(なのか？)。

#+begin_src lisp
(prog1 'smartparens
  (autoload #'turn-on-smartparens-mode "smartparens" nil t)
  (straight-use-package 'smartparens)
  (add-hook 'prog-mode-hook #'turn-on-smartparens-mode)
  (eval-after-load 'smartparens
    '(progn
       (require 'smartparens-config)
       (show-smartparens-global-mode t)
       (diminish 'smartparens-mode))))
#+end_src

これは便利なので、今後も積極的に使っていきたい。
** DONE CodeReady ContainersでWindows上にOpenShift環境を構築する :openshift:kubernetes:crc:
   CLOSED: [2019-10-19 Sat 14:03]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2019\" \"2019-10\")
:EXPORT_FILE_NAME: 201910191042
:END:
OpenShift4.2がリリースされたので、家で使ってみようと、 =CodeReady Containers(crc)= をインストールしてみた。
CodeReady Containersは、これまで =minishift= という名前のプロダクトだったものが、OpenShift 4.xになって名前が変わったもので、
テストとか開発とかに使えるものである。

10/17にgithub上では1.0.0のタグが切られていたが、まだpre-releaseのようだ。
Red Hat Developers Programに登録していれば、Developer Preview版が利用できるようだ。
*** 導入した環境
- Windows 10 Professional
- メモリ 64 GB (メモリはわりと食うので少ないとつらいと思う)

*** ダウンロード
ここからリンクを辿っていくとダウンロードできる。

https://developers.redhat.com/products/codeready-containers

OSごとのバイナリと、インストール時に入力が必要なpull secretをダウンロードしておく。
2GBぐらいあり、わりと重たい。
*** 起動

基本ドキュメント通りにやればよいはず。

1. ダウンロードしたファイルを展開し、 =crc= バイナリをパスの通った場所に配置する
2. 仮想マシンを作成する

   たぶん場合によってはHyper-Vのネットワークが作成されたりするはず。
   #+begin_src bat
λ crc setup
INFO Checking if running as normal user
INFO Caching oc binary
INFO Unpacking bundle from the CRC binary
INFO Check Windows 10 release
INFO Hyper-V installed
INFO Is user a member of the Hyper-V Administrators group
INFO Does the Hyper-V virtual switch exist
Setup is complete, you can now run 'crc start' to start a CodeReady Containers instance
#+end_src
3. 起動する

   起動時にデフォルトではメモリを8GBで起動するが、何かやるには到底足りないので、16GBぐらいは指定しておきたい。
   また、DNSサーバを指定しておかないと、他の仮想マシンを動かしてたり仮想ネットワークが複数あったりした場合に、
   うまく名前解決できないケースがあったので指定しておくのが吉。
   #+begin_src bat
λ crc start -m 16384 -n 8.8.8.8
INFO Checking if running as normal user
INFO Checking if oc binary is cached
INFO Check Windows 10 release
INFO Hyper-V installed and operational
INFO Is user a member of the Hyper-V Administrators group
INFO Does the Hyper-V virtual switch exist
#+end_src
4. pull secretを入力する

   初回起動時には、pull secretの入力を求められるのでバイナリと一緒にダウンロードしておいたjsonから情報を貼り付ける。
   #+begin_src bat
? Image pull secret [? for help] **********************************
INFO Loading bundle: crc_hyperv_4.2.0-0.nightly-2019-09-26-192831.crcbundle ...
INFO Creating CodeReady Containers VM for OpenShift 4.2.0-0.nightly-2019-09-26-192831...
INFO Verifying validity of the cluster certificates ...
INFO Adding 8.8.8.8 as nameserver to Instance ...
INFO Will run as admin: add dns server address to interface vEthernet (Default Switch)
INFO Check internal and public dns query ...
INFO Copying kubeconfig file to instance dir ...
INFO Adding user's pull secret and cluster ID ...
INFO Starting OpenShift cluster ... [waiting 3m]
INFO
INFO To access the cluster, first set up your environment by following 'crc oc-env' instructions
INFO Then you can access it by running 'oc login -u developer -p developer https://api.crc.testing:6443'
INFO To login as an admin, username is 'kubeadmin' and password is XXXXX-XXXXX-XXXXX-XXXXX
INFO
INFO You can now run 'crc console' and use these credentials to access the OpenShift web console
CodeReady Containers instance is running
#+end_src

   インストール直後は、一般ユーザである =developer= ユーザ(パスワードはdeveloper)と、
   管理者ユーザである =kubeadmin= ユーザの2種類のユーザが存在する。kubeadminユーザのパスワードは起動時に表示されるのでそれを見ておく。
*** ログイン
ログインは、CLIとWebコンソールのふたつがある。
**** CLI ログイン
OpenShiftでは、kubernetesでいうところの =kubectl= に相当する、 =oc= コマンドが存在する。
crcにもocコマンドは同梱されているので、以下のコマンドでパスの通しかたがわかる。

#+begin_src bat
λ crc oc-env
SET PATH=C:\Users\grugrut\.crc\bin;%PATH%
REM Run this command to configure your shell:
REM     @FOR /f "tokens=*" %i IN ('crc oc-env') DO @call %i %i
#+end_src

ocコマンドの場所にパスを通したら、あとはログインするだけである。

#+begin_src ba
λ oc login
Authentication required for https://api.crc.testing:6443 (openshift)
Username: kubeadmin
Password:
Login successful.

You have access to 51 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
#+end_src
**** Webコンソールログイン
OpenShiftには、はじめからブラウザ経由でアクセスできるWebコンソールが用意されているので、
そちらを使うことも多いだろう。
=crc console= コマンドを実行することで、ブラウザが起動し、Webコンソールにアクセスできる。

オレオレ証明書なので、そこは目をつぶってそのまま接続するとログイン画面が出てくる。

[[file:images/20191019-crc-login.png]]

kubeadminユーザでログインするときは、 =kube:admin= を、developerユーザでログインするときは、 =htpasswd_provider= を選択する。

ログインに成功すると、ダッシュボードが表示されるはずだ。

[[file:images/20191019-crc-dashboard.png]]
*** 初期設定

ここまでですぐにOpenShiftが使える状態ではあるが、ベータ版でさわってみてた感じ、以下の設定はやっておいたほうがよさそう。
- 監視機能の有効化
- ユーザの作成
**** 監視機能の有効化
ダッシュボードでクラスタのリソース状況が見れたり、Podの状況が見れる枠はあるものの、
デフォルトでは監視機能が無効化されているため、まったく意味をなしていない。

そこで、監視を有効化して、情報を収集できるようにしておく。

方法はドキュメントに書いてあるとおりで、以下のコマンドを順にCLIで実行すればよい。
ドキュメントだと、セミコロン区切りでまとめて書いてあるが、windowsの場合はセミコロンで複数コマンドを順番に実行する
ことができないので、ひとつずつ分割して実行する。

#+begin_src bat
λ oc scale --replicas=1 statefulset --all -n openshift-monitoring
statefulset.apps/alertmanager-main scaled
statefulset.apps/prometheus-k8s scaled

λ oc scale --replicas=1 deployment --all -n openshift-monitoring
deployment.extensions/cluster-monitoring-operator scaled
deployment.extensions/grafana scaled
deployment.extensions/kube-state-metrics scaled
deployment.extensions/openshift-state-metrics scaled
deployment.extensions/prometheus-adapter scaled
deployment.extensions/prometheus-operator scaled
deployment.extensions/telemeter-client scaled
#+end_src

しばらくすると、ダッシュボードに収集した値が表示されるようになるだろう。
ちなみに監視機能は結構メモリを消費するので、デフォルトの8GBだとメモリが足りなくて必要なPodを起動できず動かない問題が確認できている。
**** ユーザの追加
kubeadminユーザでWebコンソールにログインすると上の方で警告画面がでているところからもわかるとおり、
kubeadminユーザは一時的なユーザらしく、あまりこれを使うのは好ましくないらしい。
まあパスワードも覚えにくいし、適当に自分で作ったほうがよいだろう。

ログイン手段の作成方法もいくつかあるが、デフォルトで用意されているdeveloperユーザ用の
htpasswdに自分用のユーザを作成するのが楽だろう。

https://console-openshift-console.apps-crc.testing/k8s/ns/openshift-config/secrets/htpass-secret

にアクセスすると(もしくは左のメニューの =Workloads= の中の =Secrets= から、 =htpass-secret= を探すのもよい)、
ログイン用のhtpasswdが書かれたsecretの設定を見ることができる。
ここから、右上の =Actions= から =Edit Secret= を選択する。

htpasswdの設定を作成する方法はいくつかあるが、たとえば =WSL= 等のLinux環境がある場合は、
htpasswdコマンドを使えば簡単に作成できる。
今回は、私用に、grugrutユーザを作っている。
#+begin_src bash
$ sudo apt install apache2-utils
$ htpasswd -n grugrut
New password:
Re-type new password:
grugrut:XXXXXXXXXXXXXXXXXXXXXXXXXX
#+end_src

これで =oc login= してみると、作成したユーザでログインできるはず。
だが、これだけだと何もできないただログインできるだけのユーザなので、
クラスタ管理者の権限である =cluster-admin= ロールをバインドする。

kubeadminユーザで
https://console-openshift-console.apps-crc.testing/k8s/all-namespaces/rolebindings
にアクセスし、 =Create Binding= ボタンをクリック。

- =Binding Type= は、 =Cluster-wide Role Binding= を選択
- =Name= は、名前がかぶると怒られるので、 =cluster-admin-(作成するユーザ名)= とでもすればよい
- =Role Name= は、 =cluster-admin= を選択
- =Subject= は、 =User= を選び、先程作成したユーザ名を入れる

これで作成すれば、晴れてクラスタ管理者となれる。
ちなみにコマンドだと、
=oc adm policy add-cluster-role-to-user cluster-admin (ユーザ名)=
であり、もしかしたらこっちのほうが楽かもしれない。
***** kubeadminユーザの削除
自分自身をクラスタ管理者にしたら、もはやkubeadminユーザは不要なので消してしまってもよいはず。
ドキュメントにも消しかた書いてあるし。

https://docs.openshift.com/container-platform/4.2/authentication/remove-kubeadmin.html

kubeadminユーザを削除することによって、Webコンソールへのログイン時に、「kube:admin」か「htpasswd」なのか
選ばなくてよくなるので、ユーザを作ったあとは消してしまってよいかもしれない。
*** まとめ
これで家の環境でOpenShiftが使えるようになったので、今後コンテナ動かすところなども見ていきたい。
*** 注意点
今のところバグで、30日で証明書が期限切れになり、起動できなくなってしまうらしい。
解決策はなく、一度削除して(当然作成したものも消える)、作りなおす必要があるとか。
おそろしい話である。
** DONE emacs26からの行番号表示がddskk入力時にガタつくのを防ぐ    :emacs:skk:
   CLOSED: [2019-10-20 Sun 23:51]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2019\" \"2019-10\")
:EXPORT_FILE_NAME: 201910202227
:END:
Emacs26までは、行番号の表示をemacs lispで頑張るというアプローチがために、
重たくてなかなか使いどころに困る問題があった。
それに対してEmacs26では、待望のCで書かれた組み込みの行番号表示である、 =display-line-numbers-mode= が導入された。
これは軽くてたいへん便利なのであるが、使っていて、ひとつめちゃくちゃ気になる問題があった。

それはごらんの通り、ddskkで日本語を入力するときに行番号の表示がずれて、がたがたとなり見辛いのである。


[[file:images/display-line-numbers-mode-gatagata.gif]]

これには困っていたのだけど、言語化しづらいところもあり解決策が見付けられなかったが、
ソースコード見てパラメータいじってたら以下のパラメータを有効化することで
がたつかなくなることがわかった。

#+begin_src lisp
(global-display-line-numbers-mode t)
(custom-set-variables '(display-line-numbers-width-start t))
#+end_src

先程のgifと見比べてみると今度はまったくがたつきがないのがわかる。

[[file:images/display-line-numbers-mode-not-gatagata.gif]]

今のところ、この設定で困ったことはなく、強いて言えば、
見比べてみるとわかるが、はじめから行番号のスペースが広めにとられてしまっている。
そのため、少し画面サイズは小さくなっているものの、これで快適な行番号生活をおくることができる。
** DONE Google Cloud Certified - Professional Cloud Architect 認定試験に合格した :gcp:
   CLOSED: [2019-12-29 Sun 10:11]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2019\" \"2019-12\")
:EXPORT_FILE_NAME: 201912290846
:END:
タイトルの通りですが、12月の中旬ぐらいにGCPのProfessional Cloud Architectの試験を受け、無事に一発で合格しました。

パブリッククラウドは業務で若干使うぐらいで、どちらかというと試験を通じてクラウドについて学ぼうというのがモチベーションでした。
AWSとかAzureの試験も受けたことなく体験記を見る限り、GCPは単にGCPの使い方がわかればよいというより、
デプロイ戦略とかkubernetesの使い方とか、そういう一般的知識も求められる(後述のcouseraでGCPの人もそう言ってた)。
そちらについてはけっこうケイパビリティあるつもりなので、本当にGCPのサービスについてきちんとおさえてから挑みました。

勉強期間としては2週間ぐらい。
基本的には、courseraの公式の教材で学習しました。

[[https://www.coursera.org/learn/preparing-cloud-professional-cloud-architect-exam-jp][Preparing for the Google Cloud Professional Cloud Architect Exam 日本語版]]

試験勉強を通じて、これまで使ってなかったパブクラの機能もいろいろとわかってきたので、
今後も公私ともにもっと活用していきたいですね。

* 2020
** DONE org modeのファイルをパースする                            :emacs:org:
   CLOSED: [2020-01-10 Fri 08:56]
   :PROPERTIES:
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-01\")
   :EXPORT_FILE_NAME: 202001100849
   :END:

   やりたいことがあって、inbox.orgをパースして、個々のノードの情報を得たかった。
   ざっと以下のコマンドでいけることがわかった。
   とりあえず動作確認は、 =M-:= でさくっと確認しただけだけど。

   #+begin_src 
(org-map-entries (lambda() (princ (org-entry-properties))))
   #+end_src

   =org-map-entries= が、条件にあうノードに対してmap関数を適用するための関数。
   =org-entry-properties= が、個々のノードのプロパティの連想リストを取得する関数。てっきりプロパティドロワーにあるものだけ抽出するのかと思ってたらアイテム名とか、TODO状態とか全部取れてるっぽいので、これベースにごにょごにょすればいい感じにいけそう。
** DONE ergodox ezを購入した                                        :ergodox:
   CLOSED: [2020-05-18 Mon 23:11]
   :PROPERTIES:
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-05\")
   :EXPORT_FILE_NAME: 202005182209
   :END:
   今さらながら、分割キーボード界ではおなじみの、ergodox ezを購入してみた。
   動機としては、自分も昨今の事情でテレワークしていて、
   家ではHappy Hacking Proを使ってたのだけど、背中がつらくなってきたのと、
   もろもろオフとか出て自作キーボードに興味があったから。
*** Ergodox ezにした理由
    自分が望んでいるものは何か考えたところ、以下だったので、レデイーメイドなErgodox EZにまずはチャレンジすることにした。

    - JIS配列を愛用してるので、ある程度キー数が多いこと
    - emacsでハイパーキー、super キーを使ってるので親指でmodifier keyをいろいろ使えること
    - 分割キーボードであること
    - いったんは、はんだ付けなしで沼への第一歩をふみだせること

    ちなみに、職場では、今は亡き、Barrocoの日本語配列を使っている。
    [[https://www.archisite.co.jp/products/mistel/barocco-jp/]]

    この子もいいこだし、マクロでいろいろできるのは判ってるけど、やっぱりかゆいところに手が届かないのがつらかった。

    正直、今回のは、自分が今後沼れるのかどうか、試金石的な要素がつよいかも。

*** 購入方法
    特に既存のググった結果と変わらないので割愛。
    5/3に注文して、5/18に受け取ったので、賞味2週間でうけとってる。思った以上に早いね。
    ちなみに、注文したモデルは白色・無刻印。軸は赤軸にしてみました。
    そのうち、キーキャップを別途購入して、よりオシャンティーにしていきたいですね。

    [[file:images/20200518_ergodox_0.jpg]]

    ポインティングデバイスは、人差し指トラックボールを使ってるので、配置はこんな感じにしてみました。

    [[file:images/20200518_ergodox_1.jpg]]

*** キー配列

    先述のとおり、普段からJIS配列を愛用していて、記号の位置など、できるだけ踏襲したかったので、
    それ用にキーマップを書いた。

    https://github.com/grugrut/qmk_firmware/blob/b639d036d4c76b0d9b71a431dd92a8a69a0fd234/keyboards/ergodox_ez/keymaps/grugrut/keymap.c

    基本的には、まずは、kinesisキーボードの日本語配列をベースとしている。

    届いて真っ先に、この自分でビルドしたHexファイルを焼き込もうとしたのだけど、
    qmk toolboxだとリセット後のデバイスを認識できずにビビるなどした。
    その後、teensyに切り替えたら、普通に焼き込めて一安心。

*** 一時間程度さわってみての所感
    - Colomn Stuggered配列に慣れない。特に一段目のキーのタイポが多い

    - 親指の修飾キーの奥の方が意外と押しにくい
      - 自分、そんなに手も小さいほうじゃないので、いけると思ってたら、意外とつらかった

    まあ、これは触りながら、適宜キー配列を変えていって慣らすしかなさそうですね。
** DONE CKA(Certified kubernetes Administrator)に合格した        :kubernetes:
   CLOSED: [2020-07-07 Tue 09:40]
   :PROPERTIES:
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-07\")
   :EXPORT_FILE_NAME: 202007070839

   :END:
   Kubernetesの管理者向け資格であるCertified Kubernets Administratorを受験して
   無事に合格したので、合格体験記はすでに巷にいくらでもあるが、せっかくなのでメモ。
*** CKAとは
    Linux Foundationが管理している、kubernetesの認定試験。
    Kubernetesの操作やkubernetes自体の管理について問われる。
    よくある4択問題のようなテストではなく、実際にkubernetesクラスタを操作する実技試験。

    試験時間は3時間で、24問。問題によって得点は異なり、74%以上で合格。

*** バックグラウンド
    kubernetes歴は15ヶ月ぐらい。うち、ほとんどはOpenShiftだったので、
    純粋にkubernetesを触っているのは、半年ぐらいか。
    自宅で、kubeadmを使って仮想マシンだったりラズパイおうちクラスタだったり作ってたので、
    k8sのインストールは慣れてた。
*** 試験に役立ったもの
**** Udemyのコース
     他の人の結果を見て、以下のUdemyのコースがよさそうだったので、こちらでやった。
     これ書いてる今もそうだけど、しょっちゅうセールしてて、元の価格はなんなんだ。。。ってなりがち。

     [[https://px.a8.net/svt/ejp?a8mat=3BHUM0+C673K2+3L4M+BW8O2&a8ejpredirect=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fcertified-kubernetes-administrator-with-practice-tests%2F][Certified Kubernetes Administrator (CKA) with Practice Tests]]

     動画と演習がセットになってて、最初はマジメに動画を見てたけど、途中で飽きてしまったので演習だけやったようなもん。

**** Ergodox EZ
     試験問題では、 abcってPodを作ってください、みたいな問題が出てくる。当然、確実に作成するためにコピペしたいのだけど、
     試験はブラウザ上のアプリ(katacodaとかCloudShell的な)でおこなう。
     ブラウザなので、コピペは =Ctrl-C/Ctrl-V= ではできない。Windowsの場合は、 =Ctrl-Insert/Shift-Insert= でおこなう。
     正直、Insertキーなんて普通のキーボードでは使い勝手の良いところにないと思う。
     自分は、 [[ergodox ezを購入した]] の通り、Ergodox EZを使っていたので、Insertキーを =Lower-I= にバインドしていたので
     手をホームポジションから移すことなく、スムーズにペーストすることができて、自作キーボード万歳!って思った。

     そうは言っても、そもそもペーストが、 =Shift-Insert= ってことに慣れてないので一週間ぐらいは、普段から意識して
     ペーストをこちらのキーバインドでおこなうようにしていた。
     今回初めて知ったのだけど、これ、別に特殊なキーバインドじゃなくて、他のWindowsアプリでもこれでペーストできるのね。

*** 試験
    体験記を見ると、貸し会議室で受験した人が多かったけど
    - 貸し会議室のWifiの品質やポートブロックが心配だった
    - ノートPCの小さいディスプレイで頑張れる自信がなかった
    - そもそも、最近ノートPCの調子が悪くトラブルが怖かった
    などの理由により、自宅で受けることにした。

    机の横に本棚があるので心配だったが、受験サイトでチャットができ、問題ないか聞いてみたところ
    「大丈夫だけど、もしかしたら布でかくせって言われるかもね〜」とのことだったので、
    事前に布をかけておいた。当日はなにも言われなかったので多分それでよいのでしょう。

    ちなみに、数々の合格体験記ではGoogle翻訳プラグインはOKだったって書かれてたけど
    自分の場合はダメって言われてしまった。

*** 結果
    93%だった。一応全問問いたものの、7%の問題だけ挙動が怪しかったので、たぶんそれのやりかたが間違ってたのだと思う。
    部屋の綺麗さを保ててるうちに、CKADも取ってしまいたいので、さっそく今日から勉強再開だ。
** DONE CKAD(Certified Kubernetes Application Developer)に合格した :kubernetes:
   CLOSED: [2020-07-11 Sat 10:56]
   :PROPERTIES:
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-07\")
   :EXPORT_FILE_NAME: 202007111027
   :END:

   [[CKA(Certified kubernetes Administrator)に合格した]] の勢いで、4日後にCKADも受験し、
   無事に合格したのでメモ

*** CKADとは
    Linux Foundationが管理している、kubernetesの認定試験。
    CKAと異なり、kubernetesの操作のみでkubernetesの管理については問われない。
    よくある4択問題のようなテストではなく、実際にkubernetesクラスタを操作する実技試験。

    試験時間は2時間で、19問。問題によって得点は異なり、66%以上で合格。

*** 試験準備
    CKAの試験対策でUdemyの講座がよかったので、Udemyのコースで勉強した。

    [[https://px.a8.net/svt/ejp?a8mat=3BHUM0+C6720I+3L4M+BW8O2&a8ejpredirect=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fcertified-kubernetes-application-developer%2F][Kubernetes Certified Application Developer (CKAD) with Tests]]

    CKAの勉強をしていれば、CKAD用の準備はいらないと聞いていたので、最後のLightning TestとMock Examをメインでやった。
    Lightningの方の問題が時間がかかるものが多く、1問5分で解くって無理っしょ、、、と思ったあとにMock Examは簡単だったのでほっとした。

    CKAから中3日での登板なので、できたことといえばこんなもん。

    あとは、試験に耐えれる室内環境を維持するため、エントロピーの低い暮らしをこころがけた(笑)

*** 試験受けてみて  
    CKADのほうが難しいと感じた。試験に合格するという観点で言ったらCKADの方が求められる点数が低いので合格しやすいと思うが、
    問題の最大難易度はCKADの方が難しい。試験準備のとおり、結構余裕かまして受験に臨んだので、1問目がMAX難しい問題で結構あせった。
    より正確には、難しいというか制限時間の割に必要な設定数が多い問題が多かった。

    また、CKAに比べて日本語がこなれてない(というか破綻してる)ものがいくつかあり、
    英語と見比べながら問われてることを理解する必要もあり、そこでも時間がとられてしまった。

    結局ひととおり解くのに90分ぐらいかかってしまい、30分しか見直しの時間がとれず、見直し途中でタイムオーバーに。

*** 結果
    96%だった。おそらく何聞かれてるんだか明確でない問題が1問あり、ま、こんなもんだろで回答したものが1つあったので、それだと思う。
    CKA、CKAD両方受けてみて、これまでの知識の棚卸しができてよかったと思う。
    これ取ったから何というわけではないので、これをステップにより知識を高めていきましょう。
** DONE CRI-O + Kata containers + Weavenetでkubernetesをインストールする :kubernetes:
   CLOSED: [2020-07-12 Sun 09:33]
   :PROPERTIES:
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-07\")
   :EXPORT_FILE_NAME: 202007111344
   :END:
普段はCRIはDocker、OCIはrunc、CNIはcalicoで構成することが多いのだけど、たまには違う構成でもとってみようと思いインストールしてみる。
特にこれまでKata containersはさわったことなかったので。
OSはUbuntuを適当に入れた

*** Kataのインストール
https://github.com/kata-containers/documentation/blob/master/install/ubuntu-installation-guide.md

#+begin_src bash
ARCH=$(arch)
BRANCH="${BRANCH:-master}"
sudo sh -c "echo 'deb http://download.opensuse.org/repositories/home:/katacontainers:/releases:/${ARCH}:/${BRANCH}/xUbuntu_$(lsb_release -rs)/ /' > /etc/apt/sources.list.d/kata-containers.list"
curl -sL  http://download.opensuse.org/repositories/home:/katacontainers:/releases:/${ARCH}:/${BRANCH}/xUbuntu_$(lsb_release -rs)/Release.key | sudo apt-key add -
sudo -E apt-get update
sudo -E apt-get -y install kata-runtime kata-proxy kata-shim
#+end_src

*** CRI-Oのインストール
https://github.com/cri-o/cri-o#installing-cri-o

#+begin_src bash
. /etc/os-release
sudo sh -c "echo 'deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/x${NAME}_${VERSION_ID}/ /' > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list"
wget -nv https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/x${NAME}_${VERSION_ID}/Release.key -O- | sudo apt-key add -

sudo apt-get update -qq
apt-get install -y cri-o-1.17
sudo systemctl enable crio
#+end_src

Ubuntuのパッケージは、1.18がまだ無いようなので1.17を利用した。

*** CRI-Oのランタイムの設定
https://github.com/kata-containers/documentation/blob/master/how-to/run-kata-with-k8s.md#cri-o

/etc/crio/crio.conf に書かれている設定を入れた。
デフォルトはruncのままにしてある。

#+begin_src 
[crio.runtime.runtimes.kata-runtime]
  runtime_path = "/usr/bin/kata-runtime"
  runtime_type = "oci"
#+end_src

*** kubernetesのインストール

kubeadmでインストール。

全ノードで
#+begin_src bash
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
sudo sysctl --system

sudo apt-get update && sudo apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update
sudo apt-get install -y kubelet=1.17.0-00 kubeadm=1.17.0-00 kubectl=1.17.0-00
sudo apt-mark hold kubelet kubeadm kubectl

cat <<EOF | sudo tee /etc/systemd/system/kubelet.service.d/0-crio.conf
[Service]
Environment="KUBELET_EXTRA_ARGS=--container-runtime=remote --cgroup-driver=systemd --runtime-request-timeout=15m --container-runtime-endpoint=unix:///var/run/crio/crio.sock"
EOF
sudo systemctl daemon-reload
sudo systemctl restart kubelet
#+end_src

コントロールプレーンで以下を実行。
#+begin_src bash
sudo kubeadm init --skip-preflight-checks --cri-socket /var/run/crio/crio.sock --pod-network-cidr=10.244.0.0/16
#+end_src

実行後には、joinコマンドが表示されるので、今度はそれを各ノードで実行する。もし、見逃してしまった場合は、以下のコマンドで再表示できる。

#+begin_src bash
kubeadm token create --print-join-command
#+end_src

前に入れたときは、CNIプラグイン入れないとNodeの状態がREADYにならなかったはずなのに、
今回試したらNodeが参加した時点でREADYになってた。ランタイムが違うから？そんなことある？

とりあえず、WeaveNetをいれておく。

#+begin_src bash
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
#+end_src

*** クラスタのテスト
OCIとして、runcを使うPodとkataを使うPodをデプロイしてみる

#+begin_src bash
kubectl run hello-runc --image=gcr.io/google-samples/hello-app:1.0 --restart Never
cat <<EOF | kubectl apply -f -
apiVersion: node.k8s.io/v1beta1
kind: RuntimeClass
metadata:
  name: kata
handler: kata-runtime
EOF
kubectl get pod hello-runc -o yaml > hello-kata.yaml
#+end_src

hello-kata.yamlを以下の通り編集
#+begin_src yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: hello-kata
  name: hello-kata
spec:
  containers:
  - image: gcr.io/google-samples/hello-app:1.0
    imagePullPolicy: IfNotPresent
    name: hello-kata
  dnsPolicy: ClusterFirst
  restartPolicy: Never
  runtimeClassName: kata
#+end_src

これを流したんたけどPodが起動しない。eventを見てみると以下のようなログが。

#+begin_src
Failed to create pod sandbox: rpc error: code = Unknown desc = container create failed: failed to launch qemu: exit status 1, error messages from qemu log: Could not access KVM kernel module: No such file or directory
qemu-vanilla-system-x86_64: failed to initialize kvm: No such file or directory
#+end_src

今回ESXi上の仮想マシンでやったのだけど、CPUの仮想化を有効にするの忘れてた。仮想マシンの設定変更から、
「CPU仮想化 ハードウェア アシストによる仮想化をゲストOSに公開」を有効にしたところ解決。

#+begin_src
kubectl get pod -o wide
NAME         READY   STATUS    RESTARTS   AGE   IP          NODE    NOMINATED NODE   READINESS GATES
hello-kata   1/1     Running   0          9h    10.32.0.2   node1   <none>           <none>
hello-runc   1/1     Running   0          9h    10.38.0.3   node2   <none>           <none>
#+end_src

無事に起動したっぽい。

**** 動作を見比べる
うまいことnode1とnode2に分散してPodを動かしたので、通常のruncで動くパターンとkataで動くパターンのプロセス構成などを見てみる。

***** kata-runtime list
kataで動いているコンテナのリストは、 =kata-runtime list= で確認することができる。

- Node1 (kata利用)
#+begin_src
$ sudo kata-runtime list
ID                                                                 PID         STATUS      BU
NDLE                                                                                                                 CREATED                          OWNER
fa157caa041230c1593ced717618dc2f96a80f4c0704b7d965421a8e95dc791f   2850        running     /run/containers/storage/overlay-containers/fa157caa041230c1593ced717618dc2f96a80f4c0704b7d965421a8e95dc791f/userdata   2020-07-11T23:51:20.244499159Z   #0
4fe1ddb9154cbfc14a7ca514e2705b91f54bfc9b89300c940ff1000b2f0bd17c   3115        running     /run/containers/storage/overlay-containers/4fe1ddb9154cbfc14a7ca514e2705b91f54bfc9b89300c940ff1000b2f0bd17c/userdata   2020-07-11T23:51:26.190503017Z   #0
#+end_src

- Node2 (runc利用)
#+begin_src
$ sudo kata-runtime list
ID          PID         STATUS      BUNDLE      CREATED     OWNER
#+end_src

たしかに、Node1では動いているプロセスがいて、Node2にはいないことがわかる。
でも、なんで2つ？ Podはひとつしか起動してないのに。

もう少しNode1側を詳しく見てみる。

#+begin_src
$ sudo kata-runtime state fa157caa041230c1593ced717618dc2f96a80f4c0704b7d965421a8e95dc791f
{
  "ociVersion": "1.0.1-dev",
  "id": "fa157caa041230c1593ced717618dc2f96a80f4c0704b7d965421a8e95dc791f",
  "status": "running",
  "pid": 2850,
  "bundle": "/run/containers/storage/overlay-containers/fa157caa041230c1593ced717618dc2f96a80f4c0704b7d965421a8e95dc791f/userdata",
  "annotations": {
    "io.katacontainers.pkg.oci.bundle_path": "/run/containers/storage/overlay-containers/fa157caa041230c1593ced717618dc2f96a80f4c0704b7d965421a8e95dc791f/userdata",
    "io.katacontainers.pkg.oci.container_type": "pod_sandbox"
  }
}
$ sudo kata-runtime state 4fe1ddb9154cbfc14a7ca514e2705b91f54bfc9b89300c940ff1000b2f0bd17c
{
  "ociVersion": "1.0.1-dev",
  "id": "4fe1ddb9154cbfc14a7ca514e2705b91f54bfc9b89300c940ff1000b2f0bd17c",
  "status": "running",
  "pid": 3115,
  "bundle": "/run/containers/storage/overlay-containers/4fe1ddb9154cbfc14a7ca514e2705b91f54bfc9b89300c940ff1000b2f0bd17c/userdata",
  "annotations": {
    "io.katacontainers.pkg.oci.bundle_path": "/run/containers/storage/overlay-containers/4fe1ddb9154cbfc14a7ca514e2705b91f54bfc9b89300c940ff1000b2f0bd17c/userdata",
    "io.katacontainers.pkg.oci.container_type": "pod_container"
  }
}
#+end_src

コンテナタイプが違うのがわかる。公式のドキュメントのアーキテクチャのところを見ると、
pod_sandboxの中に、pod_containerがあるようだ。

https://github.com/kata-containers/documentation/blob/master/design/architecture.md

#+begin_src
$ sudo kata-runtime exec 4fe1ddb9154cbfc14a7ca514e2705b91f54bfc9b89300c940ff1000b2f0bd17c ps
PID   USER     TIME   COMMAND
    1 root       0:00 ./hello-app
   28 root       0:00 ps
$ sudo kata-runtime exec fa157caa041230c1593ced717618dc2f96a80f4c0704b7d965421a8e95dc791f ps
rpc error: code = Internal desc = Could not run process: container_linux.go:349: starting container process caused "exec: \"ps\": executable file not found in $PATH"
#+end_src
pod_contaierの方で、期待するアプリが動いていることが確認できた。sandboxのほうは、shすら起動できなかったので、何が動いているんだろうか。

***** psの結果
プロセスツリーも見比べてみた。適当にプロセスは実際のものから削っている。

- Node1 (kata利用)
#+begin_src
systemd-+-2*[conmon-+-pause]
        |           `-{conmon}]
        |-conmon-+-kube-proxy---7*[{kube-proxy}]
        |        `-{conmon}
        |-conmon-+-kube-utils---8*[{kube-utils}]
        |        |-launch.sh---weaver---15*[{weaver}]
        |        `-{conmon}
        |-conmon-+-kata-proxy---8*[{kata-proxy}]
        |        |-kata-shim---8*[{kata-shim}]
        |        |-qemu-vanilla-sy---3*[{qemu-vanilla-sy}]
        |        `-{conmon}
        |-conmon-+-weave-npc-+-ulogd
        |        |           `-9*[{weave-npc}]
        |        `-{conmon}
        |-conmon-+-kata-shim---10*[{kata-shim}]
        |        `-{conmon}
        |-crio---14*[{crio}]
        |-kubelet---16*[{kubelet}]
        `-lxcfs---2*[{lxcfs}]
#+end_src

- Node2 (runc利用)
#+begin_src
systemd-+-3*[conmon-+-pause]
        |           `-{conmon}]
        |-conmon-+-kube-proxy---8*[{kube-proxy}]
        |        `-{conmon}
        |-conmon-+-kube-utils---8*[{kube-utils}]
        |        |-launch.sh---weaver---16*[{weaver}]
        |        `-{conmon}
        |-conmon-+-weave-npc-+-ulogd
        |        |           `-9*[{weave-npc}]
        |        `-{conmon}
        |-conmon-+-hello-app---3*[{hello-app}]
        |        `-{conmon}
        |-crio---14*[{crio}]
        |-kubelet---16*[{kubelet}]
        `-lxcfs---2*[{lxcfs}]
#+end_src

見比べてみると、たしかにruncだと目的のhello-appが直接動いているのに対して、
kataの場合は、hello-appは直接ホストから見えない。
kata-shimで隠蔽されていて、隔離された環境で動いていることがわかる。

**** まとめ
Kata Containersは、これまで安全にコンテナ実行するために使う、ぐらいしか聞いておらず
どういう風に動くのかよくわかっていなかったが、今回構築してみてその動きが理解できた。
構築も、ドキュメントによって書いてあること違ったりでいくつかトラブルところもあったが、
だいたいログ見たらどこがあやしいかわかるし、それほど苦労することはなかった。
1枚噛んでるレイヤが増えるので、性能面とリソースのオーバーヘッドが気になるので、今後その辺見てみたい。
** DONE Tektonをさわってみた                              :kubernetes:tekton:
   CLOSED: [2020-07-19 Sun 12:25]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-07\")
:EXPORT_FILE_NAME: 202007191122
:END:

kubernetesで動かすCI/CDツールとして、聞いてはいたものの、これまでぜんぜんさわれてなかったtektonをちょっとだけさわってみた。

https://tekton.dev/

タスクやパイプラインがCRDとして定義されているので、ぜんぶフォーマットを統一できるのがよさそう。

*** インストール
https://github.com/tektoncd/pipeline/blob/master/docs/install.md
にしたがって実施。
#+begin_src bash
kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml
#+end_src

=tekton-pipeline= namespaceができてるのでPodを確認。
#+begin_export
NAME                                           READY   STATUS    RESTARTS   AGE
tekton-pipelines-controller-559bd4d4df-9rwjl   1/1     Running   0          54s
tekton-pipelines-webhook-7bfd859f8c-mzc2n      1/1     Running   0          54s
#+end_export

ビルド成果物を格納するためにPersistentVolumeの設定をする。S3やGoogleCloudStorageのような、クラウドストレージも利用できるようだ。
=config-artifact-pvc= がすでにできていて、StorageClassやVolumeのサイズを設定できるようだ。
今回は、デフォルト値で動かすことに。

また、tekton cliもインストールしておく。kubectlのプラグインになるようにシンボリックリンクで、kubectl-xxxのファイルを作成する。
https://github.com/tektoncd/cli

#+begin_src bash
sudo ln -s /usr/bin/tkn /usr/local/bin/kubectl-tkn
#+end_src

*** チュートリアルの実施
https://github.com/tektoncd/pipeline/blob/master/docs/tutorial.md

#+begin_src bash
cat <<EOF | kubectl apply -f -
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: echo-hello-world
spec:
  steps:
    - name: echo
      image: ubuntu
      command:
        - echo
      args:
        - "Hello World"
EOF

cat <<EOF | kubectl apply -f -
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: echo-hello-world-task-run
spec:
  taskRef:
    name: echo-hello-world
EOF

kubectl tkn taskrun logs echo-hello-world-task-run
#+end_src

=Task= と =TaskRun= があり、Taskは実際にやることを書き、実行するにはTaskRunを作成する、と。

*** まとめ
いったんインストールとタスクの定義、その実行まで見てみた。
これだけだとCI/CDツールっぽさがないので、パイプラインはこのあと見ていく予定。

やりました。 [[Tektonでパイプラインを動かす]]

** DONE Tektonでパイプラインを動かす                                 :tekton:kubernetes:
   CLOSED: [2020-07-24 Fri 15:47]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-07\")
:EXPORT_FILE_NAME: 202007231454
:END:

[[Tektonをさわってみた]] のつづき

簡単なパイプラインをくんで動かしてみた。

*** 作るもの

Goで作ったシンプルなWebサーバのアプリ。8080ポートでListenしてて、アクセスするとホスト名を返してくれるだけのやつ。

これを、githubからpullしてきて、ビルドしてイメージ化してpushするだけのシンプルなパイプラインを作る。

以下の通り、ソースコードとパイプライン含め、githubに配置している。

https://github.com/grugrut/go-web-hello

*** タスクを作る

パイプラインは、複数のタスクを順番に実行していくものなので、パイプラインの前にタスクを作る必要がある。

もちろんタスクを前回のように、自分で定義するのもよいが、TektonではCatalogというリポジトリに
いろいろな人が作ったTaskが公開されているので、これを使うのが簡単。

https://github.com/tektoncd/catalog/

この中から、githubからソースコードを取得するのに =git-clone= 、
goのビルドをするのに =golang-build= 、コンテナイメージを作成してDockerHubにpushするのに
=buildah= を利用してみた。

*** パイプラインを定義する

パイプラインも他のリソースと同様に、yamlで定義する。

#+begin_src yaml
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: go-web-hello-pipeline
spec:
  workspaces:
    - name: shared-data
  tasks:
    - name: fetch-repo
      taskRef:
        name: git-clone
      workspaces:
        - name: output
          workspace: shared-data
      params:
        - name: url
          value: https://github.com/grugrut/go-web-hello.git
    - name: build
      taskRef:
        name: golang-build
      runAfter:
        - fetch-repo
      params:
        - name: package
          value: github.com/grugrut/go-web-hello
        - name: packages
          value: ./...
      workspaces:
        - name: source
          workspace: shared-data
    - name: docker-build
      taskRef:
        name: buildah
      runAfter:
        - build
      params:
        - name: IMAGE
          value: grugrut/go-web-hello
      workspaces:
        - name: source
          workspace: shared-data
#+end_src

パイプラインのspecには大きくふたつの定義をおこなう。

- workspaces
  各タスクでの作業領域。同じ名前のワークスペースを使うことでタスク間で中間成果物を受け渡すことができる。
  実体としては、Podにvolumeがマウントされる。具体的なvolumeの定義は実行時におこなう。
- tasks
  具体的なタスク群を記載していく。
  今回は3つのタスクを実行するが、具体的な定義内容は以下の通り。

**** GitHubからソースコードをクローン

#+begin_src yaml
    - name: fetch-repo
      taskRef:
        name: git-clone
      workspaces:
        - name: output
          workspace: shared-data
      params:
        - name: url
          value: https://github.com/grugrut/go-web-hello.git
#+end_src

=git-clone= タスクを利用した。outputのワークスペースにソースコードをcloneして、次のタスクに渡すことができる。
今回はリポジトリのurlしか指定していないが、ブランチ名を指定することなどももちろんできる。

**** Goのソースをビルド

#+begin_src  yaml
    - name: build
      taskRef:
        name: golang-build
      runAfter:
        - fetch-repo
      params:
        - name: package
          value: github.com/grugrut/go-web-hello
        - name: packages
          value: ./...
      workspaces:
        - name: source
          workspace: shared-data
#+end_src

=golang-build= タスクを利用した。sourceのワークスペースに対して、 =go build -v $(packages)= をしてくれる。
また、ソースコードのcloneがおこなわれてから実行されるように、 runAfterで指定している。
これがないと並列にタスクが実行されてしまい、うまくいかないはず。

昔に、Jenkinsでgoのビルドをしたことがある(http://grugrut.hatenablog.jp/entry/2017/04/10/201607)が、
=GOPATH= のあつかいが面倒で、withEnvとか駆使しないといけなかった。
Tektonの場合、それぞれのタスクごとにPodがわかれていて、 =GOPATH= も設定ずみのところにソースが配置されるように
あらかじめ設定されているので、まったく気にすることなくビルドできて便利だと思った。

**** コンテナイメージのビルドとPush

#+begin_src yaml
    - name: docker-build
      taskRef:
        name: buildah
      runAfter:
        - build
      params:
        - name: IMAGE
          value: grugrut/go-web-hello
      workspaces:
        - name: source
          workspace: shared-data
#+end_src

=buildah= タスクを利用した。何も指定しないと、workspace直下のDockerfileでビルドして、
イメージ名にもとづき、イメージのpushまでをおこなってくれる。

そのため、たとえばDockerHubのような認証が必要な場合は、事前に認証情報を作成しておく。

#+begin_src bash
kubectl create secret generic basic-user-pass --type kubernetes.io/basic-auth --from-literal username=user --from-literal password=pass
kubectl annotate secrets basic-user-pass tekton.dev/docker-0=https://docker.io
#+end_src

=tekton.dev/docker-0= のアノテーションをつけておくことで、docker pushするときの認証として使われるようになる。

あとは、これがパイプラインが動作する際に利用されるように、ServiceAccountを作成しておく。

#+begin_src yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-bot
secrets:
  - name: basic-user-pass
#+end_src

*** パイプラインを実行する
パイプラインを実行する場合は、 =PipelineRun= のリソースを作成する。またこの際に実行するパイプラインの情報をいろいろとつける。

#+begin_src bash
cat <<EOF | kubectl create -f -
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: go-web-hello-pipeline-
spec:
  pipelineRef:
    name: go-web-hello-pipeline
  serviceAccountName: build-bot
  workspaces:
    - name: shared-data
      volumeClaimTemplate:
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 1Gi
          storageClassName: nfs-client
EOF
#+end_src

今回はgenerateNameを使っているので、 =apply= ではなく、 =create= していることに注意。
PipelineRunでは、どのパイプラインを実行するかとserviceAccountとworkspaceとして利用するvolumeの情報を書いている。

*** パイプラインの結果を見る
パイプラインのタスクはPodとして動くが、 =tkn= コマンドでよりわかりやすく見ることができる。

- パイプラインの実行結果の一覧を見る

  =tkn pipelinerun list=

  #+begin_src
$ tkn pr list
NAME                              STARTED       DURATION     STATUS             
go-web-hello-pipeline-c9dhn       3 hours ago   59 seconds   Succeeded          
go-web-hello-pipeline-xx5qb       1 day ago     55 seconds   Failed
  #+end_src

- パイプラインの実行結果の詳細を見る

  =tkn pipelinerun describe xxxx=

  #+begin_src
$ tkn pr describe go-web-hello-pipeline-c9dhn
Name:              go-web-hello-pipeline-c9dhn
Namespace:         default
Pipeline Ref:      go-web-hello-pipeline
Service Account:   build-bot

??  Status

STARTED       DURATION     STATUS
3 hours ago   59 seconds   Succeeded

? Resources

 No resources

? Params

 No params

?  Taskruns

 NAME                                               TASK NAME      STARTED       DURATION     STATUS
 ・ go-web-hello-pipeline-c9dhn-docker-build-mpht6   docker-build   3 hours ago   40 seconds   Succeeded
 ・ go-web-hello-pipeline-c9dhn-build-x72xl          build          3 hours ago   11 seconds   Succeeded
 ・ go-web-hello-pipeline-c9dhn-fetch-repo-nmkx7     fetch-repo     3 hours ago   8 seconds    Succeeded
  #+end_src
- パイプラインの実行時のログを見る(=-f= オプションをつけることで実行中でも見れる)

  =tkn pipeline log xxxxx=

*** まとめ

Tektonを使ってパイプラインを実行することができた。CI/CDというには、実行のトリガーのところとか、デプロイのところができてないので、
次はそのへんを見ていく予定。(コンテナイメージもlatestタグになってて、超微妙だし。。。)
** DONE インストール後にkube-proxyの動作モードをIPVSモードに変更する :kubernetes:
   CLOSED: [2020-07-27 Mon 23:25]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-07\")
:EXPORT_FILE_NAME: 202007272308
:END:

今、家で使っているKubernetesクラスタについて、インストール時に気にかけておらず、
=kube-proxy= が =iptables= モードで動いているのでは？ と思ったので、確認して =ipvs= モードに変更してみた。

ちなみに、たしかにipvsモードのほうがパフォーマンスに優れると言われてはいる。
しかしながら、Calicoでおなじみのtigeraによると、大規模になれば違いはでてくるが、
100程度のオーダーでは違いは無いらしい。もはや、ただの自己満である。

https://www.tigera.io/blog/comparing-kube-proxy-modes-iptables-or-ipvs/
*** 現状確認
設定見ればすぐだが、ログを見ても動作確認はできる。
#+begin_src 
$ kubectl -n kube-system logs kube-proxy-6vvrf kube-proxy
(trim)
W0711 06:32:25.413300       1 server_others.go:324] Unknown proxy mode "", assuming iptables proxy
I0711 06:32:25.418063       1 server_others.go:145] Using iptables Proxier.
I0711 06:32:25.418401       1 server.go:571] Version: v1.17.8
(trim)
#+end_src

未設定なので、iptagblesモードで動くよとばっちり出ている。
*** ipvsモードに修正
=kubectl -n kube-system edit configmaps kube-proxy= して、
=mode: ""= になっているところを、 =mode: ipvs= に修正する。

修正したら、 =kubectl -n rollout restart daemonset kube-proxy= して、再起動すればおしまい。
(もちろん、各Podをdeleteして再作成するのも可)
*** 修正後確認

#+begin_src 
$ kubectl -n kube-system logs kube-proxy-z8nwd kube-proxy
I0727 14:02:58.646065       1 server_others.go:172] Using ipvs Proxier.
W0727 14:02:58.646292       1 proxier.go:420] IPVS scheduler not specified, use rr by default
I0727 14:02:58.646423       1 server.go:571] Version: v1.17.8
#+end_src

Warningがでているが、IPVSモードでは、kube-proxyの負荷分散方式を、ラウンドロビンや
リーストコネクションなどから選べるようだ。
指定していないとラウンドロビンになるようだが、まあそれでいいのではないかな。

https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/#ipvs-based-kube-proxy
*** まとめ
ipvsの場合の負荷分散方式が、いろいろあるってことは知らなかったので、やってみてよかった。
** DONE Hyper-V上でGitLabサーバを構築する              :hyperv:gitlab:docker:
   CLOSED: [2020-07-30 Thu 07:09]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-07\")
:EXPORT_FILE_NAME: 202007292345
:END:

いろいろあって、GitLabを構築を試す必要があったので手順のメモ。普段使っている検証用の
VMware環境は、kubernetesが動いていて、特にメモリ確保が厳しそうだったので、
WindowsのHyper-V上に作ることにした。

*** Hyper-Vに仮想マシンを作る

とにかくHyper-Vのネットワークが難解で、デフォルトスイッチで作ると
OS起動のたびにIPアドレスが変動するし、
外部ネットワークも下手に作ると母艦で通信できなくなるという
なんでこんなことになってるの？って動きをしてくださいますので、
"管理オペレーティング システムにこのネットワーク アダプターの共有を許可する"
にチェックをつけて、外部ネットワークにつなげる。
これにより、VMware Playerなどのブリッジ接続と同じになる。

多分これが一番早いと思います。

検索すると、vNICをふたつ作って内部ネットワークを固定して、インターネット通信は
デフォルトスイッチにするのがよいってのが多々あったけど、めんどくさいよ。

OSは適当に最新のFedoraのISOをもってきてインストールした。

ホスト名は、シンプルに =gitlab.local= にしている。

ちなみに、SELinuxとFirewallは無効化している。

*** GitLabをインストールする

GitLab EEには魅力的な機能が多数あるが、今回はざっと作るだけなので CEの機能があれば十分。
なのだが、公式サイトにも別にライセンス登録しないEEはCEと変わらんのでEE入れれば？ってあるので
GitLab EEを入れることにする。

https://www.gitlab.jp/install/ce-or-ee/

Fedoraは公式には対応していないので、あたかもRHEL8であるかのようにごまかして導入する必要がある。

#+begin_src bash
curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.rpm.sh -o script.sh
chmod +x script.sh
os=el dist=8 ./script.sh
EXTERNAL_URL="http://gitlab.local" dnf install -y gitlab-ee
#+end_src

インストールがおわったら、 =http://gitlab.local= にアクセスすると、 =root= ユーザのパスワード設定が求められ、
設定後ログインが可能になる。

*** 自己署名証明書を作る

イメージレジストリを有効化したいが、こちらはhttpだとdocker操作時に怒られてしまって面倒なので、証明書を準備する。
Let's Encriptのほうが楽だと思うのだけど、今回は自己署名証明書を作ることにする。

#+begin_src bash
openssl req -newkey rsa:4096 -nodes -sha256 -keyout registry.gitlab.local.key -x509 -days 3650 -out registry.gitlab.local.crt
#+end_src

CNだけ指定した。

#+begin_src 
Country Name (2 letter code) [XX]:
State or Province Name (full name) []:
Locality Name (eg, city) [Default City]:
Organization Name (eg, company) [Default Company Ltd]:
Organizational Unit Name (eg, section) []:
Common Name (eg, your name or your server's hostname) []:registry.gitlab.local
Email Address []:
#+end_src

*** GitLabのイメージレジストリの有効化

#+begin_src bash
mkdir -p /etc/gitlab/ssl
cp registry.gitlab.local.crt registry.gitlab.local.key /etc/gitlab/ssl/
#+end_src

=/etc/gitlab/gitlab.rb= を編集して、以下の行を追加(コメント化されてあるので、
コメント化解除して値を書き換え)。

#+begin_src
registry_external_url 'https://registry.gitlab.local'

gitlab_rails['registry_host'] = "registry.gitlab.local"
#+end_src

変更を反映する。
#+begin_src bash
gitlab-ctl reconfigure
#+end_src

試しに適当にプロジェクトを作ってみると、コンテナレジストリも有効化されていることがわかる。

[[file:images/20200730-gitlab-registry.png]]

*** イメージをpushする
テストとして、docker login & イメージプッシュしたいが、オレオレ証明書なので
そのままでは利用できない。

- Linuxの場合
先ほど作った registry.gitlab.local.crt を クライアント側(docker loginする側)の
=/etc/docker/certs.d/registry.gitlab.local/ca.crt= にコピーする。
ディレクトリがなければ作成する。

- Windowsの場合
Docker for Windowsのダッシュボードを開いて、SettingsのDocker Engineから以下のように設定する。

#+begin_src json
{
  "registry-mirrors": [],
  "insecure-registries": ["registry.gitlab.local"],
  "debug": true,
  "experimental": false
}
#+end_src

これで、docker loginならびにdocker pushできるはず。
** DONE Hyper-Vにokd4(OpenShift Origin)をインストールする  :hyperv:openshift:
   CLOSED: [2020-08-01 Sat 13:09]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-08\")
:EXPORT_FILE_NAME: 202008011309
:END:
最近出たというOpenShiftのupstream版である =okd= をインストールしようとしたら結構てこずったのでメモ。
Hyper-Vは対応プラットフォームに書かれていないので、手探り感がすごい。

okdは昔は =OpenShift Origin= と呼ばれていたが、OpenShift3の途中からokdという名前に変わった。
その後、OpenShift 4になって、ずっと出ていなかったが、OpenShift 4.5にあわせて再登場したらしい。

https://www.publickey1.jp/blog/20/red_hatkubernetesokd_4.html


ちなみに、OpenShift4を手軽に試す環境として、 =CodeReady Containers= というものあり、
こちらは1VMで動かす =minikube= や =minishift= みたいなものだ。

CodeReady Containersも以前構築してみたことがあり、そのときの記事がこちら。

[[CodeReady ContainersでWindows上にOpenShift環境を構築する]]

CodeReady Containersは完全お試し用のもので、30日で証明書が切れると再インストール、
すなわち最初からやりなおしという致命的な弱点がある。
OKDはバージョンアップの方法も書かれているので、そういったことは無いと信じたい。

以下の実施内容は、基本的に公式ドキュメントの記載にもとづいておこなった。

https://docs.okd.io/latest/installing/installing_bare_metal/installing-bare-metal.html

*** 準備するもの

| 名前           | CPU   | メモリ | 備考                   |
| Master         | 4vCPU | 24GB   | コントロールプレーン   |
| Bootstrap      | 4vCPU | 16GB   | インストール時だけ必要 |
| ロードバランサ |       |        | nginxを利用            |
| DNS            |       |        | dnsmasqを利用          |
| HTTPサーバ     |       |        | nginxを利用            |

ドキュメントには、コントロールプレーンが3台、ワーカーノードが2台必要って書かれているが、
以下のFAQに、コントロールプレーン1台でもOKって書かれていたので、そのようにしてみた。

Bootstrapサーバがインストール時だけ必要なくせに、16GB必要だし、おかげでLBも用意しなきゃいけないしでつらい。

本来コントロールプレーンもメモリ16GBでよいのだが、さすがに1ノードにまとめるならもうちょっと入れておくかと24GBにした。
本当は32GB確保したかったのだけど、Bootstrapのせいで確保できなかったので妥協。

https://github.com/openshift/okd/blob/master/FAQ.md#can-i-run-a-single-node-cluster

ロードバランサ、DNS、HTTPサーバは先日作ったGitlab用のVMがあったので、そこにまとめて入れることにした。

また、okd(openshift)は、 =xxx.クラスタ名.ベースドメイン= という形式のFQDNでアクセスすることになる。
今回は、クラスタ名を =okd= 、ベースドメインを =local= とした。

*** インストール

**** 作業端末の準備
1. 作業端末を準備する。 =openshift-install= コマンドがLinuxとmacだけだったのでどちらか。
   自分は、Windowsなので、wsl上で作業した。

2. インストーラをgithubからダウンロードする。

   https://github.com/openshift/okd/releases/tag/4.5.0-0.okd-2020-07-29-070316

  #+begin_src bash
  wget https://github.com/openshift/okd/releases/download/4.5.0-0.okd-2020-07-29-070316/openshift-client-linux-4.5.0-0.okd-2020-07-29-070316.tar.gz
  wget https://github.com/openshift/okd/releases/download/4.5.0-0.okd-2020-07-29-070316/openshift-install-linux-4.5.0-0.okd-2020-07-29-070316.tar.gz

  tar xf openshift-client-linux-4.5.0-0.okd-2020-07-29-070316.tar.gz
  tar xf openshift-install-linux-4.5.0-0.okd-2020-07-29-070316.tar.gz

  #+end_src

3. こちらのページから、Red Hat Developerに登録したアカウントで pull-secret なるファイルをダウンロードする
   
   https://cloud.redhat.com/openshift/install/pull-secret

4. インストール設定を格納するディレクトリを作成する。このとき、ディレクトリ名はクラスタ名にそろえるということなので、 =mkdir okd= とokdディレクトリを作成した。

5. =okd/install-config.yaml= を以下の内容で作成する。

   #+begin_src yaml
apiVersion: v1
baseDomain: local
compute:
- hyperthreading: Enabled
  name: worker
  replicas: 0
controlPlane:
  hyperthreading: Enabled
  name: master
  replicas: 1
metadata:
  name: okd
networking:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  networkType: OpenShiftSDN
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
fips: false
pullSecret: '{"auths": ...}' #先ほどダウンロードした pull-secretの中身
sshKey: 'ssh-ed25519 AAAA...' #公開鍵。なければ適当につくる。
#+end_src

   先述のとおり、今回はコントロールプレーンは1台だけなので、ドキュメントと違って、 =.controlPlane.replicas= を1にしている。

6. マニフェストの作成
install-config.yamlを配置した okdディレクトリの1階層上で以下のコマンドを実行する。
   #+begin_src bash
openshift-install create manifests --dir=okd
   #+end_src

実行すると、install-config.yamlは消えてしまうので、何回もやり直しそうならコピーしておくのが無難。

実行後、設定を変更してコントロールプレーンではPodが起動しないように設定することもできるが、
今回は1台構成なので変更せずにそのままとする。

7. ignitionファイルの作成
   okd(OpenShift)のインストールでは、ignitionファイルとよばれるものの中に、
   install-config.yamlおよびそこから生成されたマニフェストの情報が含まれているようだ。

   #+begin_src bash
openshift-install create ignition-configs --dir=<
#+end_src

   を実行すると、 =bootstrap.ign= =master.ign= =worker.ign= というファイルが作成される。

   作業用端末での操作はいったん止めて、次に周辺サーバの準備、および Fedora CoreOSのインストールをおこなう。

**** DNSの準備
dnsmasqを適当に設定すればよい。自分は以下の通り/etc/hostsを設定し、必要な名前解決ができるようにした。

#+begin_src hosts
192.168.2.11 api.okd.local api-int.okd.local 
192.168.2.12 master.okd.local etcd-0.okd.local _etcd-server-ssl._tcp.okd.local
192.168.2.13 bootstrap.okd.local
#+end_src

**** HTTPサーバの準備

1. nginxをインストール
   #+begin_src bash
dnf install nginx nginx-mod-stream
#+end_src
   のちにLBとしても利用したかったので、streamモジュールもインストール。

2. 通常は、インストールすればおしまいだが、Gitlabが 80ポートを占領しているので、Listenするポートを10080に変更した。

   #+begin_src nginx
http {
  server {
    listen 10080;
    listen [::]:10080;
  }
}
#+end_src

3. /usr/share/nginx/htmlに必要資材を配置

   先ほど作成された =bootstrap.ign= =master.ign= =worker.ign= を配置する。

   以下のサイトからRawファイルとsignatureファイルをダウンロードして配置する。
   ドキュメントにはsignatureファイルのことが書かれてないが配置しないとインストールに失敗するので注意(1敗)。

   また、Rawファイルは展開する必要はない。圧縮ファイルのままで配置しないとインストールに失敗するので注意(1敗)。
   なお、RawとRaw(4k Native)の2種類あるが、よくわからなかったので、Rawのほうを使った。

   https://getfedora.org/en/coreos/download?tab=metal_virtualized&stream=stable

**** Load Balancerの設定
nginxをTCPロードバランサとするために、 =/etc/nginx/nginx.conf= に以下の設定を入れた。

#+begin_src nginx
stream {
  upstream k8s-api {
    server 192.168.2.12:6443;
    server 192.168.2.13:6443;
  }
  upstream machine-config {
    server 192.168.2.12:22623;
    server 192.168.2.13:22623;
  }
  server {
    listen 6443;
    proxy_pass k8s-api;
  }
  server {
    listen 22623;
    proxy_pass machine-config;
  }
}
#+end_src

   
**** Fedora CoreOSをインストール

ここから実際にbootstrapサーバやMasterサーバにokdを動かすためのOSであるFedora CoreOSをインストールしていく。

1. ISOをダウンロード
   
   先ほどRawファイルをダウンロードしたときと同じサイトだが、
   https://getfedora.org/en/coreos/download?tab=metal_virtualized&stream=stable
   からISOをダウンロードする

2. Hyper-Vの管理コンソールから、仮想マシンを作成する。

   メモリの動的割当は無効化した。

3. ISOをマウントしてOSを起動する

4. インストール選択画面で *タブキー* を押し、カーネル引数を入力できるようにする

5. 表示されているパラメータに続けて以下のように入力する

   - Bootstrap

     #+begin_src 
coreos.inst=yes coreos.inst.install_dev=sda coreos.inst.image_url=http://192.168.2.11:10080/fedora-coreos-32.raw.xz coreos.inst.ignition_url=http://192.168.2.11:10080/bootstrap.ign ip=192.168.2.13::192.168.2.1:255.255.255.0:bootstrap.okd.local:eth0:none nameserver=192.168.2.11
#+end_src

   - Master

     #+begin_src 
coreos.inst=yes coreos.inst.install_dev=sda coreos.inst.image_url=http://192.168.2.11:10080/fedora-coreos-32.raw.xz coreos.inst.ignition_url=http://192.168.2.11:10080/master.ign ip=192.168.2.12::192.168.2.1:255.255.255.0:master.okd.local:eth0:none  nameserver=192.168.2.11
     #+end_src

   それぞれのパラメータの詳細は以下のとおり。

   =coreos.inst.image_url= には、rawファイルにアクセスできるURLを書く。 

   =coreos.inst.ignition_url= には、それぞれのignitionファイルにアクセスできるURLを書く。

   =ip= は、次のフォーマットでIPアドレスを設定する =IPアドレス::デフォルトゲートウェイ:サブネットマスク:ホスト名:デバイス名:none= 。
   IPアドレスの直後だけコロンが2つなことに注意(1敗)。

   =nameserver= は、ネームサーバのアドレスを書く。これもドキュメントには目立たないところにあるので忘れないように(1敗)。

   ちなみに手入力がつらかったのだが、Hyper-Vにはクリップボードから入力という神機能があった。
   ただし、動作としてペーストではなくて1文字ずつ解釈して変わりに入力してくれるRPAみたいな機能のため、
   JIS配列とUS配列の記号位置による違いがそのまま反映される。 === が =_= になってしまうとか。

   これは、あらかじめ自分の環境もIMEを変更してUSキーボードにしておくことで回避できた。
   
6. 入力したらエンターキーを押すとインストールがはじまる。
   
   インストールがおわると一瞬エラーメッセージのようなものが *赤字* で表示されて再起動してしまうのだけれどエラーではないので注意(N敗)。
   単に、再起動時にISOをunmountしようとして、できなかったといってるだけである。
   
   コンマ何秒しか表示されないので、しかたがなく画面を録画して確認して、がっかり。

7. 再起動したら、再度タブを押してISOを手動でunmountしてから再起動する。
   
   これをやらないと、せっかくFedore CoreOSがインストールできたのに、またインストール処理に入ってしまう。
   しかも、未設定の状態で(N敗)。
   
   タブを押すとパラメータ入力待ちとなってくれるため、おちついてディスクをとりはずして再起動できる。

   これでFedora CoreOSのインストールはおしまい。

**** okdのインストール

CoreOSのインストールもおわり、あとはokdをインストールするだけ……なのだけど、実はインストールは
先ほどのignitionファイルの内容をもとに裏で勝手におこなわれるため、待ってるだけでよい。

作業端末で、以下のコマンドを実行するとインストール状況を監視してくれる。

#+begin_src bash
openshift-install --dir=okd wait-for bootstrap-complete --log-level=debug
#+end_src

なお、インストールは30分ぐらいかかるが、20分ぐらいはずっとエラーメッセージが出続ける
(上記コマンドの標準出力も、CoreOSの標準出力も)。

見てても落ち着かないだけなので、のんびり待ちましょう(1敗)。

最終的に、以下のような感じで出力されてコマンドが終了する。
そうしたら無事に終了である。

#+begin_src 
DEBUG Bootstrap status: complete
INFO It is now safe to remove the bootstrap resources
INFO Time elapsed: 30m
#+end_src

*** okdへのアクセス

okd/auth 配下に、kubeconfigファイルとkubeadmin-passwdファイルができている。

kubeconfigファイルを使えば =oc= 、 =kubectl= でCLIアクセスできるし、
https://console-openshift-console.apps.okd.local/ にアクセスすることでWebコンソールも利用可能。
初期ユーザは kubeadmin であり、パスワードは kubeadmin-passwd に書かれている。

この辺は、CodeReady Containersと同じだ。

[[file:images/20200801-okd-login.png]]

CodeReady Containersのころは、OpenShift 4.2相当だったと思うので、
だいぶみためが変わっている。

[[file:images/20200801-okd-dashboard.png]]

インストールが無事にできたので、今度はなんか適当にアプリケーションを動かしてみよう。
** DONE kubernetes-mixinのダッシュボードでgrafanaダッシュボードを簡単に構築する :kubernetes:grafana:
   CLOSED: [2020-08-10 Mon 12:56]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-08\")
:EXPORT_FILE_NAME: 202008032123
:END:
kubernetesのメトリクスモニタリングを、SaaSではなく手元でやろうとしたらPrometheusがおそらく最大の候補であろう。

Prometheusを使うならダッシュボードにはgrafanaを使うことになると思うが、
grafanaはダッシュボード表示エンジンであって、ダッシュボード自体は自分で作ることになる。
これが、やっぱり自分でクエリを書いてレイアウトも考えてと結構面倒くさい。

もちろんgrafanaのコミュニティにも誰かが作ったkubernetes用のダッシュボードは存在するのだが、
kubernetes-monitoringというプロジェクトがあって、そこでダッシュボードやアラートルールを整備しているものがある。
コントリビュータをピックアップしてみると、Red Hatのメンバーが多いみたい。

https://github.com/kubernetes-monitoring/kubernetes-mixin

今回はこれを使ってみることにした。
ネット上の情報だと、とりあえずhelmでprometheusやgrafana入れてみました〜♪ で終わってて
どうやってダッシュボード使うのよ、まで書かれてなかったりするがそこまでやっている。

*** 導入するもの

今回は、Prometheus、Grafanaをhelmを使って導入する。

https://github.com/helm/charts/tree/master/stable/prometheus

https://github.com/helm/charts/tree/master/stable/grafana

prometheusを入れるだけであれば、Prometheus Operator、kube-prometheusなど
いくつか選択肢がある。今回helmにしたのは、pushgatewayがはじめから含まれているのが理由だ。
今のところpushgatewayを使いたいものも無いのだけど、それだけ個別に導入するのも嫌だったので。

prometheusをhelmで入れるなら、grafanaもhelmでいっか、という感じである。
*** kubernetes-mixinのyaml定義を作る
kubernetes-mixinのプロジェクトでは、yaml形式では提供されておらず、
jsonnetという形式で提供されている。

特にダッシュボードのyaml定義だと、同じ記載内容があちこちにでてしまって
修正もれの恐れがあったり、そもそも修正箇所がわかりにくかったりという課題がある。
jsonnetはそれをプログラミング言語のように構造化することで、わかりやすくしている。
といっても、最初自分が見たときも、なにをすればよいのやらという感じでわかりにくいと感じたが。

README.mdに書いてあるとおり、以下のようなかんじで、ダッシュボード、レコーディングルール、アラートルールのyamlを作ることができる。

#+begin_src bash
# 必要なコマンドの取得
go get github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb
pip install jsonnet

# 依存定義の取得
jb install

# yaml定義の出力
make dashboards_out
make prometheus_rules.yaml
make prometheus_alerts.yaml
#+end_src
*** ダッシュボード定義の修正
先ほどの手順で作成した各種yamlを使えばよいのだが、dashboardの各種yaml定義のPromQLは、
helmで入れるprometheusに対して一部動かないところがある。
例えば、CPU Utilisationや各グラフが *N/A* や *No data* になる。

helmで入れるprometheusのjob設定に書かれたメトリクスに付与されるラベルと、
kubernetes-mixinで期待するラベル定義に差があるのが原因だ。

それをhelmチャートにあわせて修正したものが以下の内容である。

https://github.com/grugrut/kubernetes-mixin/commit/d4361ca715b4fcbeab289cf2f7c29282f316651b

やってから気付いたが、本来は直接修正するんじゃなくて、
Configだけ作って上書きするのが正しかったのだと思う。

とはいえ、 =$__interval= でうまくいかず、 =$__range= にするとかもやったので修正は必要。
=$__range= だとうまくいくのが、そもそも誤りなのか、自分の環境固有なのかわかってない。
さすがにこのレベルのミスが、issueにもあがってないというのは奇妙なので自分の環境固有な気がしてる。
*** 各コンポーネントが、メトリクスを返せるようにする
あとは、そもそもPrometheusがメトリクスを収集できるように、いくつかのコンポーネントを修正する必要がある。

 1. metrics-serverを導入する

 2. kube-controllerのメトリクスをprometheusが収集できるようにする

    Masterサーバの =/etc/kubernetes/manifests/kube-controller-manager.yaml= を修正。
    podにannotationを付与。

    #+begin_src yaml
 # 追加するところだけ記載
 metadata:
   annotations:
     prometheus.io/scrape: "true" #追加
     prometheus.io/port: "10252"  #追加
    #+end_src

 3. kube-proxyがメトリクスを外部からアクセスできるようにしていなかったので修正する

    =kubectl -n kube-system edit configmap kube-proxy= でconfig.confを修正。
    =.metrisBindAddress= が、デフォルトでは =""= になっているので、 ="0.0.0.0"= とする

 4. kube-proxyのメトリクスをprometheusが収集できるようにする

    =kubectl -n kube-system edit daemonset kube-proxy= で、podにannotationを付与。
 
    #+begin_src yaml
 # 追加するところだけ記載
 spec:
   template:
     metadata:
       annotations:
         prometheus.io/port: "10249"  #追加
         prometheus.io/scrape: "true" #追加
    #+end_src
 5. kube-schedulerのメトリクスをprometheusが収集できるようにする

    Masterサーバの =/etc/kubernetes/manifests/kube-scheduler.yaml= を修正。
    podにannotationを付与。

    #+begin_src yaml
 # 追加するところだけ記載
 metadata:
   annotations:
     prometheus.io/scrape: "true" #追加
     prometheus.io/port: "10251"  #追加
    #+end_src
*** helmでprometheusとgrafanaを入れる
ここまで下準備ができたら、helmでprometheusとgrafanaを入れるだけだ。
基本的には各chartのドキュメント通りに入れればおしまいなのだが、
作ったyamlを読みこむために、それぞれ以下のような仕込みをする。

ちなみに自分の作った定義は以下に配置している。

https://github.com/grugrut/k8s-playground/tree/4073c565320d396467348a9c7839bcde90873e3a/03_monitoring

**** prometheus
レコーディングルールとアラートルールを、それぞれhelm chartの変数 =serverFiles.recording_rules.yml= と
=serverFiles.alerting_rules.yml= で指定する必要がある。
もし他のルールが必要ないのであれば、以下のように、先ほど作った =prometheus-rules.yaml= を編集して作るのが楽だと思う。

#+begin_src yaml
serverFiles:
  recording_rules.yml:
    # 全ての行に4つスペースをつけてインデントさせた prometheus-rules.yamlを流しこむ
#+end_src

できたファイルがこんな感じ。

https://github.com/grugrut/k8s-playground/blob/0c6c4025686027c6e18aa723d4ac4779f00a3043/03_monitoring/prometheus_rules-variables.yaml

yqとか使っていいかんじに作れないかなと思ったのだけど、式のところが崩れてしまってダメだった。
内部的にjsonに変換する都合上、パイプを使った複数行の表現がうまくいかないのだろう。

ちなみに、emacsなら以下な感じで簡単に作れる。

1. =serverFiles:= と =recording_rules.yaml:= の行を書く
2. =C-x i= で =prometheus-rules.yaml= の内容を挿入する
3. 3行目の先頭で =C-SPC= してマークし、 =M->= で最終行までジャンプする
4. =C-x r t=で、各行の先頭0バイトを空白4つに置き換える

それ以外の設定は、nodeExporterをmasterノードにも配置されるようにしたり、
PVの設定を少ししている。

最終的には、以下のコマンドでインストールできる。

#+begin_src bash
helm install prometheus stable/prometheus -n monitoring -f prometheus-variables.yaml -f prometheus_rules-variables.yaml -f prometheus_alerts-variables.yaml
#+end_src

**** grafana
grafana側では作ったダッシュボードを読み込ませる必要がある。方法としてはいくつかある。

1. インストール後に設定する
2. ひとつのConfigMapにまとめる
3. 別々のConfigMapとする

いろいろと試してみたが、別々のConfigMapにする方法がいちばん簡単だった。

これを実現するには、 grafanaのhelm chartsの =sidecar.dashboards.enabled= をtrueにする。
すると以下のページにもあるとおり、 =grafana_dashboard= というラベルがついたConfigMapが
自動で読み込まれダッシュボードとして使えるようになる。しかも、オンラインなので
設定の反映のために再起動なども不要である。

https://github.com/helm/charts/tree/master/stable/grafana#sidecar-for-dashboards

dashboardのyamlはたくさんあるので、ひとつひとつConfigMapを作るのも、それはそれで面倒だ。
特定のディレクトリにyamlを配置しておけば、以下のようにワンライナーで設定できる。

#+begin_src bash
find dashboards/ -name "*.json" -printf "%f\n" | xargs -t -IXXX -- kubectl -n monitoring create configmap dashboard-XXX --from-file=dashboards/XXX
find dashboards/ -name "*.json" -printf "%f\n" | xargs -t -IXXX -- kubectl -n monitoring label configmap dashboard-XXX grafana_dashboard=1
#+end_src

更新したい場合も、ひとつひとつやってもいいが、以下のようにまとめてやることもできる。

#+begin_src bash
find dashboards/ -name "*.json" -printf "%f\n" | xargs -t -IXXX -- sh -c "kubectl -n monitoring create configmap dashboard-XXX --from-file=dashboards/XXX --dry-run=client -o yaml | kubectl replace -f -"
find dashboards/ -name "*.json" -printf "%f\n" | xargs -t -IXXX -- kubectl -n monitoring label configmap dashboard-XXX grafana_dashboard=1
#+end_src

あとは、helmでgrafanaを入れてしまえばよい。

#+begin_src bash
helm install grafana stable/grafana -f grafana-variables.yaml
#+end_src

ちなみに、実行結果にも出力されるが、grafanaのadminパスワードは自動生成されてsecretに格納されている。
以下のようにして取得できる。

#+begin_src bash
kubectl get secret --namespace monitoring grafana -o jsonpath="{.data.admin-password}" | base64 --decode
#+end_src

これでしばらくすれば、prometheusに収集されたデータをgrafanaで確認することができるはず。
** DONE SpringBootとNuxtJSでTODOアプリを作る             :springboot:nuxstjs:
   CLOSED: [2020-08-25 Tue 23:28]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-08\")
:EXPORT_FILE_NAME: 202008221329
:END:

tektonでもう少し複雑なCI/CDフローを作ったり、別の基盤で動かしたりするサンプルとして、
フロントエンドとバックエンドにわかれたアプリを作ってみた。
とはいえ、あまり複雑にするつもりはなかったので、シンプルにTODOアプリにしている。

普段はGolangで書くことが多いのだけど、今回はフロントエンドに =NuxtJS= 、
バックエンドに =SpringBoot= を使ってみた。

アーキテクチャとしては、こんな感じ。
最初は =VueJS= で考えていたが、ユーザ側に公開するサービスは1つだけにしたかったので
=NuxtJS= のほうがやりやすいかな、とこういう構成にしてみた。

#+begin_src plantuml :file 20200822-architecture.png
left to right direction

actor User

cloud {
rectangle "Web Server" as web {
        rectangle NuxtJS
}
rectangle "AP Server" as ap {
        rectangle SpringBoot
}
database "DB"
}

User --> NuxtJS :HTTP
NuxtJS --> SpringBoot :HTTP
SpringBoot --> DB :JDBC
#+end_src

ソースコードは以下に配置している。

https://github.com/grugrut/todo-springboot

基本的な作りはシンプルだが、やはり初めてさわるものであり、いくつかハマったところがあるので紹介。
*** バックエンド側のSpringBoot
**** 本番・開発で設定をわける
DB接続先情報など、本番と開発で設定をわけたいものがある。
SpringBootでは、 =application.properties= の =spring.profile.active= というプロパティで
プロファイルを設定することができる。

これを設定しておくことで、 =application.properties= が読みこまれた後に、
=application-(プロファイル名).properties= が追加で読みこまれる。

こちらには環境変数が使えるので、
#+begin_src properties
spring.profiles.active = ${ENV:dev}
#+end_src
のように、環境変数 =ENV= が設定されていたらそれをプロファイル名として読みこみ、
未設定の場合には、devが設定されるようにした。

未設定の場合のデフォルト値を設定したことで、開発環境では環境変数を設定しておく必要がないのが便利。

https://github.com/grugrut/todo-springboot/blob/5017ac9b720b8d1d7e6462a4ff439db0a9dfdd07/src/backend/src/main/resources/application.properties

**** 開発時のテーブル作成
開発環境ではH2データベースを利用した。
H2データベースは、javaで書かれたRDBMSで、eclipseからも簡単に扱うことができる。

SpringBootでは、開発環境での実行時にDDLとDMLをそれぞれ実行することができ、
やはり =application.properties= で実行するsqlファイルを設定する。

#+begin_src properties
spring.datasource.schema=classpath:schema.sql
spring.datasource.data=classpath:data.sql
#+end_src

初期データは空でよかったので、schema.sqlの実行だけでよかったのだが、
data.sqlを作成していなかったり作成しても空ファイルだとエラーが出てしまった。

そのため、しかたがなく =SELECT 1;= だけ実行する意味のない =data.sql= を作成して回避している。

これはさすがにもっと良いやり方があるのかと思うが、回避できてるのでヨシ!

#+begin_src plain
org.springframework.context.ApplicationContextException: Unable to start web server; nested exception is org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:161) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:545) ~[spring-context-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:143) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:750) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at net.grugrut.todo.TodoSpringApplication.main(TodoSpringApplication.java:10) ~[classes/:na]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49) ~[spring-boot-devtools-2.3.2.RELEASE.jar:2.3.2.RELEASE]
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:142) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:104) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:437) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:191) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:178) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:158) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	... 14 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'h2Console' defined in class path resource [org/springframework/boot/autoconfigure/h2/H2ConsoleAutoConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.boot.web.servlet.ServletRegistrationBean]: Factory method 'h2Console' threw exception; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.autoconfigure.jdbc.DataSourceInitializerInvoker': Invocation of init method failed; nested exception is org.springframework.jdbc.datasource.init.UncategorizedScriptException: Failed to execute database script from resource [URL [file:/git/todo-springboot/src/backend/target/classes/data.sql]]; nested exception is java.lang.IllegalArgumentException: 'script' must not be null or empty
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:635) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1176) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:556) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:516) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:324) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:322) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:202) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addServletContextInitializerBeans(ServletContextInitializerBeans.java:96) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:85) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:255) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:229) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:53) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5128) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1384) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1374) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[na:na]
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140) ~[na:na]
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:909) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:841) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1384) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1374) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[na:na]
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140) ~[na:na]
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:909) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:262) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:421) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:930) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:486) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:123) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	... 19 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.boot.web.servlet.ServletRegistrationBean]: Factory method 'h2Console' threw exception; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.autoconfigure.jdbc.DataSourceInitializerInvoker': Invocation of init method failed; nested exception is org.springframework.jdbc.datasource.init.UncategorizedScriptException: Failed to execute database script from resource [URL [file:/git/todo-springboot/src/backend/target/classes/data.sql]]; nested exception is java.lang.IllegalArgumentException: 'script' must not be null or empty
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:185) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:650) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	... 59 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.autoconfigure.jdbc.DataSourceInitializerInvoker': Invocation of init method failed; nested exception is org.springframework.jdbc.datasource.init.UncategorizedScriptException: Failed to execute database script from resource [URL [file:/git/todo-springboot/src/backend/target/classes/data.sql]]; nested exception is java.lang.IllegalArgumentException: 'script' must not be null or empty
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:602) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:516) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:324) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:322) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1307) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory$DependencyObjectProvider.getIfAvailable(DefaultListableBeanFactory.java:1947) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.ObjectProvider.ifAvailable(ObjectProvider.java:91) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.boot.autoconfigure.h2.H2ConsoleAutoConfiguration.h2Console(H2ConsoleAutoConfiguration.java:72) ~[spring-boot-autoconfigure-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	... 60 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.autoconfigure.jdbc.DataSourceInitializerInvoker': Invocation of init method failed; nested exception is org.springframework.jdbc.datasource.init.UncategorizedScriptException: Failed to execute database script from resource [URL [file:/git/todo-springboot/src/backend/target/classes/data.sql]]; nested exception is java.lang.IllegalArgumentException: 'script' must not be null or empty
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1794) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:594) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:516) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:324) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:322) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:227) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveNamedBean(DefaultListableBeanFactory.java:1175) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveBean(DefaultListableBeanFactory.java:420) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBean(DefaultListableBeanFactory.java:350) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBean(DefaultListableBeanFactory.java:343) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.boot.autoconfigure.jdbc.DataSourceInitializerPostProcessor.postProcessAfterInitialization(DataSourceInitializerPostProcessor.java:52) ~[spring-boot-autoconfigure-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:430) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1798) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:594) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	... 75 common frames omitted
Caused by: org.springframework.jdbc.datasource.init.UncategorizedScriptException: Failed to execute database script from resource [URL [file:/git/todo-springboot/src/backend/target/classes/data.sql]]; nested exception is java.lang.IllegalArgumentException: 'script' must not be null or empty
	at org.springframework.jdbc.datasource.init.ScriptUtils.executeSqlScript(ScriptUtils.java:645) ~[spring-jdbc-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.jdbc.datasource.init.ResourceDatabasePopulator.populate(ResourceDatabasePopulator.java:254) ~[spring-jdbc-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.jdbc.datasource.init.DatabasePopulatorUtils.execute(DatabasePopulatorUtils.java:49) ~[spring-jdbc-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.boot.autoconfigure.jdbc.DataSourceInitializer.runScripts(DataSourceInitializer.java:202) ~[spring-boot-autoconfigure-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.autoconfigure.jdbc.DataSourceInitializer.initSchema(DataSourceInitializer.java:119) ~[spring-boot-autoconfigure-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.autoconfigure.jdbc.DataSourceInitializerInvoker.initialize(DataSourceInitializerInvoker.java:75) ~[spring-boot-autoconfigure-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.autoconfigure.jdbc.DataSourceInitializerInvoker.afterPropertiesSet(DataSourceInitializerInvoker.java:65) ~[spring-boot-autoconfigure-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1853) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1790) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	... 89 common frames omitted
Caused by: java.lang.IllegalArgumentException: 'script' must not be null or empty
	at org.springframework.util.Assert.hasText(Assert.java:287) ~[spring-core-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.jdbc.datasource.init.ScriptUtils.splitSqlScript(ScriptUtils.java:214) ~[spring-jdbc-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.jdbc.datasource.init.ScriptUtils.executeSqlScript(ScriptUtils.java:592) ~[spring-jdbc-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	... 97 common frames omitted
#+end_src

*** フロントエンド側のNuxtJS

**** 本番・開発で設定をわける
バックエンドと同じくフロントエンド側も、本番では環境変数をあたえることで
設定をわけるようにしたい。

NuxtJSの場合は、 =nuxt.config.js= で設定することができる。

#+begin_src javascript
proxy: {
  '/api/': {
    target: process.env.API_BASE_URL || 'http://localhost:8080',
    pathRewrite: {'^/api/': ''}
  }
},
#+end_src

こんな感じ。他のソース中での変数参照として、 =process.env.ENV_VALUE= を上書きする方法もあるようだったが、
今回の範囲では利用するシーンがなかったので省略。

https://github.com/grugrut/todo-springboot/blob/5017ac9b720b8d1d7e6462a4ff439db0a9dfdd07/src/frontend/nuxt.config.js

**** NuxtJSでサーバサイドをバックエンド呼びだしのプロキシとして利用する

本来であればjavascriptなので、そこからのAPI呼びだしはクライアント(つまり利用者のブラウザ)から
バックエンドのAPIサーバへの直接の呼びだしになる。

しかし、バックエンドのエンドポイントを外にだしたくない場合、NuxtJSのサーバサイドでも動いている特性を活かして、
NuxtJSのサーバ側をリバースプロキシとして利用することができる。

その実現のために、axiosのproxy機能を利用する。

サーバサイドでaxiosを利用するために、 =npm install --save @nuxtjs/axios= して =nuxt.config.js= に以下の設定をすればよい。

#+begin_src javascript
  modules: [
    "@nuxtjs/axios",
  ],
  axios: {
    proxy: true
  },
  proxy: {
    '/api/': {
      // for WSL2, `API_BASE_URL=http://$(cat /etc/resolv.conf | grep nameserver | awk '{print $2}'):8080 npm run dev`
      target: process.env.API_BASE_URL || 'http://localhost:8080',
      pathRewrite: {'^/api/': ''}
    }
  },
#+end_src

これにより、この設定の場合は /api/xxx に対するアクセスが、 サーバ側で動いているExpress経由で target/xxx にアクセスできる。
そのため、バックエンドのエンドポイントはフロントエンドのプロセスからのみアクセスできればよく、
クライアントにまで公開しておく必要がない。

あちこちのスラッシュの有無で、期待通りに動かなかったので、そこが難しかった。

**** サーバサイドとクライアントサイドでのVuexストアの扱い
今回はTODOアプリを作っているので、TODOリストの各アイテムの情報をVuexストアに格納して、
ユーザ操作に応じて最終的なDBとの同期をとっている。

本来であれば、サーバサイドで一度Vuexストアに情報を格納したあと、
以降はクライアントサイドでいい感じに扱いたかったのだが、うまくいかなかったので
Vuexストアは結局すべてクライアントサイドに寄せている。

詳細は以下のふたつのソースを見たほうが確実だと思う。

https://github.com/grugrut/todo-springboot/blob/5017ac9b720b8d1d7e6462a4ff439db0a9dfdd07/src/frontend/pages/index.vue

https://github.com/grugrut/todo-springboot/blob/5017ac9b720b8d1d7e6462a4ff439db0a9dfdd07/src/frontend/store/index.js

このVuexストアの書きかたは、すでに非推奨になっているので書きかえが必要だが。

*** まとめ
フロントエンドをNuxtJS、バックエンドをSpringBootとするシンプルなTODOアプリができた。
このあとは、コンテナ化してkubernetesなどで動かす予定。
