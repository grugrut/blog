#+hugo_base_dir: ../
#+hugo_selection: ./
#+options: author:nil

* 2019
** blogのお引越し                                                      :hugo:
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2019\" \"2019-10\")
:EXPORT_FILE_NAME: 201910030745
:EXPORT_DATE: 2019-10-03
:END:

これまでメインブログをはてなブログ、日々のメモをorgmodeからhugoとかやってたけど、
そもそもの更新頻度が低いのにバラバラになってるとさらに頻度が低くなってしまう。

どっちかに統一しようかと考え、最近の動向から長年連れそってきたはてなブログのほうをやめて、
こちらに持ってくるようにします。
昔のはてなブログの記事も、今でも有用なものはこっちに持ってこようかしら。
** emacs on WSLでSuper/Hyperキーを使う                            :emacs:wsl:windows:
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2019\" \"2019-10\")
:EXPORT_FILE_NAME: 201910040445
:EXPORT_DATE: 2019-10-04
:END:
*** はじめに
Emacs勉強会(に限った話ではないが)だとMacユーザが多いので、私が普段使っている
Control/Meta以外に、Superキー/Hyperキーを割りあててる人もそこそこいる。
以前、Windowsの場合はwinキーがsuperキーになると聞いた覚えもあったのだけど、
うまくいかなかったのでWSLで使ってないキーにSuper/Hyperを割りあててデビューしようと調べてみた。
*** 環境
- WSL
- WSL上で動くEmacs
- X410(Xサーバ)

上記前提なので、 =Emacs for windows= とか、 =mingw= 上のemacsの話ではない。
これであれば、他の手段[fn:modifier]が使えるはず。
 
なお、Xサーバはやはり無償で利用できる =xming= とか =vcxsrv= とかが利用者多いと思うが、
emacsとの相性がよくない[fn:emacs-on-wsl]ので、有償のものを使っている。
=X410= は、ストアアプリとして購入でき、しょっちゅう割引きセールしているので、そのタイミングで買うのが吉。


*** 目指したゴール
無変換キーをSuperキー、変換キーをHyperキーとして使えるようにする。

本来は冒頭の通り、WindowsキーがSuperキーなのが普通だと思うのだけど、
なんかトラップされてうまくうごかなかったのと、Win+○のキーバインドも
結構使うので、それを潰してしまうのも惜しかったので、そのようにした。

前置きおわり。

*** Superキー/Hyperキーを設定する

**** 手段その1 AutoHotKeyを使う

     多分これが一番早いと思います。
     サンプルも多いはず。

     自分は、なんとなくWindowsのユーザランドでキーいじるやつが嫌だったので採用しなかった。
     (CapsとCtrlの入れかえで、結局レジストリいじらないとうまくいかなかったりというのに以前ぶつかったりしたので)

**** 手段その2 xmodmapを使う
=xmodmap= はlinuxというか、Xの世界でキーマップ変更するためのもの。
調べてみると最近は別のがあるっぽいけど、xmodmapでうまく動いたので気にしない。
     
     そうそうキーコードなんて変わらないと思うので、同じ設定で同じように動くはず。
     
***** 手順1. キーコードを特定する
       =xev= というコマンドを使うと、Xのアプリが起動して、そこでもろもろのイベントを標準出力に出して確認することができる。
       これを使うと、変換と無変換のキーコードは以下のとおり、変換が =129= 、無変換が =131= であることがわかる。

#+begin_verse
KeyRelease event, serial 33, synthetic NO, window 0x800001,
    root 0xf4, subw 0x0, time 426541921, (67,97), root:(179,232),
    state 0x0, keycode 131 (keysym 0xff22, Muhenkan), same_screen YES,
    XLookupString gives 0 bytes:
    XFilterEvent returns: False
    
KeyPress event, serial 33, synthetic NO, window 0x800001,
    root 0xf4, subw 0x0, time 426596187, (843,330), root:(955,465),
    state 0x0, keycode 129 (keysym 0xff23, Henkan_Mode), same_screen YES,
    XLookupString gives 0 bytes:
    XmbLookupString gives 0 bytes:
    XFilterEvent returns: False
#+end_verse

***** 手順2. xmodmapの動作確認する
      キーコードがわかったら、=xmodmap= コマンドで一時的にキーシムを書きかえて動作確認してみる。
      
#+begin_example
# xmodmap -e "keycode 131 = Super_L Super_L"
# xmodmap -e "keycode 129 = Hyper_L Hyper_L"
#+end_example

実行後、再度xevを使って期待通り変わっていることを確認する。

#+begin_verse
KeyPress event, serial 33, synthetic NO, window 0x800001,
    root 0xf4, subw 0x0, time 426819890, (125,123), root:(263,284),
    state 0x0, keycode 131 (keysym 0xffeb, Super_L), same_screen YES,
    XKeysymToKeycode returns keycode: 115
    XLookupString gives 0 bytes:
    XmbLookupString gives 0 bytes:
    XFilterEvent returns: False
#+end_verse

先程はキーコード131が無変換だったのが、Superに変わっていることがわかる。


***** 手順3. xmodmapの設定を作成する
       調べると、 ~xmodmap -pke~ を実行して、必要なところだけ書きかえましょう。というのが出てくるのだが、実際のところ必要な設定だけ書けばよかったので、いきなり =.Xmodmap= ファイルを作成する。
       WSLで手持ちのXサーバ使う分には別にファイル名は何でもいいと思うのだが、ここは慣例に従っておく。(なお、 =startx= コマンドでXを起動するときは、雛形で =.Xmodmap=を読み込むのでファイル名重要)

       自分の設定は、こんなかんじ。

       https://github.com/grugrut/dotfiles/blob/master/.Xmodmap

#+begin_src
clear  mod3
clear  mod4
!<muhenkan>
keycode 129 = Hyper_L Hyper_L Hyper_L Hyper_L
!<henkan>
keycode 131 = Super_L Super_L Super_L Super_L
add    mod3 = Hyper_L
add    mod4 = Super_L Super_R
#+end_src
       デフォルトの状態だと、SuperキーとHyperキーが同じ修飾キーとしてあつかわれていて、Hyperキー単体でうまくうけとれないので使われていないmod3にHyperキーを割当ておいた。

       
***** 手順4. 自動で適用されるようにする
       =xmodmap ~/.Xmodmap= とコマンド実行すればよいのだけど、注意点が一つ。
       xmodmapはXサーバに対して設定をおこなうコマンドなので、Xサーバが起動していない状態ではうまく動かない。
       LinuxやBSD使ってるときにもxmodmap使ってたけど、当時は常にxorg-serverが起動してたので今回はじめてそのこと知った。

       ついでなので、OSログイン時にXサーバを起動すべく適当なbatを作成した。
       https://github.com/grugrut/dotfiles/blob/master/_windows/startx.bat

#+begin_src bat
start /b x410.exe

ubuntu.exe run "DISPLAY=127.0.0.1:0.0 xmodmap ~/.Xmodmap"
#+end_src

       x410.exeが使っているXサーバ。バックグラウンド実行させたいので、 =/b= オプションをつけてる。
       そして、 ~ubuntu.exe run~  をすることでwslでコマンド実行できる。

これを Windowsの =shell:startup= に配置することでスタートアップ時に自動実行することができる。
       ただし、直接おくよりもショートカットを配置することをおすすめする。直接配置すると、実行時にコマンドプロンプトが一瞬表示されてうっとうしいが、ショートカットであれば最小化して実行することができるので気にならないからだ。

       
****  まとめ
     xmodmapを使うことで、他のアプリには影響なくwslのX使うアプリだけにいろいろ手をいれられることが確認できた。他にもその手の機能で便利なのありそう。

[fn:modifier] たぶん =w32-lwindow-modifier= とかが使えるはず
[fn:emacs-on-wsl] https://speakerdeck.com/grugrut/emacs-on-wsldefalsekun-rigoto
** DONE skk-azikで「っ」が誤爆しやすい問題                        :emacs:skk:
   CLOSED: [2019-10-22 Tue 22:05]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2019\" \"2019-10\")
:EXPORT_FILE_NAME: 201910222100
:END:

skk48に名乗りを上げたとおり、普段は =skk= を使っているのだけれども、
私は単なるskkではなく、 =azik= を使っている。

azikは簡単に言うと、日本語ローマ字入力(qwertyを想定)に特化したskkの拡張で、
以下のような便利なマッピングになっている。
- 長音(ー)は小指が遠いので「:」で入力できる
- 促音も二回重ねるのが面倒なので「;」で入力できる
- 日本語は「子音+an(晩餐はb + an, s + anとか)」、「子音+in(新品はs + in, p + in)」のように =子音 + 母音 + n= からなる語が多く含まれるので、「子音 + zで子音+az」「子音 + kで子音+in」といった感じに、少ないキータイプで入力することが可能になる。(なので、 =az= =ik= で =azik= )
- さらに拡張で「 =ds= で =です= 」「 =ms= で =ます= 」のように、さらに簡易にするマッピングもされている(自分はあまりこれは使ってない)

詳細については、公式サイトを見てほしい。
[[http://hp.vector.co.jp/authors/VA002116/azik/azikinfo.html]]

ところで、われらがemacsの =ddskk= にもazik用の設定が搭載されているのだが、
なぜかそのマッピングの中に =tU= が、 =っ= にわりあてられている。
そのため、よく入力中に、意図せず =っ= が入力されてしまう問題が発生していた。

例えば「疲れた」や「積む」のような「つ」から始まる感じを入力しようとして、「▽っかれた」のように頭が =つ= ではなく =っ= になってしまう人がいたら同じ症状だと思う。おそらく意識せず =Tu= と打とうとして、 =TU= とか =tU= と入力しているはず。

いろいろ試して以下の設定で改善することが確認できた。
私も長年、そもそも何がおきているかわからずに困っていたのだけれど、もし同様に困っている人いたら参考になれば幸いである。

#+begin_src lisp
(leaf ddskk
  :straight t
  :bind
  (("C-x C-j" . skk-mode)
   ("C-x j"   . skk-mode))
  :init
  (defvar dired-bind-jump nil)  ; dired-xがC-xC-jを奪うので対処しておく
  :custom
  (skk-use-azik . t)                     ; AZIKを使用する
  (skk-azik-keyboard-type . 'jp106)      ;
  :hook
  (skk-azik-load-hook . my/skk-azik-disable-tU)
  :preface
  (defun my/skk-azik-disable-tU ()
    "ddskkのazikモードが`tU'を`つ'として扱うのを抑制する."
    (setq skk-rule-tree (skk-compile-rule-list
                         skk-rom-kana-base-rule-list
                         (skk-del-alist "tU" skk-rom-kana-rule-list)))))
#+end_src
*** 内容の解説

基本的にドキュメントを読む限り、 =skk-rom-kana-rule-list= にユーザ独自の設定は入れるので、
そこから消せばよいはずなのだが、再コンパイルしないとだめだったのでそのようにしている。
ちなみに、 =skk-del-alist= は =skk-rom-kana-rule-list= から不要なのを削除するための便利な関数である。追加したい場合は普通に =append= すればよい。

参考: [[http://mail.ring.gr.jp/skk/200106/msg00009.html]]

また、普通なら =leaf= なり =use-package= なりの =:config= ブロックに設定すればよいのだけど、
ロード後の処理の影響からかazikの設定に上書きされてしまっているように見えたので、
skk-azikの中で最後に呼ばれる =skk-azik-load-hook= を使って、自前のルール修正関数を呼ぶようにしている。

printデバッグしてみたら、 =:config= がそもそも呼ばれてなかったようにも見えたので
もうすこし上手いやりかたがあるのかもしれない。

** DONE leaf-expandでleafのデバッグをする            :emacs:smartparens:leaf:
   CLOSED: [2019-10-14 Mon 22:18]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2019\" \"2019-10\")
:EXPORT_FILE_NAME: 201910141028
:END:

設定しているsmartparensが期待通りに動かずに困っていた。
具体的には、lispを編集するモード(emacs-lisp-modeとか)のときに、「'(シングルクオート)」がダブルクオートとかと同じく「''」となってしまいかえって面倒なことに。

beforeの設定がこんな感じ。
最近は、 =use-package= のかわりに =leaf= を使っているが、use-packageでもだいたい同じだと思う。

#+begin_src lisp
(leaf smartparens
  :straight t
  :require t
  :diminish smartparens-mode
  :config
  (leaf smartparens-config
    :require t
    :after smartparens
    :hook
    (prog-mode-hook . smartparens-mode)))
#+end_src

smartparensの設定は、 =(require 'smartparens-config)= が楽だし確実、というのを見て、たしかにそのように設定してるんだけどなあ。。。って感じだった。

いろいろ見た結果、期待通りにrequireできてないんじゃないの？って結論に至るのだけど、そういったときのデバッグって大変ですよね。

そんな時に便利なのが、 =leaf-expand= で、これはleafマクロで書かれた箇所を展開するとどうなるかがその場でわかる。
それにより、この展開後が

#+begin_src lisp
(prog1 'smartparens-config
  (autoload #'smartparens-mode "smartparens-config" nil t)
  (eval-after-load 'smartparens
    '(progn
       (add-hook 'prog-mode-hook #'smartparens-mode)
       (eval-after-load 'smartparens-config
         '(progn
            (require 'smartparens-config))))))
#+end_src

であることがわかり、これってちゃんとrequireされないよね、ということがわかった。

ちなみに解決後の設定は以下な感じ。

#+begin_src lisp
(leaf smartparens
  :straight t
  :require smartparens-config
  :diminish smartparens-mode
  :hook
  (prog-mode-hook . turn-on-smartparens-mode)
  :config
  (show-smartparens-global-mode t))
#+end_src

これだと展開後の姿も、以下のようになり、とても綺麗(なのか？)。

#+begin_src lisp
(prog1 'smartparens
  (autoload #'turn-on-smartparens-mode "smartparens" nil t)
  (straight-use-package 'smartparens)
  (add-hook 'prog-mode-hook #'turn-on-smartparens-mode)
  (eval-after-load 'smartparens
    '(progn
       (require 'smartparens-config)
       (show-smartparens-global-mode t)
       (diminish 'smartparens-mode))))
#+end_src

これは便利なので、今後も積極的に使っていきたい。
** DONE CodeReady ContainersでWindows上にOpenShift環境を構築する :openshift:kubernetes:crc:windows:
   CLOSED: [2019-10-19 Sat 14:03]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2019\" \"2019-10\")
:EXPORT_FILE_NAME: 201910191042
:END:
OpenShift4.2がリリースされたので、家で使ってみようと、 =CodeReady Containers(crc)= をインストールしてみた。
CodeReady Containersは、これまで =minishift= という名前のプロダクトだったものが、OpenShift 4.xになって名前が変わったもので、
テストとか開発とかに使えるものである。

10/17にgithub上では1.0.0のタグが切られていたが、まだpre-releaseのようだ。
Red Hat Developers Programに登録していれば、Developer Preview版が利用できるようだ。
*** 導入した環境
- Windows 10 Professional
- メモリ 64 GB (メモリはわりと食うので少ないとつらいと思う)

*** ダウンロード
ここからリンクを辿っていくとダウンロードできる。

https://developers.redhat.com/products/codeready-containers

OSごとのバイナリと、インストール時に入力が必要なpull secretをダウンロードしておく。
2GBぐらいあり、わりと重たい。
*** 起動

基本ドキュメント通りにやればよいはず。

1. ダウンロードしたファイルを展開し、 =crc= バイナリをパスの通った場所に配置する
2. 仮想マシンを作成する

   たぶん場合によってはHyper-Vのネットワークが作成されたりするはず。
   #+begin_src bat
λ crc setup
INFO Checking if running as normal user
INFO Caching oc binary
INFO Unpacking bundle from the CRC binary
INFO Check Windows 10 release
INFO Hyper-V installed
INFO Is user a member of the Hyper-V Administrators group
INFO Does the Hyper-V virtual switch exist
Setup is complete, you can now run 'crc start' to start a CodeReady Containers instance
#+end_src
3. 起動する

   起動時にデフォルトではメモリを8GBで起動するが、何かやるには到底足りないので、16GBぐらいは指定しておきたい。
   また、DNSサーバを指定しておかないと、他の仮想マシンを動かしてたり仮想ネットワークが複数あったりした場合に、
   うまく名前解決できないケースがあったので指定しておくのが吉。
   #+begin_src bat
λ crc start -m 16384 -n 8.8.8.8
INFO Checking if running as normal user
INFO Checking if oc binary is cached
INFO Check Windows 10 release
INFO Hyper-V installed and operational
INFO Is user a member of the Hyper-V Administrators group
INFO Does the Hyper-V virtual switch exist
#+end_src
4. pull secretを入力する

   初回起動時には、pull secretの入力を求められるのでバイナリと一緒にダウンロードしておいたjsonから情報を貼り付ける。
   #+begin_src bat
? Image pull secret [? for help] **********************************
INFO Loading bundle: crc_hyperv_4.2.0-0.nightly-2019-09-26-192831.crcbundle ...
INFO Creating CodeReady Containers VM for OpenShift 4.2.0-0.nightly-2019-09-26-192831...
INFO Verifying validity of the cluster certificates ...
INFO Adding 8.8.8.8 as nameserver to Instance ...
INFO Will run as admin: add dns server address to interface vEthernet (Default Switch)
INFO Check internal and public dns query ...
INFO Copying kubeconfig file to instance dir ...
INFO Adding user's pull secret and cluster ID ...
INFO Starting OpenShift cluster ... [waiting 3m]
INFO
INFO To access the cluster, first set up your environment by following 'crc oc-env' instructions
INFO Then you can access it by running 'oc login -u developer -p developer https://api.crc.testing:6443'
INFO To login as an admin, username is 'kubeadmin' and password is XXXXX-XXXXX-XXXXX-XXXXX
INFO
INFO You can now run 'crc console' and use these credentials to access the OpenShift web console
CodeReady Containers instance is running
#+end_src

   インストール直後は、一般ユーザである =developer= ユーザ(パスワードはdeveloper)と、
   管理者ユーザである =kubeadmin= ユーザの2種類のユーザが存在する。kubeadminユーザのパスワードは起動時に表示されるのでそれを見ておく。
*** ログイン
ログインは、CLIとWebコンソールのふたつがある。
**** CLI ログイン
OpenShiftでは、kubernetesでいうところの =kubectl= に相当する、 =oc= コマンドが存在する。
crcにもocコマンドは同梱されているので、以下のコマンドでパスの通しかたがわかる。

#+begin_src bat
λ crc oc-env
SET PATH=C:\Users\grugrut\.crc\bin;%PATH%
REM Run this command to configure your shell:
REM     @FOR /f "tokens=*" %i IN ('crc oc-env') DO @call %i %i
#+end_src

ocコマンドの場所にパスを通したら、あとはログインするだけである。

#+begin_src ba
λ oc login
Authentication required for https://api.crc.testing:6443 (openshift)
Username: kubeadmin
Password:
Login successful.

You have access to 51 projects, the list has been suppressed. You can list all projects with 'oc projects'

Using project "default".
#+end_src
**** Webコンソールログイン
OpenShiftには、はじめからブラウザ経由でアクセスできるWebコンソールが用意されているので、
そちらを使うことも多いだろう。
=crc console= コマンドを実行することで、ブラウザが起動し、Webコンソールにアクセスできる。

オレオレ証明書なので、そこは目をつぶってそのまま接続するとログイン画面が出てくる。

[[file:images/20191019-crc-login.png]]

kubeadminユーザでログインするときは、 =kube:admin= を、developerユーザでログインするときは、 =htpasswd_provider= を選択する。

ログインに成功すると、ダッシュボードが表示されるはずだ。

[[file:images/20191019-crc-dashboard.png]]
*** 初期設定

ここまでですぐにOpenShiftが使える状態ではあるが、ベータ版でさわってみてた感じ、以下の設定はやっておいたほうがよさそう。
- 監視機能の有効化
- ユーザの作成
**** 監視機能の有効化
ダッシュボードでクラスタのリソース状況が見れたり、Podの状況が見れる枠はあるものの、
デフォルトでは監視機能が無効化されているため、まったく意味をなしていない。

そこで、監視を有効化して、情報を収集できるようにしておく。

方法はドキュメントに書いてあるとおりで、以下のコマンドを順にCLIで実行すればよい。
ドキュメントだと、セミコロン区切りでまとめて書いてあるが、windowsの場合はセミコロンで複数コマンドを順番に実行する
ことができないので、ひとつずつ分割して実行する。

#+begin_src bat
λ oc scale --replicas=1 statefulset --all -n openshift-monitoring
statefulset.apps/alertmanager-main scaled
statefulset.apps/prometheus-k8s scaled

λ oc scale --replicas=1 deployment --all -n openshift-monitoring
deployment.extensions/cluster-monitoring-operator scaled
deployment.extensions/grafana scaled
deployment.extensions/kube-state-metrics scaled
deployment.extensions/openshift-state-metrics scaled
deployment.extensions/prometheus-adapter scaled
deployment.extensions/prometheus-operator scaled
deployment.extensions/telemeter-client scaled
#+end_src

しばらくすると、ダッシュボードに収集した値が表示されるようになるだろう。
ちなみに監視機能は結構メモリを消費するので、デフォルトの8GBだとメモリが足りなくて必要なPodを起動できず動かない問題が確認できている。
**** ユーザの追加
kubeadminユーザでWebコンソールにログインすると上の方で警告画面がでているところからもわかるとおり、
kubeadminユーザは一時的なユーザらしく、あまりこれを使うのは好ましくないらしい。
まあパスワードも覚えにくいし、適当に自分で作ったほうがよいだろう。

ログイン手段の作成方法もいくつかあるが、デフォルトで用意されているdeveloperユーザ用の
htpasswdに自分用のユーザを作成するのが楽だろう。

https://console-openshift-console.apps-crc.testing/k8s/ns/openshift-config/secrets/htpass-secret

にアクセスすると(もしくは左のメニューの =Workloads= の中の =Secrets= から、 =htpass-secret= を探すのもよい)、
ログイン用のhtpasswdが書かれたsecretの設定を見ることができる。
ここから、右上の =Actions= から =Edit Secret= を選択する。

htpasswdの設定を作成する方法はいくつかあるが、たとえば =WSL= 等のLinux環境がある場合は、
htpasswdコマンドを使えば簡単に作成できる。
今回は、私用に、grugrutユーザを作っている。
#+begin_src bash
$ sudo apt install apache2-utils
$ htpasswd -n grugrut
New password:
Re-type new password:
grugrut:XXXXXXXXXXXXXXXXXXXXXXXXXX
#+end_src

これで =oc login= してみると、作成したユーザでログインできるはず。
だが、これだけだと何もできないただログインできるだけのユーザなので、
クラスタ管理者の権限である =cluster-admin= ロールをバインドする。

kubeadminユーザで
https://console-openshift-console.apps-crc.testing/k8s/all-namespaces/rolebindings
にアクセスし、 =Create Binding= ボタンをクリック。

- =Binding Type= は、 =Cluster-wide Role Binding= を選択
- =Name= は、名前がかぶると怒られるので、 =cluster-admin-(作成するユーザ名)= とでもすればよい
- =Role Name= は、 =cluster-admin= を選択
- =Subject= は、 =User= を選び、先程作成したユーザ名を入れる

これで作成すれば、晴れてクラスタ管理者となれる。
ちなみにコマンドだと、
=oc adm policy add-cluster-role-to-user cluster-admin (ユーザ名)=
であり、もしかしたらこっちのほうが楽かもしれない。
***** kubeadminユーザの削除
自分自身をクラスタ管理者にしたら、もはやkubeadminユーザは不要なので消してしまってもよいはず。
ドキュメントにも消しかた書いてあるし。

https://docs.openshift.com/container-platform/4.2/authentication/remove-kubeadmin.html

kubeadminユーザを削除することによって、Webコンソールへのログイン時に、「kube:admin」か「htpasswd」なのか
選ばなくてよくなるので、ユーザを作ったあとは消してしまってよいかもしれない。
*** まとめ
これで家の環境でOpenShiftが使えるようになったので、今後コンテナ動かすところなども見ていきたい。
*** 注意点
今のところバグで、30日で証明書が期限切れになり、起動できなくなってしまうらしい。
解決策はなく、一度削除して(当然作成したものも消える)、作りなおす必要があるとか。
おそろしい話である。
** DONE emacs26からの行番号表示がddskk入力時にガタつくのを防ぐ    :emacs:skk:
   CLOSED: [2019-10-20 Sun 23:51]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2019\" \"2019-10\")
:EXPORT_FILE_NAME: 201910202227
:END:
Emacs26までは、行番号の表示をemacs lispで頑張るというアプローチがために、
重たくてなかなか使いどころに困る問題があった。
それに対してEmacs26では、待望のCで書かれた組み込みの行番号表示である、 =display-line-numbers-mode= が導入された。
これは軽くてたいへん便利なのであるが、使っていて、ひとつめちゃくちゃ気になる問題があった。

それはごらんの通り、ddskkで日本語を入力するときに行番号の表示がずれて、がたがたとなり見辛いのである。


[[file:images/display-line-numbers-mode-gatagata.gif]]

これには困っていたのだけど、言語化しづらいところもあり解決策が見付けられなかったが、
ソースコード見てパラメータいじってたら以下のパラメータを有効化することで
がたつかなくなることがわかった。

#+begin_src lisp
(global-display-line-numbers-mode t)
(custom-set-variables '(display-line-numbers-width-start t))
#+end_src

先程のgifと見比べてみると今度はまったくがたつきがないのがわかる。

[[file:images/display-line-numbers-mode-not-gatagata.gif]]

今のところ、この設定で困ったことはなく、強いて言えば、
見比べてみるとわかるが、はじめから行番号のスペースが広めにとられてしまっている。
そのため、少し画面サイズは小さくなっているものの、これで快適な行番号生活をおくることができる。
** DONE Google Cloud Certified - Professional Cloud Architect 認定試験に合格した :gcp:certification:
   CLOSED: [2019-12-29 Sun 10:11]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2019\" \"2019-12\")
:EXPORT_FILE_NAME: 201912290846
:END:
タイトルの通りですが、12月の中旬ぐらいにGCPのProfessional Cloud Architectの試験を受け、無事に一発で合格しました。

パブリッククラウドは業務で若干使うぐらいで、どちらかというと試験を通じてクラウドについて学ぼうというのがモチベーションでした。
AWSとかAzureの試験も受けたことなく体験記を見る限り、GCPは単にGCPの使い方がわかればよいというより、
デプロイ戦略とかkubernetesの使い方とか、そういう一般的知識も求められる(後述のcouseraでGCPの人もそう言ってた)。
そちらについてはけっこうケイパビリティあるつもりなので、本当にGCPのサービスについてきちんとおさえてから挑みました。

勉強期間としては2週間ぐらい。
基本的には、courseraの公式の教材で学習しました。

[[https://www.coursera.org/learn/preparing-cloud-professional-cloud-architect-exam-jp][Preparing for the Google Cloud Professional Cloud Architect Exam 日本語版]]

試験勉強を通じて、これまで使ってなかったパブクラの機能もいろいろとわかってきたので、
今後も公私ともにもっと活用していきたいですね。

* 2020
** DONE org modeのファイルをパースする                            :emacs:org:
   CLOSED: [2020-01-10 Fri 08:56]
   :PROPERTIES:
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-01\")
   :EXPORT_FILE_NAME: 202001100849
   :END:

   やりたいことがあって、inbox.orgをパースして、個々のノードの情報を得たかった。
   ざっと以下のコマンドでいけることがわかった。
   とりあえず動作確認は、 ~M-:~ でさくっと確認しただけだけど。

   #+begin_src 
(org-map-entries (lambda() (princ (org-entry-properties))))
   #+end_src

   =org-map-entries= が、条件にあうノードに対してmap関数を適用するための関数。
   =org-entry-properties= が、個々のノードのプロパティの連想リストを取得する関数。てっきりプロパティドロワーにあるものだけ抽出するのかと思ってたらアイテム名とか、TODO状態とか全部取れてるっぽいので、これベースにごにょごにょすればいい感じにいけそう。
** DONE ergodox ezを購入した                                        :ergodox:keyboard:
   CLOSED: [2020-05-18 Mon 23:11]
   :PROPERTIES:
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-05\")
   :EXPORT_FILE_NAME: 202005182209
   :END:
   今さらながら、分割キーボード界ではおなじみの、ergodox ezを購入してみた。
   動機としては、自分も昨今の事情でテレワークしていて、
   家ではHappy Hacking Proを使ってたのだけど、背中がつらくなってきたのと、
   もろもろオフとか出て自作キーボードに興味があったから。
*** Ergodox ezにした理由
    自分が望んでいるものは何か考えたところ、以下だったので、レデイーメイドなErgodox EZにまずはチャレンジすることにした。

    - JIS配列を愛用してるので、ある程度キー数が多いこと
    - emacsでハイパーキー、super キーを使ってるので親指でmodifier keyをいろいろ使えること
    - 分割キーボードであること
    - いったんは、はんだ付けなしで沼への第一歩をふみだせること

    ちなみに、職場では、今は亡き、Barrocoの日本語配列を使っている。
    [[https://www.archisite.co.jp/products/mistel/barocco-jp/]]

    この子もいいこだし、マクロでいろいろできるのは判ってるけど、やっぱりかゆいところに手が届かないのがつらかった。

    正直、今回のは、自分が今後沼れるのかどうか、試金石的な要素がつよいかも。

*** 購入方法
    特に既存のググった結果と変わらないので割愛。
    5/3に注文して、5/18に受け取ったので、賞味2週間でうけとってる。思った以上に早いね。
    ちなみに、注文したモデルは白色・無刻印。軸は赤軸にしてみました。
    そのうち、キーキャップを別途購入して、よりオシャンティーにしていきたいですね。

    [[file:images/20200518_ergodox_0.jpg]]

    ポインティングデバイスは、人差し指トラックボールを使ってるので、配置はこんな感じにしてみました。

    [[file:images/20200518_ergodox_1.jpg]]

*** キー配列

    先述のとおり、普段からJIS配列を愛用していて、記号の位置など、できるだけ踏襲したかったので、
    それ用にキーマップを書いた。

    https://github.com/grugrut/qmk_firmware/blob/b639d036d4c76b0d9b71a431dd92a8a69a0fd234/keyboards/ergodox_ez/keymaps/grugrut/keymap.c

    基本的には、まずは、kinesisキーボードの日本語配列をベースとしている。

    届いて真っ先に、この自分でビルドしたHexファイルを焼き込もうとしたのだけど、
    qmk toolboxだとリセット後のデバイスを認識できずにビビるなどした。
    その後、teensyに切り替えたら、普通に焼き込めて一安心。

*** 一時間程度さわってみての所感
    - Colomn Stuggered配列に慣れない。特に一段目のキーのタイポが多い

    - 親指の修飾キーの奥の方が意外と押しにくい
      - 自分、そんなに手も小さいほうじゃないので、いけると思ってたら、意外とつらかった

    まあ、これは触りながら、適宜キー配列を変えていって慣らすしかなさそうですね。
** DONE CKA(Certified kubernetes Administrator)に合格した        :kubernetes:certification:
   CLOSED: [2020-07-07 Tue 09:40]
   :PROPERTIES:
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-07\")
   :EXPORT_FILE_NAME: 202007070839

   :END:
   Kubernetesの管理者向け資格であるCertified Kubernets Administratorを受験して
   無事に合格したので、合格体験記はすでに巷にいくらでもあるが、せっかくなのでメモ。
*** CKAとは
    Linux Foundationが管理している、kubernetesの認定試験。
    Kubernetesの操作やkubernetes自体の管理について問われる。
    よくある4択問題のようなテストではなく、実際にkubernetesクラスタを操作する実技試験。

    試験時間は3時間で、24問。問題によって得点は異なり、74%以上で合格。

*** バックグラウンド
    kubernetes歴は15ヶ月ぐらい。うち、ほとんどはOpenShiftだったので、
    純粋にkubernetesを触っているのは、半年ぐらいか。
    自宅で、kubeadmを使って仮想マシンだったりラズパイおうちクラスタだったり作ってたので、
    k8sのインストールは慣れてた。
*** 試験に役立ったもの
**** Udemyのコース
     他の人の結果を見て、以下のUdemyのコースがよさそうだったので、こちらでやった。
     これ書いてる今もそうだけど、しょっちゅうセールしてて、元の価格はなんなんだ。。。ってなりがち。

     [[https://px.a8.net/svt/ejp?a8mat=3BHUM0+C673K2+3L4M+BW8O2&a8ejpredirect=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fcertified-kubernetes-administrator-with-practice-tests%2F][Certified Kubernetes Administrator (CKA) with Practice Tests]]

     動画と演習がセットになってて、最初はマジメに動画を見てたけど、途中で飽きてしまったので演習だけやったようなもん。

**** Ergodox EZ
     試験問題では、 abcってPodを作ってください、みたいな問題が出てくる。当然、確実に作成するためにコピペしたいのだけど、
     試験はブラウザ上のアプリ(katacodaとかCloudShell的な)でおこなう。
     ブラウザなので、コピペは =Ctrl-C/Ctrl-V= ではできない。Windowsの場合は、 =Ctrl-Insert/Shift-Insert= でおこなう。
     正直、Insertキーなんて普通のキーボードでは使い勝手の良いところにないと思う。
     自分は、 [[ergodox ezを購入した]] の通り、Ergodox EZを使っていたので、Insertキーを =Lower-I= にバインドしていたので
     手をホームポジションから移すことなく、スムーズにペーストすることができて、自作キーボード万歳!って思った。

     そうは言っても、そもそもペーストが、 =Shift-Insert= ってことに慣れてないので一週間ぐらいは、普段から意識して
     ペーストをこちらのキーバインドでおこなうようにしていた。
     今回初めて知ったのだけど、これ、別に特殊なキーバインドじゃなくて、他のWindowsアプリでもこれでペーストできるのね。

*** 試験
    体験記を見ると、貸し会議室で受験した人が多かったけど
    - 貸し会議室のWifiの品質やポートブロックが心配だった
    - ノートPCの小さいディスプレイで頑張れる自信がなかった
    - そもそも、最近ノートPCの調子が悪くトラブルが怖かった
    などの理由により、自宅で受けることにした。

    机の横に本棚があるので心配だったが、受験サイトでチャットができ、問題ないか聞いてみたところ
    「大丈夫だけど、もしかしたら布でかくせって言われるかもね〜」とのことだったので、
    事前に布をかけておいた。当日はなにも言われなかったので多分それでよいのでしょう。

    ちなみに、数々の合格体験記ではGoogle翻訳プラグインはOKだったって書かれてたけど
    自分の場合はダメって言われてしまった。

*** 結果
    93%だった。一応全問問いたものの、7%の問題だけ挙動が怪しかったので、たぶんそれのやりかたが間違ってたのだと思う。
    部屋の綺麗さを保ててるうちに、CKADも取ってしまいたいので、さっそく今日から勉強再開だ。
** DONE CKAD(Certified Kubernetes Application Developer)に合格した :kubernetes:certification:
   CLOSED: [2020-07-11 Sat 10:56]
   :PROPERTIES:
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-07\")
   :EXPORT_FILE_NAME: 202007111027
   :END:

   [[CKA(Certified kubernetes Administrator)に合格した]] の勢いで、4日後にCKADも受験し、
   無事に合格したのでメモ

*** CKADとは
    Linux Foundationが管理している、kubernetesの認定試験。
    CKAと異なり、kubernetesの操作のみでkubernetesの管理については問われない。
    よくある4択問題のようなテストではなく、実際にkubernetesクラスタを操作する実技試験。

    試験時間は2時間で、19問。問題によって得点は異なり、66%以上で合格。

*** 試験準備
    CKAの試験対策でUdemyの講座がよかったので、Udemyのコースで勉強した。

    [[https://px.a8.net/svt/ejp?a8mat=3BHUM0+C6720I+3L4M+BW8O2&a8ejpredirect=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fcertified-kubernetes-application-developer%2F][Kubernetes Certified Application Developer (CKAD) with Tests]]

    CKAの勉強をしていれば、CKAD用の準備はいらないと聞いていたので、最後のLightning TestとMock Examをメインでやった。
    Lightningの方の問題が時間がかかるものが多く、1問5分で解くって無理っしょ、、、と思ったあとにMock Examは簡単だったのでほっとした。

    CKAから中3日での登板なので、できたことといえばこんなもん。

    あとは、試験に耐えれる室内環境を維持するため、エントロピーの低い暮らしをこころがけた(笑)

*** 試験受けてみて  
    CKADのほうが難しいと感じた。試験に合格するという観点で言ったらCKADの方が求められる点数が低いので合格しやすいと思うが、
    問題の最大難易度はCKADの方が難しい。試験準備のとおり、結構余裕かまして受験に臨んだので、1問目がMAX難しい問題で結構あせった。
    より正確には、難しいというか制限時間の割に必要な設定数が多い問題が多かった。

    また、CKAに比べて日本語がこなれてない(というか破綻してる)ものがいくつかあり、
    英語と見比べながら問われてることを理解する必要もあり、そこでも時間がとられてしまった。

    結局ひととおり解くのに90分ぐらいかかってしまい、30分しか見直しの時間がとれず、見直し途中でタイムオーバーに。

*** 結果
    96%だった。おそらく何聞かれてるんだか明確でない問題が1問あり、ま、こんなもんだろで回答したものが1つあったので、それだと思う。
    CKA、CKAD両方受けてみて、これまでの知識の棚卸しができてよかったと思う。
    これ取ったから何というわけではないので、これをステップにより知識を高めていきましょう。
** DONE CRI-O + Kata containers + Weavenetでkubernetesをインストールする :kubernetes:
   CLOSED: [2020-07-12 Sun 09:33]
   :PROPERTIES:
   :EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-07\")
   :EXPORT_FILE_NAME: 202007111344
   :ID:       bfbf1ff9-e413-4c60-875d-e9851efbe2ef
   :END:
普段はCRIはDocker、OCIはrunc、CNIはcalicoで構成することが多いのだけど、たまには違う構成でもとってみようと思いインストールしてみる。
特にこれまでKata containersはさわったことなかったので。
OSはUbuntuを適当に入れた

*** Kataのインストール
https://github.com/kata-containers/documentation/blob/master/install/ubuntu-installation-guide.md

#+begin_src bash
ARCH=$(arch)
BRANCH="${BRANCH:-master}"
sudo sh -c "echo 'deb http://download.opensuse.org/repositories/home:/katacontainers:/releases:/${ARCH}:/${BRANCH}/xUbuntu_$(lsb_release -rs)/ /' > /etc/apt/sources.list.d/kata-containers.list"
curl -sL  http://download.opensuse.org/repositories/home:/katacontainers:/releases:/${ARCH}:/${BRANCH}/xUbuntu_$(lsb_release -rs)/Release.key | sudo apt-key add -
sudo -E apt-get update
sudo -E apt-get -y install kata-runtime kata-proxy kata-shim
#+end_src

*** CRI-Oのインストール
https://github.com/cri-o/cri-o#installing-cri-o

#+begin_src bash
. /etc/os-release
sudo sh -c "echo 'deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/x${NAME}_${VERSION_ID}/ /' > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list"
wget -nv https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/x${NAME}_${VERSION_ID}/Release.key -O- | sudo apt-key add -

sudo apt-get update -qq
apt-get install -y cri-o-1.17
sudo systemctl enable crio
#+end_src

Ubuntuのパッケージは、1.18がまだ無いようなので1.17を利用した。

*** CRI-Oのランタイムの設定
https://github.com/kata-containers/documentation/blob/master/how-to/run-kata-with-k8s.md#cri-o

/etc/crio/crio.conf に書かれている設定を入れた。
デフォルトはruncのままにしてある。

#+begin_src 
[crio.runtime.runtimes.kata-runtime]
  runtime_path = "/usr/bin/kata-runtime"
  runtime_type = "oci"
#+end_src

*** kubernetesのインストール

kubeadmでインストール。

全ノードで
#+begin_src bash
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
sudo sysctl --system

sudo apt-get update && sudo apt-get install -y apt-transport-https curl
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get update
sudo apt-get install -y kubelet=1.17.0-00 kubeadm=1.17.0-00 kubectl=1.17.0-00
sudo apt-mark hold kubelet kubeadm kubectl

cat <<EOF | sudo tee /etc/systemd/system/kubelet.service.d/0-crio.conf
[Service]
Environment="KUBELET_EXTRA_ARGS=--container-runtime=remote --cgroup-driver=systemd --runtime-request-timeout=15m --container-runtime-endpoint=unix:///var/run/crio/crio.sock"
EOF
sudo systemctl daemon-reload
sudo systemctl restart kubelet
#+end_src

コントロールプレーンで以下を実行。
#+begin_src bash
sudo kubeadm init --skip-preflight-checks --cri-socket /var/run/crio/crio.sock --pod-network-cidr=10.244.0.0/16
#+end_src

実行後には、joinコマンドが表示されるので、今度はそれを各ノードで実行する。もし、見逃してしまった場合は、以下のコマンドで再表示できる。

#+begin_src bash
kubeadm token create --print-join-command
#+end_src

前に入れたときは、CNIプラグイン入れないとNodeの状態がREADYにならなかったはずなのに、
今回試したらNodeが参加した時点でREADYになってた。ランタイムが違うから？そんなことある？

とりあえず、WeaveNetをいれておく。

#+begin_src bash
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
#+end_src

*** クラスタのテスト
OCIとして、runcを使うPodとkataを使うPodをデプロイしてみる

#+begin_src bash
kubectl run hello-runc --image=gcr.io/google-samples/hello-app:1.0 --restart Never
cat <<EOF | kubectl apply -f -
apiVersion: node.k8s.io/v1beta1
kind: RuntimeClass
metadata:
  name: kata
handler: kata-runtime
EOF
kubectl get pod hello-runc -o yaml > hello-kata.yaml
#+end_src

hello-kata.yamlを以下の通り編集
#+begin_src yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: hello-kata
  name: hello-kata
spec:
  containers:
  - image: gcr.io/google-samples/hello-app:1.0
    imagePullPolicy: IfNotPresent
    name: hello-kata
  dnsPolicy: ClusterFirst
  restartPolicy: Never
  runtimeClassName: kata
#+end_src

これを流したんたけどPodが起動しない。eventを見てみると以下のようなログが。

#+begin_src
Failed to create pod sandbox: rpc error: code = Unknown desc = container create failed: failed to launch qemu: exit status 1, error messages from qemu log: Could not access KVM kernel module: No such file or directory
qemu-vanilla-system-x86_64: failed to initialize kvm: No such file or directory
#+end_src

今回ESXi上の仮想マシンでやったのだけど、CPUの仮想化を有効にするの忘れてた。仮想マシンの設定変更から、
「CPU仮想化 ハードウェア アシストによる仮想化をゲストOSに公開」を有効にしたところ解決。

#+begin_src
kubectl get pod -o wide
NAME         READY   STATUS    RESTARTS   AGE   IP          NODE    NOMINATED NODE   READINESS GATES
hello-kata   1/1     Running   0          9h    10.32.0.2   node1   <none>           <none>
hello-runc   1/1     Running   0          9h    10.38.0.3   node2   <none>           <none>
#+end_src

無事に起動したっぽい。

**** 動作を見比べる
うまいことnode1とnode2に分散してPodを動かしたので、通常のruncで動くパターンとkataで動くパターンのプロセス構成などを見てみる。

***** kata-runtime list
kataで動いているコンテナのリストは、 =kata-runtime list= で確認することができる。

- Node1 (kata利用)
#+begin_src
$ sudo kata-runtime list
ID                                                                 PID         STATUS      BU
NDLE                                                                                                                 CREATED                          OWNER
fa157caa041230c1593ced717618dc2f96a80f4c0704b7d965421a8e95dc791f   2850        running     /run/containers/storage/overlay-containers/fa157caa041230c1593ced717618dc2f96a80f4c0704b7d965421a8e95dc791f/userdata   2020-07-11T23:51:20.244499159Z   #0
4fe1ddb9154cbfc14a7ca514e2705b91f54bfc9b89300c940ff1000b2f0bd17c   3115        running     /run/containers/storage/overlay-containers/4fe1ddb9154cbfc14a7ca514e2705b91f54bfc9b89300c940ff1000b2f0bd17c/userdata   2020-07-11T23:51:26.190503017Z   #0
#+end_src

- Node2 (runc利用)
#+begin_src
$ sudo kata-runtime list
ID          PID         STATUS      BUNDLE      CREATED     OWNER
#+end_src

たしかに、Node1では動いているプロセスがいて、Node2にはいないことがわかる。
でも、なんで2つ？ Podはひとつしか起動してないのに。

もう少しNode1側を詳しく見てみる。

#+begin_src
$ sudo kata-runtime state fa157caa041230c1593ced717618dc2f96a80f4c0704b7d965421a8e95dc791f
{
  "ociVersion": "1.0.1-dev",
  "id": "fa157caa041230c1593ced717618dc2f96a80f4c0704b7d965421a8e95dc791f",
  "status": "running",
  "pid": 2850,
  "bundle": "/run/containers/storage/overlay-containers/fa157caa041230c1593ced717618dc2f96a80f4c0704b7d965421a8e95dc791f/userdata",
  "annotations": {
    "io.katacontainers.pkg.oci.bundle_path": "/run/containers/storage/overlay-containers/fa157caa041230c1593ced717618dc2f96a80f4c0704b7d965421a8e95dc791f/userdata",
    "io.katacontainers.pkg.oci.container_type": "pod_sandbox"
  }
}
$ sudo kata-runtime state 4fe1ddb9154cbfc14a7ca514e2705b91f54bfc9b89300c940ff1000b2f0bd17c
{
  "ociVersion": "1.0.1-dev",
  "id": "4fe1ddb9154cbfc14a7ca514e2705b91f54bfc9b89300c940ff1000b2f0bd17c",
  "status": "running",
  "pid": 3115,
  "bundle": "/run/containers/storage/overlay-containers/4fe1ddb9154cbfc14a7ca514e2705b91f54bfc9b89300c940ff1000b2f0bd17c/userdata",
  "annotations": {
    "io.katacontainers.pkg.oci.bundle_path": "/run/containers/storage/overlay-containers/4fe1ddb9154cbfc14a7ca514e2705b91f54bfc9b89300c940ff1000b2f0bd17c/userdata",
    "io.katacontainers.pkg.oci.container_type": "pod_container"
  }
}
#+end_src

コンテナタイプが違うのがわかる。公式のドキュメントのアーキテクチャのところを見ると、
pod_sandboxの中に、pod_containerがあるようだ。

https://github.com/kata-containers/documentation/blob/master/design/architecture.md

#+begin_src
$ sudo kata-runtime exec 4fe1ddb9154cbfc14a7ca514e2705b91f54bfc9b89300c940ff1000b2f0bd17c ps
PID   USER     TIME   COMMAND
    1 root       0:00 ./hello-app
   28 root       0:00 ps
$ sudo kata-runtime exec fa157caa041230c1593ced717618dc2f96a80f4c0704b7d965421a8e95dc791f ps
rpc error: code = Internal desc = Could not run process: container_linux.go:349: starting container process caused "exec: \"ps\": executable file not found in $PATH"
#+end_src
pod_contaierの方で、期待するアプリが動いていることが確認できた。sandboxのほうは、shすら起動できなかったので、何が動いているんだろうか。

***** psの結果
プロセスツリーも見比べてみた。適当にプロセスは実際のものから削っている。

- Node1 (kata利用)
#+begin_src
systemd-+-2*[conmon-+-pause]
        |           `-{conmon}]
        |-conmon-+-kube-proxy---7*[{kube-proxy}]
        |        `-{conmon}
        |-conmon-+-kube-utils---8*[{kube-utils}]
        |        |-launch.sh---weaver---15*[{weaver}]
        |        `-{conmon}
        |-conmon-+-kata-proxy---8*[{kata-proxy}]
        |        |-kata-shim---8*[{kata-shim}]
        |        |-qemu-vanilla-sy---3*[{qemu-vanilla-sy}]
        |        `-{conmon}
        |-conmon-+-weave-npc-+-ulogd
        |        |           `-9*[{weave-npc}]
        |        `-{conmon}
        |-conmon-+-kata-shim---10*[{kata-shim}]
        |        `-{conmon}
        |-crio---14*[{crio}]
        |-kubelet---16*[{kubelet}]
        `-lxcfs---2*[{lxcfs}]
#+end_src

- Node2 (runc利用)
#+begin_src
systemd-+-3*[conmon-+-pause]
        |           `-{conmon}]
        |-conmon-+-kube-proxy---8*[{kube-proxy}]
        |        `-{conmon}
        |-conmon-+-kube-utils---8*[{kube-utils}]
        |        |-launch.sh---weaver---16*[{weaver}]
        |        `-{conmon}
        |-conmon-+-weave-npc-+-ulogd
        |        |           `-9*[{weave-npc}]
        |        `-{conmon}
        |-conmon-+-hello-app---3*[{hello-app}]
        |        `-{conmon}
        |-crio---14*[{crio}]
        |-kubelet---16*[{kubelet}]
        `-lxcfs---2*[{lxcfs}]
#+end_src

見比べてみると、たしかにruncだと目的のhello-appが直接動いているのに対して、
kataの場合は、hello-appは直接ホストから見えない。
kata-shimで隠蔽されていて、隔離された環境で動いていることがわかる。

**** まとめ
Kata Containersは、これまで安全にコンテナ実行するために使う、ぐらいしか聞いておらず
どういう風に動くのかよくわかっていなかったが、今回構築してみてその動きが理解できた。
構築も、ドキュメントによって書いてあること違ったりでいくつかトラブルところもあったが、
だいたいログ見たらどこがあやしいかわかるし、それほど苦労することはなかった。
1枚噛んでるレイヤが増えるので、性能面とリソースのオーバーヘッドが気になるので、今後その辺見てみたい。
** DONE Tektonをさわってみた                              :kubernetes:tekton:
   CLOSED: [2020-07-19 Sun 12:25]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-07\")
:EXPORT_FILE_NAME: 202007191122
:END:

kubernetesで動かすCI/CDツールとして、聞いてはいたものの、これまでぜんぜんさわれてなかったtektonをちょっとだけさわってみた。

https://tekton.dev/

タスクやパイプラインがCRDとして定義されているので、ぜんぶフォーマットを統一できるのがよさそう。

*** インストール
https://github.com/tektoncd/pipeline/blob/master/docs/install.md
にしたがって実施。
#+begin_src bash
kubectl apply --filename https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml
#+end_src

=tekton-pipeline= namespaceができてるのでPodを確認。
#+begin_export
NAME                                           READY   STATUS    RESTARTS   AGE
tekton-pipelines-controller-559bd4d4df-9rwjl   1/1     Running   0          54s
tekton-pipelines-webhook-7bfd859f8c-mzc2n      1/1     Running   0          54s
#+end_export

ビルド成果物を格納するためにPersistentVolumeの設定をする。S3やGoogleCloudStorageのような、クラウドストレージも利用できるようだ。
=config-artifact-pvc= がすでにできていて、StorageClassやVolumeのサイズを設定できるようだ。
今回は、デフォルト値で動かすことに。

また、tekton cliもインストールしておく。kubectlのプラグインになるようにシンボリックリンクで、kubectl-xxxのファイルを作成する。
https://github.com/tektoncd/cli

#+begin_src bash
sudo ln -s /usr/bin/tkn /usr/local/bin/kubectl-tkn
#+end_src

*** チュートリアルの実施
https://github.com/tektoncd/pipeline/blob/master/docs/tutorial.md

#+begin_src bash
cat <<EOF | kubectl apply -f -
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: echo-hello-world
spec:
  steps:
    - name: echo
      image: ubuntu
      command:
        - echo
      args:
        - "Hello World"
EOF

cat <<EOF | kubectl apply -f -
apiVersion: tekton.dev/v1beta1
kind: TaskRun
metadata:
  name: echo-hello-world-task-run
spec:
  taskRef:
    name: echo-hello-world
EOF

kubectl tkn taskrun logs echo-hello-world-task-run
#+end_src

=Task= と =TaskRun= があり、Taskは実際にやることを書き、実行するにはTaskRunを作成する、と。

*** まとめ
いったんインストールとタスクの定義、その実行まで見てみた。
これだけだとCI/CDツールっぽさがないので、パイプラインはこのあと見ていく予定。

やりました。 [[Tektonでパイプラインを動かす]]

** DONE Tektonでパイプラインを動かす                      :tekton:kubernetes:
   CLOSED: [2020-07-24 Fri 15:47]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-07\")
:EXPORT_FILE_NAME: 202007231454
:END:

[[Tektonをさわってみた]] のつづき

簡単なパイプラインをくんで動かしてみた。

*** 作るもの

Goで作ったシンプルなWebサーバのアプリ。8080ポートでListenしてて、アクセスするとホスト名を返してくれるだけのやつ。

これを、githubからpullしてきて、ビルドしてイメージ化してpushするだけのシンプルなパイプラインを作る。

以下の通り、ソースコードとパイプライン含め、githubに配置している。

https://github.com/grugrut/go-web-hello

*** タスクを作る

パイプラインは、複数のタスクを順番に実行していくものなので、パイプラインの前にタスクを作る必要がある。

もちろんタスクを前回のように、自分で定義するのもよいが、TektonではCatalogというリポジトリに
いろいろな人が作ったTaskが公開されているので、これを使うのが簡単。

https://github.com/tektoncd/catalog/

この中から、githubからソースコードを取得するのに =git-clone= 、
goのビルドをするのに =golang-build= 、コンテナイメージを作成してDockerHubにpushするのに
=buildah= を利用してみた。

*** パイプラインを定義する

パイプラインも他のリソースと同様に、yamlで定義する。

#+begin_src yaml
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata:
  name: go-web-hello-pipeline
spec:
  workspaces:
    - name: shared-data
  tasks:
    - name: fetch-repo
      taskRef:
        name: git-clone
      workspaces:
        - name: output
          workspace: shared-data
      params:
        - name: url
          value: https://github.com/grugrut/go-web-hello.git
    - name: build
      taskRef:
        name: golang-build
      runAfter:
        - fetch-repo
      params:
        - name: package
          value: github.com/grugrut/go-web-hello
        - name: packages
          value: ./...
      workspaces:
        - name: source
          workspace: shared-data
    - name: docker-build
      taskRef:
        name: buildah
      runAfter:
        - build
      params:
        - name: IMAGE
          value: grugrut/go-web-hello
      workspaces:
        - name: source
          workspace: shared-data
#+end_src

パイプラインのspecには大きくふたつの定義をおこなう。

- workspaces
  各タスクでの作業領域。同じ名前のワークスペースを使うことでタスク間で中間成果物を受け渡すことができる。
  実体としては、Podにvolumeがマウントされる。具体的なvolumeの定義は実行時におこなう。
- tasks
  具体的なタスク群を記載していく。
  今回は3つのタスクを実行するが、具体的な定義内容は以下の通り。

**** GitHubからソースコードをクローン

#+begin_src yaml
    - name: fetch-repo
      taskRef:
        name: git-clone
      workspaces:
        - name: output
          workspace: shared-data
      params:
        - name: url
          value: https://github.com/grugrut/go-web-hello.git
#+end_src

=git-clone= タスクを利用した。outputのワークスペースにソースコードをcloneして、次のタスクに渡すことができる。
今回はリポジトリのurlしか指定していないが、ブランチ名を指定することなどももちろんできる。

**** Goのソースをビルド

#+begin_src  yaml
    - name: build
      taskRef:
        name: golang-build
      runAfter:
        - fetch-repo
      params:
        - name: package
          value: github.com/grugrut/go-web-hello
        - name: packages
          value: ./...
      workspaces:
        - name: source
          workspace: shared-data
#+end_src

=golang-build= タスクを利用した。sourceのワークスペースに対して、 =go build -v $(packages)= をしてくれる。
また、ソースコードのcloneがおこなわれてから実行されるように、 runAfterで指定している。
これがないと並列にタスクが実行されてしまい、うまくいかないはず。

昔に、Jenkinsでgoのビルドをしたことがある(http://grugrut.hatenablog.jp/entry/2017/04/10/201607)が、
=GOPATH= のあつかいが面倒で、withEnvとか駆使しないといけなかった。
Tektonの場合、それぞれのタスクごとにPodがわかれていて、 =GOPATH= も設定ずみのところにソースが配置されるように
あらかじめ設定されているので、まったく気にすることなくビルドできて便利だと思った。

**** コンテナイメージのビルドとPush

#+begin_src yaml
    - name: docker-build
      taskRef:
        name: buildah
      runAfter:
        - build
      params:
        - name: IMAGE
          value: grugrut/go-web-hello
      workspaces:
        - name: source
          workspace: shared-data
#+end_src

=buildah= タスクを利用した。何も指定しないと、workspace直下のDockerfileでビルドして、
イメージ名にもとづき、イメージのpushまでをおこなってくれる。

そのため、たとえばDockerHubのような認証が必要な場合は、事前に認証情報を作成しておく。

#+begin_src bash
kubectl create secret generic basic-user-pass --type kubernetes.io/basic-auth --from-literal username=user --from-literal password=pass
kubectl annotate secrets basic-user-pass tekton.dev/docker-0=https://docker.io
#+end_src

=tekton.dev/docker-0= のアノテーションをつけておくことで、docker pushするときの認証として使われるようになる。

あとは、これがパイプラインが動作する際に利用されるように、ServiceAccountを作成しておく。

#+begin_src yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: build-bot
secrets:
  - name: basic-user-pass
#+end_src

*** パイプラインを実行する
パイプラインを実行する場合は、 =PipelineRun= のリソースを作成する。またこの際に実行するパイプラインの情報をいろいろとつける。

#+begin_src bash
cat <<EOF | kubectl create -f -
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: go-web-hello-pipeline-
spec:
  pipelineRef:
    name: go-web-hello-pipeline
  serviceAccountName: build-bot
  workspaces:
    - name: shared-data
      volumeClaimTemplate:
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 1Gi
          storageClassName: nfs-client
EOF
#+end_src

今回はgenerateNameを使っているので、 =apply= ではなく、 =create= していることに注意。
PipelineRunでは、どのパイプラインを実行するかとserviceAccountとworkspaceとして利用するvolumeの情報を書いている。

*** パイプラインの結果を見る
パイプラインのタスクはPodとして動くが、 =tkn= コマンドでよりわかりやすく見ることができる。

- パイプラインの実行結果の一覧を見る

  =tkn pipelinerun list=

  #+begin_src
$ tkn pr list
NAME                              STARTED       DURATION     STATUS             
go-web-hello-pipeline-c9dhn       3 hours ago   59 seconds   Succeeded          
go-web-hello-pipeline-xx5qb       1 day ago     55 seconds   Failed
  #+end_src

- パイプラインの実行結果の詳細を見る

  =tkn pipelinerun describe xxxx=

  #+begin_src
$ tkn pr describe go-web-hello-pipeline-c9dhn
Name:              go-web-hello-pipeline-c9dhn
Namespace:         default
Pipeline Ref:      go-web-hello-pipeline
Service Account:   build-bot

??  Status

STARTED       DURATION     STATUS
3 hours ago   59 seconds   Succeeded

? Resources

 No resources

? Params

 No params

?  Taskruns

 NAME                                               TASK NAME      STARTED       DURATION     STATUS
 ・ go-web-hello-pipeline-c9dhn-docker-build-mpht6   docker-build   3 hours ago   40 seconds   Succeeded
 ・ go-web-hello-pipeline-c9dhn-build-x72xl          build          3 hours ago   11 seconds   Succeeded
 ・ go-web-hello-pipeline-c9dhn-fetch-repo-nmkx7     fetch-repo     3 hours ago   8 seconds    Succeeded
  #+end_src
- パイプラインの実行時のログを見る(=-f= オプションをつけることで実行中でも見れる)

  =tkn pipeline log xxxxx=

*** まとめ

Tektonを使ってパイプラインを実行することができた。CI/CDというには、実行のトリガーのところとか、デプロイのところができてないので、
次はそのへんを見ていく予定。(コンテナイメージもlatestタグになってて、超微妙だし。。。)
** DONE インストール後にkube-proxyの動作モードをIPVSモードに変更する :kubernetes:
   CLOSED: [2020-07-27 Mon 23:25]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-07\")
:EXPORT_FILE_NAME: 202007272308
:END:

今、家で使っているKubernetesクラスタについて、インストール時に気にかけておらず、
=kube-proxy= が =iptables= モードで動いているのでは？ と思ったので、確認して =ipvs= モードに変更してみた。

ちなみに、たしかにipvsモードのほうがパフォーマンスに優れると言われてはいる。
しかしながら、Calicoでおなじみのtigeraによると、大規模になれば違いはでてくるが、
100程度のオーダーでは違いは無いらしい。もはや、ただの自己満である。

https://www.tigera.io/blog/comparing-kube-proxy-modes-iptables-or-ipvs/
*** 現状確認
設定見ればすぐだが、ログを見ても動作確認はできる。
#+begin_src 
$ kubectl -n kube-system logs kube-proxy-6vvrf kube-proxy
(trim)
W0711 06:32:25.413300       1 server_others.go:324] Unknown proxy mode "", assuming iptables proxy
I0711 06:32:25.418063       1 server_others.go:145] Using iptables Proxier.
I0711 06:32:25.418401       1 server.go:571] Version: v1.17.8
(trim)
#+end_src

未設定なので、iptagblesモードで動くよとばっちり出ている。
*** ipvsモードに修正
=kubectl -n kube-system edit configmaps kube-proxy= して、
=mode: ""= になっているところを、 =mode: ipvs= に修正する。

修正したら、 =kubectl -n rollout restart daemonset kube-proxy= して、再起動すればおしまい。
(もちろん、各Podをdeleteして再作成するのも可)
*** 修正後確認

#+begin_src 
$ kubectl -n kube-system logs kube-proxy-z8nwd kube-proxy
I0727 14:02:58.646065       1 server_others.go:172] Using ipvs Proxier.
W0727 14:02:58.646292       1 proxier.go:420] IPVS scheduler not specified, use rr by default
I0727 14:02:58.646423       1 server.go:571] Version: v1.17.8
#+end_src

Warningがでているが、IPVSモードでは、kube-proxyの負荷分散方式を、ラウンドロビンや
リーストコネクションなどから選べるようだ。
指定していないとラウンドロビンになるようだが、まあそれでいいのではないかな。

https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/#ipvs-based-kube-proxy
*** まとめ
ipvsの場合の負荷分散方式が、いろいろあるってことは知らなかったので、やってみてよかった。
** DONE Hyper-V上でGitLabサーバを構築する              :hyper_v:gitlab:docker:
   CLOSED: [2020-07-30 Thu 07:09]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-07\")
:EXPORT_FILE_NAME: 202007292345
:END:

いろいろあって、GitLabを構築を試す必要があったので手順のメモ。普段使っている検証用の
VMware環境は、kubernetesが動いていて、特にメモリ確保が厳しそうだったので、
WindowsのHyper-V上に作ることにした。

*** Hyper-Vに仮想マシンを作る

とにかくHyper-Vのネットワークが難解で、デフォルトスイッチで作ると
OS起動のたびにIPアドレスが変動するし、
外部ネットワークも下手に作ると母艦で通信できなくなるという
なんでこんなことになってるの？って動きをしてくださいますので、
"管理オペレーティング システムにこのネットワーク アダプターの共有を許可する"
にチェックをつけて、外部ネットワークにつなげる。
これにより、VMware Playerなどのブリッジ接続と同じになる。

多分これが一番早いと思います。

検索すると、vNICをふたつ作って内部ネットワークを固定して、インターネット通信は
デフォルトスイッチにするのがよいってのが多々あったけど、めんどくさいよ。

OSは適当に最新のFedoraのISOをもってきてインストールした。

ホスト名は、シンプルに =gitlab.local= にしている。

ちなみに、SELinuxとFirewallは無効化している。

*** GitLabをインストールする

GitLab EEには魅力的な機能が多数あるが、今回はざっと作るだけなので CEの機能があれば十分。
なのだが、公式サイトにも別にライセンス登録しないEEはCEと変わらんのでEE入れれば？ってあるので
GitLab EEを入れることにする。

https://www.gitlab.jp/install/ce-or-ee/

Fedoraは公式には対応していないので、あたかもRHEL8であるかのようにごまかして導入する必要がある。

#+begin_src bash
curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.rpm.sh -o script.sh
chmod +x script.sh
os=el dist=8 ./script.sh
EXTERNAL_URL="http://gitlab.local" dnf install -y gitlab-ee
#+end_src

インストールがおわったら、 =http://gitlab.local= にアクセスすると、 =root= ユーザのパスワード設定が求められ、
設定後ログインが可能になる。

*** 自己署名証明書を作る

イメージレジストリを有効化したいが、こちらはhttpだとdocker操作時に怒られてしまって面倒なので、証明書を準備する。
Let's Encriptのほうが楽だと思うのだけど、今回は自己署名証明書を作ることにする。

#+begin_src bash
openssl req -newkey rsa:4096 -nodes -sha256 -keyout registry.gitlab.local.key -x509 -days 3650 -out registry.gitlab.local.crt
#+end_src

CNだけ指定した。

#+begin_src 
Country Name (2 letter code) [XX]:
State or Province Name (full name) []:
Locality Name (eg, city) [Default City]:
Organization Name (eg, company) [Default Company Ltd]:
Organizational Unit Name (eg, section) []:
Common Name (eg, your name or your server's hostname) []:registry.gitlab.local
Email Address []:
#+end_src

*** GitLabのイメージレジストリの有効化

#+begin_src bash
mkdir -p /etc/gitlab/ssl
cp registry.gitlab.local.crt registry.gitlab.local.key /etc/gitlab/ssl/
#+end_src

=/etc/gitlab/gitlab.rb= を編集して、以下の行を追加(コメント化されてあるので、
コメント化解除して値を書き換え)。

#+begin_src
registry_external_url 'https://registry.gitlab.local'

gitlab_rails['registry_host'] = "registry.gitlab.local"
#+end_src

変更を反映する。
#+begin_src bash
gitlab-ctl reconfigure
#+end_src

試しに適当にプロジェクトを作ってみると、コンテナレジストリも有効化されていることがわかる。

[[file:images/20200730-gitlab-registry.png]]

*** イメージをpushする
テストとして、docker login & イメージプッシュしたいが、オレオレ証明書なので
そのままでは利用できない。

- Linuxの場合
先ほど作った registry.gitlab.local.crt を クライアント側(docker loginする側)の
=/etc/docker/certs.d/registry.gitlab.local/ca.crt= にコピーする。
ディレクトリがなければ作成する。

- Windowsの場合
Docker for Windowsのダッシュボードを開いて、SettingsのDocker Engineから以下のように設定する。

#+begin_src json
{
  "registry-mirrors": [],
  "insecure-registries": ["registry.gitlab.local"],
  "debug": true,
  "experimental": false
}
#+end_src

これで、docker loginならびにdocker pushできるはず。
** DONE Hyper-Vにokd4(OpenShift Origin)をインストールする  :hyper_v:openshift:
   CLOSED: [2020-08-01 Sat 13:09]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-08\")
:EXPORT_FILE_NAME: 202008011309
:END:
最近出たというOpenShiftのupstream版である =okd= をインストールしようとしたら結構てこずったのでメモ。
Hyper-Vは対応プラットフォームに書かれていないので、手探り感がすごい。

okdは昔は =OpenShift Origin= と呼ばれていたが、OpenShift3の途中からokdという名前に変わった。
その後、OpenShift 4になって、ずっと出ていなかったが、OpenShift 4.5にあわせて再登場したらしい。

https://www.publickey1.jp/blog/20/red_hatkubernetesokd_4.html


ちなみに、OpenShift4を手軽に試す環境として、 =CodeReady Containers= というものあり、
こちらは1VMで動かす =minikube= や =minishift= みたいなものだ。

CodeReady Containersも以前構築してみたことがあり、そのときの記事がこちら。

[[CodeReady ContainersでWindows上にOpenShift環境を構築する]]

CodeReady Containersは完全お試し用のもので、30日で証明書が切れると再インストール、
すなわち最初からやりなおしという致命的な弱点がある。
OKDはバージョンアップの方法も書かれているので、そういったことは無いと信じたい。

以下の実施内容は、基本的に公式ドキュメントの記載にもとづいておこなった。

https://docs.okd.io/latest/installing/installing_bare_metal/installing-bare-metal.html

*** 準備するもの

| 名前           | CPU   | メモリ | 備考                   |
| Master         | 4vCPU | 24GB   | コントロールプレーン   |
| Bootstrap      | 4vCPU | 16GB   | インストール時だけ必要 |
| ロードバランサ |       |        | nginxを利用            |
| DNS            |       |        | dnsmasqを利用          |
| HTTPサーバ     |       |        | nginxを利用            |

ドキュメントには、コントロールプレーンが3台、ワーカーノードが2台必要って書かれているが、
以下のFAQに、コントロールプレーン1台でもOKって書かれていたので、そのようにしてみた。

Bootstrapサーバがインストール時だけ必要なくせに、16GB必要だし、おかげでLBも用意しなきゃいけないしでつらい。

本来コントロールプレーンもメモリ16GBでよいのだが、さすがに1ノードにまとめるならもうちょっと入れておくかと24GBにした。
本当は32GB確保したかったのだけど、Bootstrapのせいで確保できなかったので妥協。

https://github.com/openshift/okd/blob/master/FAQ.md#can-i-run-a-single-node-cluster

ロードバランサ、DNS、HTTPサーバは先日作ったGitlab用のVMがあったので、そこにまとめて入れることにした。

また、okd(openshift)は、 =xxx.クラスタ名.ベースドメイン= という形式のFQDNでアクセスすることになる。
今回は、クラスタ名を =okd= 、ベースドメインを =local= とした。

*** インストール

**** 作業端末の準備
1. 作業端末を準備する。 =openshift-install= コマンドがLinuxとmacだけだったのでどちらか。
   自分は、Windowsなので、wsl上で作業した。

2. インストーラをgithubからダウンロードする。

   https://github.com/openshift/okd/releases/tag/4.5.0-0.okd-2020-07-29-070316

  #+begin_src bash
  wget https://github.com/openshift/okd/releases/download/4.5.0-0.okd-2020-07-29-070316/openshift-client-linux-4.5.0-0.okd-2020-07-29-070316.tar.gz
  wget https://github.com/openshift/okd/releases/download/4.5.0-0.okd-2020-07-29-070316/openshift-install-linux-4.5.0-0.okd-2020-07-29-070316.tar.gz

  tar xf openshift-client-linux-4.5.0-0.okd-2020-07-29-070316.tar.gz
  tar xf openshift-install-linux-4.5.0-0.okd-2020-07-29-070316.tar.gz

  #+end_src

3. こちらのページから、Red Hat Developerに登録したアカウントで pull-secret なるファイルをダウンロードする
   
   https://cloud.redhat.com/openshift/install/pull-secret

4. インストール設定を格納するディレクトリを作成する。このとき、ディレクトリ名はクラスタ名にそろえるということなので、 =mkdir okd= とokdディレクトリを作成した。

5. =okd/install-config.yaml= を以下の内容で作成する。

   #+begin_src yaml
apiVersion: v1
baseDomain: local
compute:
- hyperthreading: Enabled
  name: worker
  replicas: 0
controlPlane:
  hyperthreading: Enabled
  name: master
  replicas: 1
metadata:
  name: okd
networking:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  networkType: OpenShiftSDN
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
fips: false
pullSecret: '{"auths": ...}' #先ほどダウンロードした pull-secretの中身
sshKey: 'ssh-ed25519 AAAA...' #公開鍵。なければ適当につくる。
#+end_src

   先述のとおり、今回はコントロールプレーンは1台だけなので、ドキュメントと違って、 =.controlPlane.replicas= を1にしている。

6. マニフェストの作成
install-config.yamlを配置した okdディレクトリの1階層上で以下のコマンドを実行する。
   #+begin_src bash
openshift-install create manifests --dir=okd
   #+end_src

実行すると、install-config.yamlは消えてしまうので、何回もやり直しそうならコピーしておくのが無難。

実行後、設定を変更してコントロールプレーンではPodが起動しないように設定することもできるが、
今回は1台構成なので変更せずにそのままとする。

7. ignitionファイルの作成
   okd(OpenShift)のインストールでは、ignitionファイルとよばれるものの中に、
   install-config.yamlおよびそこから生成されたマニフェストの情報が含まれているようだ。

   #+begin_src bash
openshift-install create ignition-configs --dir=<
#+end_src

   を実行すると、 =bootstrap.ign= =master.ign= =worker.ign= というファイルが作成される。

   作業用端末での操作はいったん止めて、次に周辺サーバの準備、および Fedora CoreOSのインストールをおこなう。

**** DNSの準備
dnsmasqを適当に設定すればよい。自分は以下の通り/etc/hostsを設定し、必要な名前解決ができるようにした。

#+begin_src hosts
192.168.2.11 api.okd.local api-int.okd.local 
192.168.2.12 master.okd.local etcd-0.okd.local _etcd-server-ssl._tcp.okd.local
192.168.2.13 bootstrap.okd.local
#+end_src

**** HTTPサーバの準備

1. nginxをインストール
   #+begin_src bash
dnf install nginx nginx-mod-stream
#+end_src
   のちにLBとしても利用したかったので、streamモジュールもインストール。

2. 通常は、インストールすればおしまいだが、Gitlabが 80ポートを占領しているので、Listenするポートを10080に変更した。

   #+begin_src nginx
http {
  server {
    listen 10080;
    listen [::]:10080;
  }
}
#+end_src

3. /usr/share/nginx/htmlに必要資材を配置

   先ほど作成された =bootstrap.ign= =master.ign= =worker.ign= を配置する。

   以下のサイトからRawファイルとsignatureファイルをダウンロードして配置する。
   ドキュメントにはsignatureファイルのことが書かれてないが配置しないとインストールに失敗するので注意(1敗)。

   また、Rawファイルは展開する必要はない。圧縮ファイルのままで配置しないとインストールに失敗するので注意(1敗)。
   なお、RawとRaw(4k Native)の2種類あるが、よくわからなかったので、Rawのほうを使った。

   https://getfedora.org/en/coreos/download?tab=metal_virtualized&stream=stable

**** Load Balancerの設定
nginxをTCPロードバランサとするために、 =/etc/nginx/nginx.conf= に以下の設定を入れた。

#+begin_src nginx
stream {
  upstream k8s-api {
    server 192.168.2.12:6443;
    server 192.168.2.13:6443;
  }
  upstream machine-config {
    server 192.168.2.12:22623;
    server 192.168.2.13:22623;
  }
  server {
    listen 6443;
    proxy_pass k8s-api;
  }
  server {
    listen 22623;
    proxy_pass machine-config;
  }
}
#+end_src

   
**** Fedora CoreOSをインストール

ここから実際にbootstrapサーバやMasterサーバにokdを動かすためのOSであるFedora CoreOSをインストールしていく。

1. ISOをダウンロード
   
   先ほどRawファイルをダウンロードしたときと同じサイトだが、
   https://getfedora.org/en/coreos/download?tab=metal_virtualized&stream=stable
   からISOをダウンロードする

2. Hyper-Vの管理コンソールから、仮想マシンを作成する。

   メモリの動的割当は無効化した。

3. ISOをマウントしてOSを起動する

4. インストール選択画面で *タブキー* を押し、カーネル引数を入力できるようにする

5. 表示されているパラメータに続けて以下のように入力する

   - Bootstrap

     #+begin_src 
coreos.inst=yes coreos.inst.install_dev=sda coreos.inst.image_url=http://192.168.2.11:10080/fedora-coreos-32.raw.xz coreos.inst.ignition_url=http://192.168.2.11:10080/bootstrap.ign ip=192.168.2.13::192.168.2.1:255.255.255.0:bootstrap.okd.local:eth0:none nameserver=192.168.2.11
#+end_src

   - Master

     #+begin_src 
coreos.inst=yes coreos.inst.install_dev=sda coreos.inst.image_url=http://192.168.2.11:10080/fedora-coreos-32.raw.xz coreos.inst.ignition_url=http://192.168.2.11:10080/master.ign ip=192.168.2.12::192.168.2.1:255.255.255.0:master.okd.local:eth0:none  nameserver=192.168.2.11
     #+end_src

   それぞれのパラメータの詳細は以下のとおり。

   =coreos.inst.image_url= には、rawファイルにアクセスできるURLを書く。 

   =coreos.inst.ignition_url= には、それぞれのignitionファイルにアクセスできるURLを書く。

   =ip= は、次のフォーマットでIPアドレスを設定する =IPアドレス::デフォルトゲートウェイ:サブネットマスク:ホスト名:デバイス名:none= 。
   IPアドレスの直後だけコロンが2つなことに注意(1敗)。

   =nameserver= は、ネームサーバのアドレスを書く。これもドキュメントには目立たないところにあるので忘れないように(1敗)。

   ちなみに手入力がつらかったのだが、Hyper-Vにはクリップボードから入力という神機能があった。
   ただし、動作としてペーストではなくて1文字ずつ解釈して変わりに入力してくれるRPAみたいな機能のため、
   JIS配列とUS配列の記号位置による違いがそのまま反映される。 === が =_= になってしまうとか。

   これは、あらかじめ自分の環境もIMEを変更してUSキーボードにしておくことで回避できた。
   
6. 入力したらエンターキーを押すとインストールがはじまる。
   
   インストールがおわると一瞬エラーメッセージのようなものが *赤字* で表示されて再起動してしまうのだけれどエラーではないので注意(N敗)。
   単に、再起動時にISOをunmountしようとして、できなかったといってるだけである。
   
   コンマ何秒しか表示されないので、しかたがなく画面を録画して確認して、がっかり。

7. 再起動したら、再度タブを押してISOを手動でunmountしてから再起動する。
   
   これをやらないと、せっかくFedore CoreOSがインストールできたのに、またインストール処理に入ってしまう。
   しかも、未設定の状態で(N敗)。
   
   タブを押すとパラメータ入力待ちとなってくれるため、おちついてディスクをとりはずして再起動できる。

   これでFedora CoreOSのインストールはおしまい。

**** okdのインストール

CoreOSのインストールもおわり、あとはokdをインストールするだけ……なのだけど、実はインストールは
先ほどのignitionファイルの内容をもとに裏で勝手におこなわれるため、待ってるだけでよい。

作業端末で、以下のコマンドを実行するとインストール状況を監視してくれる。

#+begin_src bash
openshift-install --dir=okd wait-for bootstrap-complete --log-level=debug
#+end_src

なお、インストールは30分ぐらいかかるが、20分ぐらいはずっとエラーメッセージが出続ける
(上記コマンドの標準出力も、CoreOSの標準出力も)。

見てても落ち着かないだけなので、のんびり待ちましょう(1敗)。

最終的に、以下のような感じで出力されてコマンドが終了する。
そうしたら無事に終了である。

#+begin_src 
DEBUG Bootstrap status: complete
INFO It is now safe to remove the bootstrap resources
INFO Time elapsed: 30m
#+end_src

*** okdへのアクセス

okd/auth 配下に、kubeconfigファイルとkubeadmin-passwdファイルができている。

kubeconfigファイルを使えば =oc= 、 =kubectl= でCLIアクセスできるし、
https://console-openshift-console.apps.okd.local/ にアクセスすることでWebコンソールも利用可能。
初期ユーザは kubeadmin であり、パスワードは kubeadmin-passwd に書かれている。

この辺は、CodeReady Containersと同じだ。

[[file:images/20200801-okd-login.png]]

CodeReady Containersのころは、OpenShift 4.2相当だったと思うので、
だいぶみためが変わっている。

[[file:images/20200801-okd-dashboard.png]]

インストールが無事にできたので、今度はなんか適当にアプリケーションを動かしてみよう。
** DONE kubernetes-mixinのダッシュボードでgrafanaダッシュボードを簡単に構築する :kubernetes:grafana:
   CLOSED: [2020-08-10 Mon 12:56]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-08\")
:EXPORT_FILE_NAME: 202008032123
:END:
kubernetesのメトリクスモニタリングを、SaaSではなく手元でやろうとしたらPrometheusがおそらく最大の候補であろう。

Prometheusを使うならダッシュボードにはgrafanaを使うことになると思うが、
grafanaはダッシュボード表示エンジンであって、ダッシュボード自体は自分で作ることになる。
これが、やっぱり自分でクエリを書いてレイアウトも考えてと結構面倒くさい。

もちろんgrafanaのコミュニティにも誰かが作ったkubernetes用のダッシュボードは存在するのだが、
kubernetes-monitoringというプロジェクトがあって、そこでダッシュボードやアラートルールを整備しているものがある。
コントリビュータをピックアップしてみると、Red Hatのメンバーが多いみたい。

https://github.com/kubernetes-monitoring/kubernetes-mixin

今回はこれを使ってみることにした。
ネット上の情報だと、とりあえずhelmでprometheusやgrafana入れてみました〜♪ で終わってて
どうやってダッシュボード使うのよ、まで書かれてなかったりするがそこまでやっている。

*** 導入するもの

今回は、Prometheus、Grafanaをhelmを使って導入する。

https://github.com/helm/charts/tree/master/stable/prometheus

https://github.com/helm/charts/tree/master/stable/grafana

prometheusを入れるだけであれば、Prometheus Operator、kube-prometheusなど
いくつか選択肢がある。今回helmにしたのは、pushgatewayがはじめから含まれているのが理由だ。
今のところpushgatewayを使いたいものも無いのだけど、それだけ個別に導入するのも嫌だったので。

prometheusをhelmで入れるなら、grafanaもhelmでいっか、という感じである。
*** kubernetes-mixinのyaml定義を作る
kubernetes-mixinのプロジェクトでは、yaml形式では提供されておらず、
jsonnetという形式で提供されている。

特にダッシュボードのyaml定義だと、同じ記載内容があちこちにでてしまって
修正もれの恐れがあったり、そもそも修正箇所がわかりにくかったりという課題がある。
jsonnetはそれをプログラミング言語のように構造化することで、わかりやすくしている。
といっても、最初自分が見たときも、なにをすればよいのやらという感じでわかりにくいと感じたが。

README.mdに書いてあるとおり、以下のようなかんじで、ダッシュボード、レコーディングルール、アラートルールのyamlを作ることができる。

#+begin_src bash
# 必要なコマンドの取得
go get github.com/jsonnet-bundler/jsonnet-bundler/cmd/jb
pip install jsonnet

# 依存定義の取得
jb install

# yaml定義の出力
make dashboards_out
make prometheus_rules.yaml
make prometheus_alerts.yaml
#+end_src
*** ダッシュボード定義の修正
先ほどの手順で作成した各種yamlを使えばよいのだが、dashboardの各種yaml定義のPromQLは、
helmで入れるprometheusに対して一部動かないところがある。
例えば、CPU Utilisationや各グラフが *N/A* や *No data* になる。

helmで入れるprometheusのjob設定に書かれたメトリクスに付与されるラベルと、
kubernetes-mixinで期待するラベル定義に差があるのが原因だ。

それをhelmチャートにあわせて修正したものが以下の内容である。

https://github.com/grugrut/kubernetes-mixin/commit/d4361ca715b4fcbeab289cf2f7c29282f316651b

やってから気付いたが、本来は直接修正するんじゃなくて、
Configだけ作って上書きするのが正しかったのだと思う。

とはいえ、 =$__interval= でうまくいかず、 =$__range= にするとかもやったので修正は必要。
=$__range= だとうまくいくのが、そもそも誤りなのか、自分の環境固有なのかわかってない。
さすがにこのレベルのミスが、issueにもあがってないというのは奇妙なので自分の環境固有な気がしてる。
*** 各コンポーネントが、メトリクスを返せるようにする
あとは、そもそもPrometheusがメトリクスを収集できるように、いくつかのコンポーネントを修正する必要がある。

 1. metrics-serverを導入する

 2. kube-controllerのメトリクスをprometheusが収集できるようにする

    Masterサーバの =/etc/kubernetes/manifests/kube-controller-manager.yaml= を修正。
    podにannotationを付与。

    #+begin_src yaml
 # 追加するところだけ記載
 metadata:
   annotations:
     prometheus.io/scrape: "true" #追加
     prometheus.io/port: "10252"  #追加
    #+end_src

 3. kube-proxyがメトリクスを外部からアクセスできるようにしていなかったので修正する

    =kubectl -n kube-system edit configmap kube-proxy= でconfig.confを修正。
    =.metrisBindAddress= が、デフォルトでは =""= になっているので、 ="0.0.0.0"= とする

 4. kube-proxyのメトリクスをprometheusが収集できるようにする

    =kubectl -n kube-system edit daemonset kube-proxy= で、podにannotationを付与。
 
    #+begin_src yaml
 # 追加するところだけ記載
 spec:
   template:
     metadata:
       annotations:
         prometheus.io/port: "10249"  #追加
         prometheus.io/scrape: "true" #追加
    #+end_src
 5. kube-schedulerのメトリクスをprometheusが収集できるようにする

    Masterサーバの =/etc/kubernetes/manifests/kube-scheduler.yaml= を修正。
    podにannotationを付与。

    #+begin_src yaml
 # 追加するところだけ記載
 metadata:
   annotations:
     prometheus.io/scrape: "true" #追加
     prometheus.io/port: "10251"  #追加
    #+end_src
*** helmでprometheusとgrafanaを入れる
ここまで下準備ができたら、helmでprometheusとgrafanaを入れるだけだ。
基本的には各chartのドキュメント通りに入れればおしまいなのだが、
作ったyamlを読みこむために、それぞれ以下のような仕込みをする。

ちなみに自分の作った定義は以下に配置している。

https://github.com/grugrut/k8s-playground/tree/4073c565320d396467348a9c7839bcde90873e3a/03_monitoring

**** prometheus
レコーディングルールとアラートルールを、それぞれhelm chartの変数 =serverFiles.recording_rules.yml= と
=serverFiles.alerting_rules.yml= で指定する必要がある。
もし他のルールが必要ないのであれば、以下のように、先ほど作った =prometheus-rules.yaml= を編集して作るのが楽だと思う。

#+begin_src yaml
serverFiles:
  recording_rules.yml:
    # 全ての行に4つスペースをつけてインデントさせた prometheus-rules.yamlを流しこむ
#+end_src

できたファイルがこんな感じ。

https://github.com/grugrut/k8s-playground/blob/0c6c4025686027c6e18aa723d4ac4779f00a3043/03_monitoring/prometheus_rules-variables.yaml

yqとか使っていいかんじに作れないかなと思ったのだけど、式のところが崩れてしまってダメだった。
内部的にjsonに変換する都合上、パイプを使った複数行の表現がうまくいかないのだろう。

ちなみに、emacsなら以下な感じで簡単に作れる。

1. =serverFiles:= と =recording_rules.yaml:= の行を書く
2. ~C-x i~ で =prometheus-rules.yaml= の内容を挿入する
3. 3行目の先頭で ~C-SPC~ してマークし、 ~M->~ で最終行までジャンプする
4. ~C-x r t~ で、各行の先頭0バイトを空白4つに置き換える

それ以外の設定は、nodeExporterをmasterノードにも配置されるようにしたり、
PVの設定を少ししている。

最終的には、以下のコマンドでインストールできる。

#+begin_src bash
helm install prometheus stable/prometheus -n monitoring -f prometheus-variables.yaml -f prometheus_rules-variables.yaml -f prometheus_alerts-variables.yaml
#+end_src

**** grafana
grafana側では作ったダッシュボードを読み込ませる必要がある。方法としてはいくつかある。

1. インストール後に設定する
2. ひとつのConfigMapにまとめる
3. 別々のConfigMapとする

いろいろと試してみたが、別々のConfigMapにする方法がいちばん簡単だった。

これを実現するには、 grafanaのhelm chartsの =sidecar.dashboards.enabled= をtrueにする。
すると以下のページにもあるとおり、 =grafana_dashboard= というラベルがついたConfigMapが
自動で読み込まれダッシュボードとして使えるようになる。しかも、オンラインなので
設定の反映のために再起動なども不要である。

https://github.com/helm/charts/tree/master/stable/grafana#sidecar-for-dashboards

dashboardのyamlはたくさんあるので、ひとつひとつConfigMapを作るのも、それはそれで面倒だ。
特定のディレクトリにyamlを配置しておけば、以下のようにワンライナーで設定できる。

#+begin_src bash
find dashboards/ -name "*.json" -printf "%f\n" | xargs -t -IXXX -- kubectl -n monitoring create configmap dashboard-XXX --from-file=dashboards/XXX
find dashboards/ -name "*.json" -printf "%f\n" | xargs -t -IXXX -- kubectl -n monitoring label configmap dashboard-XXX grafana_dashboard=1
#+end_src

更新したい場合も、ひとつひとつやってもいいが、以下のようにまとめてやることもできる。

#+begin_src bash
find dashboards/ -name "*.json" -printf "%f\n" | xargs -t -IXXX -- sh -c "kubectl -n monitoring create configmap dashboard-XXX --from-file=dashboards/XXX --dry-run=client -o yaml | kubectl replace -f -"
find dashboards/ -name "*.json" -printf "%f\n" | xargs -t -IXXX -- kubectl -n monitoring label configmap dashboard-XXX grafana_dashboard=1
#+end_src

あとは、helmでgrafanaを入れてしまえばよい。

#+begin_src bash
helm install grafana stable/grafana -f grafana-variables.yaml
#+end_src

ちなみに、実行結果にも出力されるが、grafanaのadminパスワードは自動生成されてsecretに格納されている。
以下のようにして取得できる。

#+begin_src bash
kubectl get secret --namespace monitoring grafana -o jsonpath="{.data.admin-password}" | base64 --decode
#+end_src

これでしばらくすれば、prometheusに収集されたデータをgrafanaで確認することができるはず。
** DONE SpringBootとNuxtJSでTODOアプリを作る             :springboot:nuxstjs:
   CLOSED: [2020-08-25 Tue 23:28]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-08\")
:EXPORT_FILE_NAME: 202008221329
:END:

tektonでもう少し複雑なCI/CDフローを作ったり、別の基盤で動かしたりするサンプルとして、
フロントエンドとバックエンドにわかれたアプリを作ってみた。
とはいえ、あまり複雑にするつもりはなかったので、シンプルにTODOアプリにしている。

普段はGolangで書くことが多いのだけど、今回はフロントエンドに =NuxtJS= 、
バックエンドに =SpringBoot= を使ってみた。

アーキテクチャとしては、こんな感じ。
最初は =VueJS= で考えていたが、ユーザ側に公開するサービスは1つだけにしたかったので
=NuxtJS= のほうがやりやすいかな、とこういう構成にしてみた。

#+begin_src plantuml :file 20200822-architecture.png
left to right direction

actor User

cloud {
rectangle "Web Server" as web {
        rectangle NuxtJS
}
rectangle "AP Server" as ap {
        rectangle SpringBoot
}
database "DB"
}

User --> NuxtJS :HTTP
NuxtJS --> SpringBoot :HTTP
SpringBoot --> DB :JDBC
#+end_src

ソースコードは以下に配置している。

https://github.com/grugrut/todo-springboot

基本的な作りはシンプルだが、やはり初めてさわるものであり、いくつかハマったところがあるので紹介。
*** バックエンド側のSpringBoot
**** 本番・開発で設定をわける
DB接続先情報など、本番と開発で設定をわけたいものがある。
SpringBootでは、 =application.properties= の =spring.profile.active= というプロパティで
プロファイルを設定することができる。

これを設定しておくことで、 =application.properties= が読みこまれた後に、
=application-(プロファイル名).properties= が追加で読みこまれる。

こちらには環境変数が使えるので、
#+begin_src properties
spring.profiles.active = ${ENV:dev}
#+end_src
のように、環境変数 =ENV= が設定されていたらそれをプロファイル名として読みこみ、
未設定の場合には、devが設定されるようにした。

未設定の場合のデフォルト値を設定したことで、開発環境では環境変数を設定しておく必要がないのが便利。

https://github.com/grugrut/todo-springboot/blob/5017ac9b720b8d1d7e6462a4ff439db0a9dfdd07/src/backend/src/main/resources/application.properties

**** 開発時のテーブル作成
開発環境ではH2データベースを利用した。
H2データベースは、javaで書かれたRDBMSで、eclipseからも簡単に扱うことができる。

SpringBootでは、開発環境での実行時にDDLとDMLをそれぞれ実行することができ、
やはり =application.properties= で実行するsqlファイルを設定する。

#+begin_src properties
spring.datasource.schema=classpath:schema.sql
spring.datasource.data=classpath:data.sql
#+end_src

初期データは空でよかったので、schema.sqlの実行だけでよかったのだが、
data.sqlを作成していなかったり作成しても空ファイルだとエラーが出てしまった。

そのため、しかたがなく =SELECT 1;= だけ実行する意味のない =data.sql= を作成して回避している。

これはさすがにもっと良いやり方があるのかと思うが、回避できてるのでヨシ!

#+begin_src plain
org.springframework.context.ApplicationContextException: Unable to start web server; nested exception is org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:161) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:545) ~[spring-context-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:143) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:758) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:750) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1237) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at net.grugrut.todo.TodoSpringApplication.main(TodoSpringApplication.java:10) ~[classes/:na]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
	at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49) ~[spring-boot-devtools-2.3.2.RELEASE.jar:2.3.2.RELEASE]
Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:142) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.<init>(TomcatWebServer.java:104) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:437) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:191) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:178) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:158) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	... 14 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'h2Console' defined in class path resource [org/springframework/boot/autoconfigure/h2/H2ConsoleAutoConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.boot.web.servlet.ServletRegistrationBean]: Factory method 'h2Console' threw exception; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.autoconfigure.jdbc.DataSourceInitializerInvoker': Invocation of init method failed; nested exception is org.springframework.jdbc.datasource.init.UncategorizedScriptException: Failed to execute database script from resource [URL [file:/git/todo-springboot/src/backend/target/classes/data.sql]]; nested exception is java.lang.IllegalArgumentException: 'script' must not be null or empty
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:655) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:635) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1336) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1176) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:556) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:516) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:324) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:322) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:202) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addServletContextInitializerBeans(ServletContextInitializerBeans.java:96) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.servlet.ServletContextInitializerBeans.<init>(ServletContextInitializerBeans.java:85) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:255) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:229) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:53) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5128) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1384) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1374) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[na:na]
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140) ~[na:na]
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:909) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:841) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1384) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1374) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[na:na]
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:140) ~[na:na]
	at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:909) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:262) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.StandardService.startInternal(StandardService.java:421) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:930) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.apache.catalina.startup.Tomcat.start(Tomcat.java:486) ~[tomcat-embed-core-9.0.37.jar:9.0.37]
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:123) ~[spring-boot-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	... 19 common frames omitted
Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.boot.web.servlet.ServletRegistrationBean]: Factory method 'h2Console' threw exception; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.autoconfigure.jdbc.DataSourceInitializerInvoker': Invocation of init method failed; nested exception is org.springframework.jdbc.datasource.init.UncategorizedScriptException: Failed to execute database script from resource [URL [file:/git/todo-springboot/src/backend/target/classes/data.sql]]; nested exception is java.lang.IllegalArgumentException: 'script' must not be null or empty
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:185) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:650) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	... 59 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataSource' defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceConfiguration$Hikari.class]: Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.autoconfigure.jdbc.DataSourceInitializerInvoker': Invocation of init method failed; nested exception is org.springframework.jdbc.datasource.init.UncategorizedScriptException: Failed to execute database script from resource [URL [file:/git/todo-springboot/src/backend/target/classes/data.sql]]; nested exception is java.lang.IllegalArgumentException: 'script' must not be null or empty
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:602) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:516) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:324) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:322) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:202) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.config.DependencyDescriptor.resolveCandidate(DependencyDescriptor.java:276) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.doResolveDependency(DefaultListableBeanFactory.java:1307) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory$DependencyObjectProvider.getIfAvailable(DefaultListableBeanFactory.java:1947) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.ObjectProvider.ifAvailable(ObjectProvider.java:91) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.boot.autoconfigure.h2.H2ConsoleAutoConfiguration.h2Console(H2ConsoleAutoConfiguration.java:72) ~[spring-boot-autoconfigure-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:na]
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na]
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	... 60 common frames omitted
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.boot.autoconfigure.jdbc.DataSourceInitializerInvoker': Invocation of init method failed; nested exception is org.springframework.jdbc.datasource.init.UncategorizedScriptException: Failed to execute database script from resource [URL [file:/git/todo-springboot/src/backend/target/classes/data.sql]]; nested exception is java.lang.IllegalArgumentException: 'script' must not be null or empty
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1794) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:594) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:516) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:324) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:226) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:322) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:227) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveNamedBean(DefaultListableBeanFactory.java:1175) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveBean(DefaultListableBeanFactory.java:420) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBean(DefaultListableBeanFactory.java:350) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBean(DefaultListableBeanFactory.java:343) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.boot.autoconfigure.jdbc.DataSourceInitializerPostProcessor.postProcessAfterInitialization(DataSourceInitializerPostProcessor.java:52) ~[spring-boot-autoconfigure-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:430) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1798) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:594) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	... 75 common frames omitted
Caused by: org.springframework.jdbc.datasource.init.UncategorizedScriptException: Failed to execute database script from resource [URL [file:/git/todo-springboot/src/backend/target/classes/data.sql]]; nested exception is java.lang.IllegalArgumentException: 'script' must not be null or empty
	at org.springframework.jdbc.datasource.init.ScriptUtils.executeSqlScript(ScriptUtils.java:645) ~[spring-jdbc-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.jdbc.datasource.init.ResourceDatabasePopulator.populate(ResourceDatabasePopulator.java:254) ~[spring-jdbc-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.jdbc.datasource.init.DatabasePopulatorUtils.execute(DatabasePopulatorUtils.java:49) ~[spring-jdbc-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.boot.autoconfigure.jdbc.DataSourceInitializer.runScripts(DataSourceInitializer.java:202) ~[spring-boot-autoconfigure-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.autoconfigure.jdbc.DataSourceInitializer.initSchema(DataSourceInitializer.java:119) ~[spring-boot-autoconfigure-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.autoconfigure.jdbc.DataSourceInitializerInvoker.initialize(DataSourceInitializerInvoker.java:75) ~[spring-boot-autoconfigure-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.boot.autoconfigure.jdbc.DataSourceInitializerInvoker.afterPropertiesSet(DataSourceInitializerInvoker.java:65) ~[spring-boot-autoconfigure-2.3.2.RELEASE.jar:2.3.2.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1853) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1790) ~[spring-beans-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	... 89 common frames omitted
Caused by: java.lang.IllegalArgumentException: 'script' must not be null or empty
	at org.springframework.util.Assert.hasText(Assert.java:287) ~[spring-core-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.jdbc.datasource.init.ScriptUtils.splitSqlScript(ScriptUtils.java:214) ~[spring-jdbc-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	at org.springframework.jdbc.datasource.init.ScriptUtils.executeSqlScript(ScriptUtils.java:592) ~[spring-jdbc-5.2.8.RELEASE.jar:5.2.8.RELEASE]
	... 97 common frames omitted
#+end_src

*** フロントエンド側のNuxtJS

**** 本番・開発で設定をわける
バックエンドと同じくフロントエンド側も、本番では環境変数をあたえることで
設定をわけるようにしたい。

NuxtJSの場合は、 =nuxt.config.js= で設定することができる。

#+begin_src javascript
proxy: {
  '/api/': {
    target: process.env.API_BASE_URL || 'http://localhost:8080',
    pathRewrite: {'^/api/': ''}
  }
},
#+end_src

こんな感じ。他のソース中での変数参照として、 =process.env.ENV_VALUE= を上書きする方法もあるようだったが、
今回の範囲では利用するシーンがなかったので省略。

https://github.com/grugrut/todo-springboot/blob/5017ac9b720b8d1d7e6462a4ff439db0a9dfdd07/src/frontend/nuxt.config.js

**** NuxtJSでサーバサイドをバックエンド呼びだしのプロキシとして利用する

本来であればjavascriptなので、そこからのAPI呼びだしはクライアント(つまり利用者のブラウザ)から
バックエンドのAPIサーバへの直接の呼びだしになる。

しかし、バックエンドのエンドポイントを外にだしたくない場合、NuxtJSのサーバサイドでも動いている特性を活かして、
NuxtJSのサーバ側をリバースプロキシとして利用することができる。

その実現のために、axiosのproxy機能を利用する。

サーバサイドでaxiosを利用するために、 =npm install --save @nuxtjs/axios= して =nuxt.config.js= に以下の設定をすればよい。

#+begin_src javascript
  modules: [
    "@nuxtjs/axios",
  ],
  axios: {
    proxy: true
  },
  proxy: {
    '/api/': {
      // for WSL2, `API_BASE_URL=http://$(cat /etc/resolv.conf | grep nameserver | awk '{print $2}'):8080 npm run dev`
      target: process.env.API_BASE_URL || 'http://localhost:8080',
      pathRewrite: {'^/api/': ''}
    }
  },
#+end_src

これにより、この設定の場合は /api/xxx に対するアクセスが、 サーバ側で動いているExpress経由で target/xxx にアクセスできる。
そのため、バックエンドのエンドポイントはフロントエンドのプロセスからのみアクセスできればよく、
クライアントにまで公開しておく必要がない。

あちこちのスラッシュの有無で、期待通りに動かなかったので、そこが難しかった。

**** サーバサイドとクライアントサイドでのVuexストアの扱い
今回はTODOアプリを作っているので、TODOリストの各アイテムの情報をVuexストアに格納して、
ユーザ操作に応じて最終的なDBとの同期をとっている。

本来であれば、サーバサイドで一度Vuexストアに情報を格納したあと、
以降はクライアントサイドでいい感じに扱いたかったのだが、うまくいかなかったので
Vuexストアは結局すべてクライアントサイドに寄せている。

詳細は以下のふたつのソースを見たほうが確実だと思う。

https://github.com/grugrut/todo-springboot/blob/5017ac9b720b8d1d7e6462a4ff439db0a9dfdd07/src/frontend/pages/index.vue

https://github.com/grugrut/todo-springboot/blob/5017ac9b720b8d1d7e6462a4ff439db0a9dfdd07/src/frontend/store/index.js

このVuexストアの書きかたは、すでに非推奨になっているので書きかえが必要だが。

*** まとめ
フロントエンドをNuxtJS、バックエンドをSpringBootとするシンプルなTODOアプリができた。
このあとは、コンテナ化してkubernetesなどで動かす予定。
** DONE ECS上のSpring Boot ActuatorのメトリクスをCloudWatchに送信する :springboot:aws:ecs:fargate:cloudwatch:
   CLOSED: [2020-09-06 Sun 23:59]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-09\")
:EXPORT_FILE_NAME: 202009061723
:END:

起動タイプがFargateのECS上で動かすアプリのCPU使用率やメモリ使用率などは、
=Container Insights= を有効にするだけで簡単にCloudWatchに送信することができる。

その一方で、例えば上で動くアプリがJava製の場合にヒープ使用率などのメトリクスをCloudWatchと連携する方法がわからなかったのでやってみた。

なんか、もっと良い方法があれば教えてください。
正直、datadog使って連携しちゃうのが一番てっとりばやいんだろうなとは調べてて思った。
*** Spring Boot Actuator
まず、上で動かすアプリだが、 =Spring Boot= を使う場合は、 =actuator= を使うと簡単にメトリクスがとれる。
=Spring Boot Actuator= は、本番対応機能ということで本番環境での運用で便利な機能を提供してくれるもので、
ヘルスチェックエンドポイントを公開したり、各種メトリクスを公開することができる。

https://spring.pleiades.io/spring-boot/docs/current/reference/html/production-ready-features.html
*** CloudWatchと連携する
Spring Boot Actuatorだが、メトリクスの公開先にいろいろと種類があり、datadogやら、prometheusやら選ぶことができる。
その中に、ドキュメントには記載されていないものの、CloudWatch連携があるのだが、Fargateでは上手く動かすことができなかった。
もろもろ設定しているにもかかわらず、ローカルアドレスのメタデータのエンドポイントで自身の情報を得ようとして、
Fargateでは当該エンドポイントにアクセスすることができず失敗してしまうのだ。
EC2だったらいけるのかな、とも思ったのだが、今回はFargateでやりたかったので、
いろいろ調べた結果StatsD連携するのが、もっともよいのではという結論に至った。
**** StatsD連携する
Spring Boot Actuatorには、StatsD連携するための仕組みが準備されているので、
=pom.xml= に以下の依存関係を加えるだけで、簡単に連携することができる。

#+begin_src xml
<dependency>
	<groupId>org.springframework.boot</groupId>
	<artifactId>spring-boot-starter-actuator</artifactId>
</dependency>
<dependency>
	<groupId>io.micrometer</groupId>
	<artifactId>micrometer-registry-statsd</artifactId>
</dependency>
#+end_src

あわせて、送信先の設定を =application.properties= に設定しておく。
たぶん、調べる限りはすべてデフォルト設定なので、設定しなくても大丈夫かも。

#+begin_src properties
management.metrics.export.statsd.enabled=true
management.metrics.export.statsd.flavor=telegraf
management.metrics.export.statsd.host=localhost
management.metrics.export.statsd.port=8125
management.metrics.export.statsd.max-packet-length=1400
management.metrics.export.statsd.polling-frequency=10s
management.metrics.export.statsd.publish-unchanged-meters=true
#+end_src
**** CloudWatchエージェントをSideCarとして動かす
起動タイプFargateの場合は、Spring Bootから送信されるStatsDメトリクスを受信するために、
CloudWatchエージェントをECSタスクのサイドカーとして起動する必要がある。

そのために、まずはCloudWatchエージェントでStatsDメトリクスをあつかえるように
CloudWatchの設定を書く必要がある。
Fargateの場合は、直接設定を書けないので、 =SSM= のパラメータで以下の設定をしておく。

#+begin_src json
{
   "metrics":{
      "metrics_collected":{
         "statsd":{
            "service_address":":8125",
            "metrics_collection_interval":60,
            "metrics_aggregation_interval":300
         }
      }
   }
}
#+end_src

これをパラメータストアに入れておき、サイドカーのCloudWatchエージェントの
環境変数で、こちらを =valueFrom= として設定するとよい。

ECSの場合、同じタスクの中のコンテナは、 =localhost= でポートを共有しており、
=localhost:8125= にメトリクスを送信するようにSpring Boot側で設定をしているので、
ヒープ使用率などのメトリクスをCloudWatchに送信することができる。
** DONE HelmからIvyに差分小さく移行する                      :emacs:helm:ivy:
   CLOSED: [2020-09-16 Wed 06:54]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-09\")
:EXPORT_FILE_NAME: 202009160653
:END:

いろいろあって、HelmからIvyに移行を進めている。
いろいろは以下の理由からだが、特に前2つによるものが大きく、でも移行面倒くさいと思っていたところに最後のが決定打となった。

- posframeとの相性が良くない

  視線移動を最小化できるposframeは便利。

  helm-posframeパッケージはあるものの、posframe表示中に、それが残ったまま新しいposframeが表示されてしまう事象が多発してしまう。
  =posframe-hide-all= を実行すれば消せるがさすがに厳しい。
  とはいえ、発動条件もよくわからず解析が厳しそうだな、と思っていた。
- 一時バッファが作られてしまう

  一時バッファが出ること自体はしょうがない。 =switch-to-buffer= で表示しないとか制御すれば普段は気にならない。

  ただ、 =mid-night= との相性が悪く、一時バッファが変に削除されると動かなくなってしまう。
  そんなに発動するものではないので、最近は遭遇しなくなっているが、調べなきゃな〜とは思っていた。 
- helmの開発が終了してしまった

  https://github.com/emacs-helm/helm/issues/2386

  プロジェクトもアーカイブされてしまった。Helmの最新機能を追いかけてるわけではなく、
  問題はすぐには起きないと思われる。
  また、利用者の多いパッケージなので、たぶんフォーク版ができて使い続けられるのではないか。
  と思ったが、その状態で上記解決を図るのとivyに移行してみるのと迷って、ivyに移行することにした。
*** helmからivyへの移行

ざっと使うだけだったらrequireするのを、 =helm= から =counsel= に変更すれば終了である。

詳しいところは、去年のEmacs勉強会でtakaxpさんが発表されてたので、それを見るのがお勧め。

https://qiita.com/takaxp/items/2fde2c119e419713342b

https://speakerdeck.com/takaxp/counsel

*** helmで使っていた機能をivyで実現する

基本的に、考え方として良くない。

郷に入っては郷に従え、と言うが別のパッケージなのだから同じことをしようと考えることが間違いである。

とはいえ、手癖になってるものを変えるのはストレスなのでいくつか実現することにした。

**** helm-mini相当の実現

helm-miniは、バッファリスト + Recentf から選択することができる。

これはシンプルで =ivy-use-virtual-buffers= を =nil= 以外にすることで =ivy-switch-buffer= の情報源に
Recentfを含めることができる。
自分は入力しやすいように ~C-;~ にバインドしている。

**** find-file中に C-l で上の階層に移動する
=helm-find-file= では、 ~C-l~ で上のディレクトリに移動することができた。

=counsel-find-file= でも、 ~C-DEL~ および ~C-BS~ で同じことがおこなえるらしいが、
~C-l~ でも使えるようにしたかったので、 =counsel-up-directory= を割当て。

**** C-zによるチラ見アクションを実現する

helmでは候補選択中に ~C-z~ に、選択を終了することなく設定された操作をおこなう =helm-execute-persistent-action= が割当てられていた。
例えばディレクトリなら移動、ファイルやバッファなら内容を表示、 ~M-x~ ならコマンドの情報を表示など。
(※途中でC-zの割当てが突然消されてしまったので、そこからは自分で設定していた)

ivyでも ~C-M-m~ でミニバッファを閉じずにファイルやバッファの中身を表示することができるが、
ディレクトリに対する操作が、移動ではなくてディレクトリの中身を表示で期待する動作と違うため自身で関数を定義した。
~M-x~ 時の挙動を書けていないので、この辺は徐々に育てていかないといけない。

*** まとめ
上記の3つの設定により、普段の操作で違和感が生じることがかなり軽減されたので
かなりコスト低くhelmからivyに移行することができた。

ivyはhydraとの連携がやりやすいなど、ivyの良さがいろいろあるようなので、その辺もカスタマイズしていきたい。

最後に、現時点のivyの設定を載せておく。上記3つの内容もこちらに含まれているので、似たようなことを実現したい人の参考になれば。

#+begin_src emacs-lisp
(leaf counsel
  :ensure t
  :require t
  :init
  (global-unset-key (kbd "C-z"))
  :config
  (ivy-mode 1)
  :custom
  (ivy-use-virtual-buffers . t)
  (ivy-wrap . t)
  (ivy-count-format . "(%d/%d) ")
  :bind
  (("C-;" . ivy-switch-buffer)
   ("C-x C-f" . counsel-find-file)
   ("M-x" . counsel-M-x)
   (ivy-minibuffer-map
    ("C-z" . grugrut/ivy-partial))
   (counsel-find-file-map
    ("C-l" . counsel-up-directory)))
  :preface
  (defun grugrut/ivy-partial ()
    "helmの `helm-execute-persistent-action' に近いものを実現する.
完全に同じものは無理だったので、ディレクトリなら入る、それ以外はできるだけ補完しバッファは抜けない動作をおこなう."
    (interactive)
    (cond
     ((eq (ivy-state-collection ivy-last) #'read-file-name-internal)
      ;; ファイルオープン
      (let (dir)
        (cond
         ((setq dir (ivy-expand-file-if-directory (ivy-state-current ivy-last)))
          ;; ディレクトリなら入る
          (ivy--cd dir))
         (t
          ;; それ以外ならチラ見アクション
          (ivy-call)))))
     (t
      (ivy-call)))))
#+end_src
** DONE kubernetesを1.17から1.18にバージョンアップする           :kubernetes:
   CLOSED: [2020-10-10 Sat 09:33]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-10\")
:EXPORT_FILE_NAME: 202010072237
:END:
[[CRI-O + Kata containers + Weavenetでkubernetesをインストールする]] で作成したクラスタが
当時はUbuntu向けのCRI-Oのパッケージが1.17までしか用意されてなかったのでk8sも1.17で作成した。
最近見たら1.19.0まで作成されていたので、まずは1.18にバージョンアップしてみる。

どういう制約があるのかわかってないけど、CRI-Oはkubernetesのバージョンと1対1対応しているので
それに合わせたバージョンを入れなければならない。

https://github.com/cri-o/cri-o#compatibility-matrix-cri-o--kubernetes

ちなみにバージョンアップ前の状態はこんな感じ。

#+begin_src plain
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.8", GitCommit:"35dc4cdc26cfcb6614059c4c6e836e5f0dc61dee", GitTreeState:"clean", BuildDate:"2020-06-26T03:43:27Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.8", GitCommit:"35dc4cdc26cfcb6614059c4c6e836e5f0dc61dee", GitTreeState:"clean", BuildDate:"2020-06-26T03:36:03Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"linux/amd64"}

$ kubectl get node -o wide
NAME     STATUS   ROLES    AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
master   Ready    master   88d   v1.17.8   192.168.2.241   <none>        Ubuntu 18.04.4 LTS   4.15.0-109-generic   cri-o://1.17.4
node1    Ready    <none>   88d   v1.17.8   192.168.2.242   <none>        Ubuntu 18.04.4 LTS   4.15.0-109-generic   cri-o://1.17.4
node2    Ready    <none>   88d   v1.17.8   192.168.2.243   <none>        Ubuntu 18.04.4 LTS   4.15.0-109-generic   cri-o://1.17.4
node3    Ready    <none>   88d   v1.17.8   192.168.2.244   <none>        Ubuntu 18.04.4 LTS   4.15.0-109-generic   cri-o://1.17.4
#+end_src
*** バージョンアップ前の準備
https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md#urgent-upgrade-notes-1

リリースノートを見て、影響を受ける変更がないか確認する。

人によっては =kubectl run= で =deployment= が作れなくなったのが大きいかも。自分は =kubectl create= を使ってるので影響なし。
いくつかのkindのバージョン指定も使えなくなるので、そこも注意。ツールがやっちゃってる場合以外は影響なさそう。



*** kubeadmのバージョンアップ
    =apt-cache madison= で、現在利用できるバージョンの一覧を取得することができる。
    パッケージが提供されないことはまずなく、kubernetesのバージョンと一致してるのだから、そこから調べてもいいんだけど。

#+begin_src plain
$ apt-cache madison kubeadm
   kubeadm |  1.19.2-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
   kubeadm |  1.19.1-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
   kubeadm |  1.19.0-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
   kubeadm |  1.18.9-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
   kubeadm |  1.18.8-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
   kubeadm |  1.18.6-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
(後略)
#+end_src

1.18系の最新マイナーが、 =1.18.9-00= であることがわかるので、これをインストールする。

#+begin_src bash
sudo apt-mark unhold kubeadm && apt update && apt install -y kubeadm=1.18.9-00 && apt-mark hold kubeadm
#+end_src

*** コントロールプレーンのバージョンアップ

普通に、drainして可能なバージョンアップ先を確認して、実行するだけ。

#+begin_src bash
kubectl drain master --ignore-daemonsets
sudo kubeadm upgrade plan
sudo kubeadm upgrade apply v1.18.9
kubectl uncordon master
#+end_src

再度 kubectl version を実行すると、1.18.9に上がっていることが確認できる。
#+begin_src plain
kubectl version
Client Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.8", GitCommit:"35dc4cdc26cfcb6614059c4c6e836e5f0dc61dee", GitTreeState:"clean", BuildDate:"2020-06-26T03:43:27Z", GoVersion:"go1.13.9", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"18", GitVersion:"v1.18.9", GitCommit:"94f372e501c973a7fa9eb40ec9ebd2fe7ca69848", GitTreeState:"clean", BuildDate:"2020-09-16T13:47:43Z", GoVersion:"go1.13.15", Compiler:"gc", Platform:"linux/amd64"}
#+end_src

*** CRI-Oのバージョンアップ
https://cri-o.io/

こちらを見て、1.18.3をインストールした。これまでの1.17は削除しておく。

#+begin_src bash
apt purge -y cri-o-1.17
export OS=xUbuntu_18.04
export VERSION=1.18
echo "deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
echo "deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list

curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.key | apt-key add -
curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | apt-key add -

apt update
apt install -y cri-o cri-o-runc
#+end_src

crioのバージョンを見て、1.18であることを確認。

#+begin_src plain
crio version
Version:       1.18.3
GitCommit:     8ccff67ade1f1eb14952db799512bb2581d4ba39
GitTreeState:  dirty
BuildDate:     2020-08-04T01:56:09Z
GoVersion:     go1.14.4
Compiler:      gc
Platform:      linux/amd64
Linkmode:      dynamic
#+end_src

*** kubelet, kubectlのバージョンアップ
これもドキュメント通りなので、特筆事項はなし。

#+begin_src bash
sudo apt-mark unhold kubelet kubectl && apt update && apt install kubelet=1.18.9-00 kubectl=1.18.9-00 && apt-mark hold kubelet kubectl
sudo systemctl daemon-reload
sudo systemctl restart kubelet
#+end_src

これで、ノードのバージョンも1.18に上がったことが確認できる。

#+begin_src plain
kubectl get node
NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   90d   v1.18.9
node1    Ready    <none>   90d   v1.17.8
node2    Ready    <none>   90d   v1.17.8
node3    Ready    <none>   90d   v1.17.8
#+end_src

*** ノードのバージョンアップ
各サーバについて =kubeadm upgrade apply= する代わりに、 =kubeadm upgrade node= すること以外は変わらない。

*** まとめ
CKAの勉強をしているときにkubeadmを使ってクラスタをバージョンアップする方法も学んだので今回やってみた。
EKSなどのマネージドなものに比べると多少面倒ではあるが、意外と簡単だし詰まりポイントも少ない印象を受けた。

とはいえ、4台構成だからまだやっていけるけど、これが多くなるようだと単純に面倒なので =kubespray= とか使ってansibleで一気に回したい気持ちはある。
** DONE Ergodox EZをJIS配列で利用して5ヶ月経過したので振り返り      :ergodox:keyboard:
   CLOSED: [2020-10-17 Sat 11:33]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-10\")
:EXPORT_FILE_NAME: 202010140032
:END:

[[ergodox ezを購入した]] から5ヶ月ぐらい経過して、当初の思っていたとおりのことと、
そうでないことがもろもろとわかってきたので振り返りとしてまとめてみる。
*** 使ってみてよかったところ
**** 分離型はよい
最初は配置として左右のキーボードを大きく離して間にトラックボールを置くスタイルにしてたのだけど、
トラックボール操作のときに腕が無理してる感あったので、普通にキーボードの横に配置するスタイルに切り替えた。

もともと分離キーボードを使っていたが、このリモート中心になって座りっぱなしの状況下において
肩を開いておけるのはよい。

とはいえ、そもそも座りっぱなし画面に向かいっぱなしであることがダメージを与えていることにはかわりないので
一体型よりダメージ軽減できてるな、ぐらい。

ストレッチとか休憩しないとダメね。

でも打合せが1時間×6セットとか、どこで休憩しろって言うんだ……
**** column staggeredは慣れる
購入直後は、row staggeredからcolumn staggeredに切り替えたときに
タイポが多く苦労したが、意外とすぐに慣れる。

そして今度は逆に普通のキーボードが使えなくなってしまうかも、と思ったが、
そこも意外と最初はてこずるものの戻れる。
ただ、親指でSpace/Back Space/Delete/Enterを使うのに慣れてしまってるので、
その点で普通のキーボードに戻るのはつらい。
*** 使ってみてよくなかったところ
**** 赤軸意外とうるさい
これまでメカニカルキーボードはあまり検討して使ったことがなく、HHKBからの乗り換えだと
結構カチャカチャ音がうるさい。
赤軸って静かな部類と聞いていたし、事前にショップのそれぞれ試せるやつで確認した感じ
確かに静かでいいなと選んだのだけど。

静音赤軸とかピンク軸とか言うやつは、カタログスペック以上に重たくて疲れそうだったので嫌だった。

リモートで喋りながらキーたたいてると、「タイピング速いですね〜」と
想像の中の京都しぐさをされてしまう。
そんな速くないしね。
**** 全部のキーを使いこなせない
ergodox ezを選んだ理由のひとつとしてキーが豊富にあることで、
購入前にディスプレイに頭身大で移してシミュレートもしてたが、
いざ届いて使ってみると、使いにくいキーが結構出てくる。

イメージとしてはこんな感じ。青色は使いやすい。赤色は使いにくい。

[[file:images/20201017-ergodoxez-key-usability.png]]

右下の赤ゾーンを修飾キーとして使えると便利になりそう、とは思っている。
もともと、右シフトとか右コントロール使ってなかったので慣れの問題ではありそう。
*** 今の配列
購入時にとりあえず作った配列を、使っているうちにいろいろ修正して、
今は以下な感じに落ち着いている。

もろもろならってStandard/Lower/Raise/Adjustレイヤの4レイヤ構成にしている。

**** Standardレイヤ/Lowerレイヤ
[[file:images/20201017-ergodoxez-layer1-2.png]]

左の親指の位置にLowerレイヤの切替キー、
右の親指の位置にRaiseレイヤの切替キーを配置している。
左右同時押しでAdjustレイヤに切り変わる。

キーの前面に表示されているのがLowerレイヤの内容である。

もともとは一般的に左下にFnキーがあるキーボードが多かったので
それにならってそこもLower切替にしていたが、
親指切替に慣れたので開放してAltキーにした。
Emacsを使ってるとAltキーは結構使うので押しやすい位置にあると嬉しい。
かわりにもともとAltをおいていたところは、Ctrl+Altにしてみた。

それ意外はStandardレイヤは一般的なJIS配列にしている。

Lowerレイヤは、本家では記号入力用だが、ファンクション系を配置している。
先のおしにくいキーにも出てきたが、コーディングやシェル操作していて
バックスラッシュとパイプが押しにくいとつらいので
Lowerレイヤでホームポジションに持ってきているところが工夫点。

スクロールロックが時々暴発するので、もっと押しにくいところに
持っていこうかな、とも画策中。

**** Raiseレイヤ
[[file:images/20201017-ergodoxez-layer-3.png]]

今のところはRaiseレイヤにはマウス操作と音量操作だけいれている。

マウス操作便利って声が多かったのでつけてみたが、全然使いこなせてない。

音量操作は、このリモート会議全盛期において、片手で上げ下げできるの便利。
自分の発話のミュート/ミュート解除もやりたいのだけど、難しいんだろうな。

**** Adjustレイヤ
[[file:images/20201017-ergodoxez-layer-4.png]]

キーレイアウトを焼き込むためのリセットしか配置していない。

このレイヤももうちょっと使いたい。

*** まとめ
こんな感じで、不満はいくつかあれど便利に使えている。

最近、ergodox ezの製造販売元であるzsaに moonlander という新たなキーボードが
扱われるようになった。

https://www.zsa.io/moonlander/

ergodox ezに親指用にキーたくさんあっても結局押せないじゃん! ってのが、
キーを減らしてコンパクトになる感じで対応されていて、結構よさそう。

blankモデルがでたら欲しい、が、ergodox ezを使い続けるんだろうな。
** DONE OpenFaaSでお手軽API環境を手にいれる             :kubernetes:openfaas:
   CLOSED: [2020-12-06 Sun 23:50]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2020\" \"2020-12\")
:EXPORT_FILE_NAME: 202012060916
:END:


これは、[[https://qiita.com/advent-calendar/2020/kubernetes2][Kubernetes2 Advent Calendar 2020]]の6日目の記事です。

おうちにある、kubernetesクラスタを有効活用するため、FaaS基盤を簡単に作れる
[[https://www.openfaas.com/][OpenFaaS]]を導入しました。

組んであるkubernetesの環境は、以下でまとめてありますが、普通にkubeadmで構築したものです。
https://www.grugrut.net/posts/202007111344/

*** OpenFaaSとは
HTTP APIとしての、Functionの実行環境を簡単に作ることができます。

単にhttpリクエストを受けディスパッチするような実行環境だけでなく、
Function作成面においても色々な言語のテンプレートがあらかじめ用意されているので
開発者はFunctionの実装をおこなうだけで、APIを作ることができます。

なんなら、実装自体はコンテナが標準入力を受けて標準出力で返すだけなので
テンプレートが用意されてなくても簡単に作れると思います。

*** Why not Knative
そもそも比較するもんじゃないよね、という気はしつつ調べてたときに出てきてたので。

Knativeもこれまで使ったことなかったので、Knativeでもいいかなと思いつつ、
結局OpenFaaSにしました。

KnativeもFaaSとして使うことはできますが、FaaSのためだけの導入には
リッチすぎるかなと思ったのが主な理由です。
Knativeが目指しているのは、kubernetesにおける開発体験の全般の改善だと
理解しているので、広すぎるなあ、と。

Eventingとかpub/subとか、Service周りの改善とか、Knativeにも気になるところは多々あるので
見てみようとは思ってます。

*** インストール
**** CLIのインストール
OpenFaaSには、FunctionをビルドしたりデプロイするためのCLI =faas-cli= コマンドが用意されています。
公式ドキュメントでは、arkadeを使ったインストールが推奨されていますが、コマンド単体の導入も可能です。

#+begin_src bash
curl -sSL https://cli.openfaas.com | sudo sh
#+end_src

**** kubernetesへのデプロイ
kubernetesへのデプロイはhelm chartが既に用意されているので、これを使います。

 https://github.com/openfaas/faas-netes/blob/master/chart/openfaas/README.md

URLを公開するのに、IngressやLoadBalancer、NodePortなどを選べますが
手元の環境はMetalLBでtype: LoadBalancerが使えるようになっているのでこれを利用するようにインストールします。

#+begin_src bash
helm repo add openfaas https://openfaas.github.io/faas-netes/
helm repo update
helm install openfaas openfaas/openfaas \
     --namespace openfaas \
     --set functionNamespace=openfaas-fn \
     --set generateBasicAuth=true \
     --set serviceType=LoadBalancer
#+end_src

上の設定だと、OpenFaaSの実行基盤が、namespace =openfaas= に、デプロイするFunctionが namespace =openfaas-fn= で動きます。

デプロイしたりWebUIを参照するために、 =admin= ユーザが用意され、パスワードは自動生成されるので適当にsecretを覗いて確認しておきます。

#+begin_src bash
kubectl -n openfaas get secret basic-auth -o jsonpath="{.data.basic-auth-password}" | base64 -d
#+end_src

*** Functionを作成する
今回は、prometheusからメトリクスを取得するAPIを作りました。

最近在宅ワーカーになったので、部屋の温湿度やCO2濃度を計測し、prometheusに入れているのですが
これをpromqlを一々たたいたりgrafana見たりしなくても確認できるようにしたい。

**** ひな形を作成する
以下のコマンドで、Functionのひな形を作成することができます。
引数 =lang= で実装する言語を設定することができ、 =python= や =NodeJS= など
Functionで一般的に使われる他の言語のものも、もちろん用意されています。

詳細は、https://github.com/openfaas/templates

#+begin_src bash
faas-cli new prom-getter --lang go
#+end_src

これを実行すると実行ディレクトリ配下に設定用のyamlファイルと
言語に応じたひな形が作られるので、ここにAPIの実装を書くだけでよい。

例えば、実装言語をgoにした場合はhandler.goが作られるのでそれに実装すればよい。

実装自体は、githubにあげてあるのでそちらをご覧ください。

https://github.com/grugrut/prom-getter

**** ビルドする
OpenFaaSのいいところとして、コンテナのベースイメージが用意されていて、
ビルドもそっちでできることがあります。

ビルドする場合は以下のコマンドだけで、各言語に応じてコンテナビルドまでいくのがいいですね。
引数にいろいろ与えてあげることで、追加の設定もできますがビルド後のイメージ名の指定などはymlの中でできるので、
基本はこれだけでいいはず。

#+begin_src bash
 faas-cli build -f ./prom-getter.yml
#+end_src

このコマンド実行後はローカルでコンテナイメージができるところまでいくので、
あとはdocker pushして適宜イメージレジストリにpushしておけばOK。

**** デプロイする
デプロイも =faas-cli= ででき、以下のコマンドを実行するだけ。
ローカルなのでIPアドレスも書いてるけど、ビルドのときと同じyamlに加えて
デプロイ先のエンドポイントも指定してあげます。

#+begin_src 
 faas-cli deploy -f ./prom-getter.yml --gateway 192.168.2.202:8080
#+end_src

これだけでPodやServiceなど必要なものは作成されるので、ホントに関数の実装だけ書けば
簡単にAPIが実装できて簡単でした。

環境構築含め数時間レベルで構築・実装・デプロイまでできました。

*** TIPS
最後に、いろいろ試してみたところをば。

**** POSTメソッドしか対応していない
参照系のAPIであれば、GETでURLパスにパラメータを含めたかったのだけど、
POSTにしか対応していないっぽい。

**** パラメータの扱い
当然、実装の中にハードコーディングだけでなく、外部でパラメータを与えたいケースがあると思います。
その場合も、環境変数としてパラメータを与えることができます。

環境変数の与えかたは大きく分けて以下の二つ

- yamlに書く

  yamlの中に環境変数を書くことができます。ビルドとデプロイで別のyamlを使うこともできるので、
  先のPOSTにしか対応していないことから、今回は同じイメージにそれぞれ環境変数にqueryを与えることで
  複数のメトリクスに対応するようにしました

- デプロイ時に与える
  
  デプロイするときの引数として =--env KEY=VALUE= の形式で引数に加えることで、
  デプロイコマンド実行時にも変数を与えることができます。
  なので、環境ごとに変わってyamlに書いておきたくない値なんかも、こういった形で設定することが可能です。

ドキュメントを見るとSecretは名前を指定するだけで使えるっぽいので
パスフレーズとかはそっちを使うのがよいかも。

*** まとめ
簡単にFaaS環境が作れるOpenFaaSを試してみました。

たまにしか使わないものをPodとして常に用意しておくのは無駄なのでゼロスケールで用意したいとか、
Cron的に定期実行したいとか、やりたいことはもっとたくさんあるので
その辺は今後見ていこうと思います。

あと、OpenFaaSですが、公式のチュートリアルの他、最近はLinux Foundationによるトレーニングもあります。
無料だと演習は無くテキストだけなのですが、これも最初のとっかかりとしてはわかりやすくて良いと思うので興味あるかたは是非。
(OpenFaasの作成者である Alex Ellis からも、オススメされたので宣伝)

https://www.openfaas.com/blog/introduction-to-serverless-linuxfoundation/
* 2021
** DONE Emacsで現在利用しているメモリを確認する                       :emacs:
   CLOSED: [2021-01-11 Mon 23:00]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2021\" \"2021-01\")
:EXPORT_FILE_NAME: 202101032247
:END:

これは、[[https://qiita.com/advent-calendar/2020/emacs][Emacs Advent Calendar 2020]] の23日目の記事です。
12月書けなくて他の人に書いてもらったのだけど、せっかくまだ書かれてない日が残ってたのでアドベントカレンダーとして書いてみる。

さて、Emacsは明示的にGarbage Collectionがあるエディタであり、利用していると時々GCが発動する。

そして、GCが発生すると動きが止まってしまうので、GC発動の閾値のチューニングがおこなわれる。
特に起動時にGCが発生すると、起動時間に影響があるため起動時だけ一時的に閾値を上げるチューニングをする人すらいる。

これまで、感覚でまあこんなもんだろ、で閾値を設定していた。
だが、実際どれぐらい使っているかがわかれば適切な閾値設定ができるのではないか、
と考え、GC後にもどれだけのメモリを利用しているのかを出力するようにした。

*** GC後に現在の使用中のメモリを表示

あまり聞いたことなかったので一筋縄じゃいかないのかもと思ってソースコードも見ていたのだけど、
結局 =garbage-collect= の実行後で以下のリストが返されるので、それを加工すればよかった。

=(オブジェクト種別名 オブジェクトサイズ オブジェクト数)=

オブジェクトによっては、これに確保しているけど利用していない領域数も含めてくれるのもあったが、
確実に必要なものだけカウントすることにした。

ということで、以下のように =garbage-collect= にアドバイスを加えて完了。

#+begin_src emacs-lisp
  (defun grugrut/gc-debug-function (str)
    (let ((sum 0))
       (dolist (x str)
          (setq sum (+ sum (* (cl-second x) (cl-third x)))))
       (message "Used Memory: %d MB" (/ sum (* 1024 1024)))))
  (advice-add 'garbage-collect :filter-return #'grugrut/gc-debug-function)
#+end_src

*** GCMH
    garbage collectionは少なからずSTW(Stop The World)が発生してしまうので、
    できるだけストレスがないようにGCのチューニングはしたい。
    
    とはいえ、GCが発生しにくいようにチューニングするだけであり、GC自体はどうしても必要である。
    
    この影響をできるだけ抑えるアプローチの一つがGCMH(Garbage Collection Magic Hack)である。

    アプローチの内容はいたってシンプルで、操作がおきないときにGCを動かせば、
    STWしたとしても利用者は気にならないよね、というもの。

    GCMHは標準で入っているので、有効化すればすぐに使える。

    #+begin_src emacs-lisp
   (leaf gcmh
     :ensure t
     :diminish gcmh
     :custom
     (gcmh-verbose . t)
     :config
     (gcmh-mode 1))
    #+end_src


*** Emacs28のメモリ管理

    ちなみに、利用領域の取得方法を調べてるときにわかったのだけど、Emacs28から
    =garbage-collect-maybe= という新しいGC用の関数が増えるらしい。

    https://emba.gnu.org/emacs/emacs/-/blob/master/etc/NEWS#L2085

    NEWSを見る限りGCのトリガーを早めに発行するためのものとのこと。

    特に書かれてないので推測だけど、GCの頻発を嫌って単純に閾値を上げると、
    真にGCが必要な場合にその処理時間が長くなってしまう。

    そのためGCしてもいいよってときにこれを実行することで、
    都合がつくときはすぐ終わるGCを発動させて、それ以外のときにGCが発生しにくい制御をすることができるのでしょう。
    ソースコード読む限り、単に閾値をこの関数経由のときだけ小さくするだけで、
    GC自体は変わらなそうなので、これ使っても時間がかかるときはかかると思う。

    28のリリースが近付けばもっと情報が出てくるかもしれないので楽しみ。
    例えば、GCMHと組み合わせることで、いいかんじに裏でGCしてくれると嬉しい。
** DONE Emacsに来たnative compileを試す                               :emacs:
   CLOSED: [2021-04-26 Mon 23:20]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2021\" \"2021-04\")
:EXPORT_FILE_NAME: 202104262248
:END:

Emacsのlispファイルをnativeでcompileする、いわゆる =gccemacs= が
Masterブランチにマージされたので、さっそく試してみた。

https://git.savannah.gnu.org/cgit/emacs.git/commit/?id=289000eee729689b0cf362a21baa40ac7f9506f6
*** 環境
Windows10のwsl2上のUbuntu 20.04.2 LTS (Focal Fossa)

*** ビルド

    まずは前提となるパッケージをインストールする。
    #+begin_src bash
sudo apt install libgccjit-9-dev
    #+end_src

    次に最新のソースコードを取ってきてビルドする。
#+begin_src bash
  ./autogen.sh
  ./configure --prefix=/usr/local --with-xwidgets --with-mailutils --with-native-compilation
  make clean
  make
  sudo make install
#+end_src
    
特に詰まることもなく起動でき、以下のとおり =NATIVE_COMP= が有効になっていることが確認できた。

[[file:images/20200426-emacs-native-comp.png]]

*** ビルド後にやったこと

    =(require 'cl)= はするな系のエラーが大量に出てうっとうしかったので、
    以下を =early-init.el= に入れて抑制した。

    #+begin_src emacs-lisp
    (custom-set-variables '(warning-suppress-types '((comp))))
    #+end_src

    
*** ビルド後の確認
裏で =package.el= で取得したlisp群がnativeコンパイルされて、
=~/.emacs.d/eln-cache/= 配下に保管されていることを確認した。

elcファイルと違ってコンパイルされたELFファイルであることもわかる。

#+begin_src plain
$ file ~/.emacs.d/elpa/all-the-icons-20210425.1035/all-the-icons.elc
/home/grugrut/.emacs.d/elpa/all-the-icons-20210425.1035/all-the-icons.elc: Emacs/XEmacs v27 byte-compiled Lisp data

$ file ~/.emacs.d/eln-cache/28.0.50-0e028d40/all-the-icons-e248ace1-2752b37c.eln
/home/grugrut/.emacs.d/eln-cache/28.0.50-0e028d40/all-the-icons-e248ace1-2752b37c.eln: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, BuildID[sha1]=305839e28b83cfac289c5e04a54cd2c92c318029, not stripped
#+end_src
** DONE EmacsのNative Compilationの性能を測定する                     :emacs:
   CLOSED: [2021-04-27 Tue 23:01]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2021\" \"2021-04\")
:EXPORT_FILE_NAME: 202104272222
:END:

[[Emacsに来たnative compileを試す]] で =--with-native-compilation= オプション付で
Emacsをビルドして、とりあえず eln ファイルができて、elファイルがコンパイルされるところは
見たので、今日はその性能について見てみた。

当然ネイティブコンパイルするからには、高速化されていることに期待。
*** 検証用コード

https://www.emacswiki.org/emacs/EmacsLispBenchmark

を見て、バブルソートで測定するのが良さそうだったので拝借。
ただ、リストの数がある程度欲しかったので、10000のリストに対してソートするようにしている。

#+begin_src emacs-lisp
(defun bubble ()
  "."
  (let* ((list (mapcar 'random (make-list 10000 most-positive-fixnum)))
              (i (length list)))
         (while (> i 1)
           (let ((b list))
             (while (cdr b)
               (when (< (cadr b) (car b))
                 (setcar b (prog1 (cadr b)
                             (setcdr b (cons (car b) (cddr b))))))
               (setq b (cdr b))))
           (setq i (1- i)))
         list))
#+end_src

あとは、この関数を =benchmark-run= で実行することで、実行時間、GC回数、GC時間を取得することができる。

#+begin_src emacs-lisp
  (benchmark-run (bubble))
  ;; => (41.7818347 0 0.0)
#+end_src

GCされてしまうと時間にブレが生じるので、GCされてないことが確認できればOK。
*** 比較対象
比較の対象として、これまでの高速化手法であったバイトコンパイルと今回のネイティブコンパイルで比較してみた。
**** バイトコンパイルとの比較
=M-x byte-compile-file= でバイトコンパイルしたファイルをロードして同じく =benchmark-run= にかけた。
**** ネイティブコンパイルとの比較
=M-x emacs-lisp-native-compile-and-load= で開いているelファイルをネイティブコンパイルした上でロードできるので、
これを同じく =benchmark-run= にかけた。
*** 比較結果
以下のとおり、バイトコンパイルより、ネイティブコンパイルはさらに高速化されることがわかる。

|          | オリジナル(el) | バイトコンパイル(elc) | ネイティブコンパイル(elc) |
|----------+----------------+-----------------------+---------------------------|
| 1回目    |          41.78 |                  7.89 |                      5.70 |
| 2回目    |          40.49 |                  8.00 |                      5.90 |
| 3回目    |          40.79 |                  8.30 |                      6.02 |
| 平均     |          41.02 |                  8.06 |                      5.87 |
| elとの比 |           1.00 |                  0.20 |                      0.14 |


バイトコンパイルが苦手なものとかがあれば、それでも試してみたいけど、何がいいんだろうか。
** DONE 手元でECSが動く ECS Anywhereを試してみた                    :aws:ecs:
   CLOSED: [2021-05-29 Sat 09:25]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2021\" \"2021-05\")
:EXPORT_FILE_NAME: 202105290925
:END:

ECS AnywhereがGAされたということで試してみた。

https://aws.amazon.com/blogs/aws/getting-started-with-amazon-ecs-anywhere-now-generally-available/
*** ECS Anywhereとは
    2020年のre:Inventで発表された、任意の環境(オンプレやデータセンタなど)でECSのタスクをデプロイできるもの。
    
    2021年の前半にリリースされると聞いていて、発表時はけっこう盛り上った記憶があるが、
    いざリリースされて数日、あまり話に聞かない気がする。
    
*** 導入してみた
    ローカルに仮想マシン(Ubuntu 20)を建てて、そこに導入してみた。

    素のUbuntuを入れるだけで、特に追加のインストール物は無し。
    唯一、curlが入ってなくてインストールコマンドのサンプルがcurlだったのでそれをインストールしたぐらい。

    マネージメントコンソールでECSの画面にアクセスし、ECSインスタンスのタブを開いてみると、
    =Externalインスタンスの登録= というのが増えている。

    これをクリックすると、アクティベーションキーの有効期間やインスタンス数、インスタンスロールを
    選択する画面がでてきて、これを選ぶとインストール用のコマンドが表示されるようになっている。

    インストールコマンドは、インストールスクリプトをダウンロードして実行する簡易なもの。
    インストールスクリプトの中で、SSM agentやdockerなど必要なものは導入してくれる。

    インストールが終わるとECS Agentが起動していることが確認できる。
    #+begin_src plain
# docker ps
CONTAINER ID   IMAGE                            COMMAND    CREATED         STATUS                   PORTS     NAMES
8ec251e819a2   amazon/amazon-ecs-agent:latest   "/agent"   3 minutes ago   Up 3 minutes (healthy)             ecs-agent
    #+end_src

    マネコンの方でも、インスタンスが登録されていることがわかる。

    [[file:images/20210529-ecs-anywhere-container-instance.png]]
    
**** つまづいたポイント
     インスタンスロールに、 =AmazonSSMRoleForAutomationAssumeQuickSetup= が出てきたので
     これでいいのかなと選んでしまったが、ECS周りのポリシーが不足しているのでインストールでこけた。
     新しいロールを作成する、で作ってもらうか =AmazonSSMManagedInstanceCore= =AmazonEC2ContainerServiceforEC2Role=
     のポリシーがアタッチされたロールを作って設定するのが正解だったようだ。

     /var/log/ecs/ecs-agent.logに以下のようなログが出るので問題がどこにあるかはすぐにわかる。
#+begin_quote
level=error time=2021-05-28T23:36:26Z msg="Unable to register as a container instance with ECS: AccessDeniedException: User: arn:aws:sts::xxxxxxxxxxxx:assumed-role/AmazonSSMRoleForAutomationAssumeQuickSetup/mi-085ac36e8f491xxxx is not authorized to perform: ecs:RegisterContainerInstance on resource: arn:aws:ecs:ap-northeast-1:xxxxxxxxxxxx:cluster/ecs-sandbox" module=client.go
#+end_quote

*** サービスを起動する
   サービス起動タイプに =EXTERNAL= というのが増えているので、
   タスク定義の方も、 =EXTERNAL= 互換のTask Definitionを作成しておく必要がある。

   当然だが、ECRのプライベートレジストリのコンテナイメージも利用することができた。

   サービスの設定も基本的なところは変わらないが、注意点としてネットワークの設定やオートスケールの設定ができない。

   ネットワークの設定ができないので、ELBから接続したり、ECSサービスディスカバリーでタスク間通信をすることはできないようだ。

   単にコンテナIDを返してくれるテスト用のイメージを使って試してみたが、
   サービスを作成すると、手元に作成したインスタンス側でコンテナが起動したことおよび動作が確認できた。

   #+begin_src plain
# docker ps
CONTAINER ID   IMAGE                                                        COMMAND                  CREATED          STATUS                    PORTS                                         NAMES
5a6dc5f7a402   123456789012.dkr.ecr.ap-northeast-1.amazonaws.com/go-hello   "/bin/sh -c /bin/go-…"   29 seconds ago   Up 26 seconds             0.0.0.0:49153->8080/tcp, :::49153->8080/tcp   ecs-go-hello-1-go-hello-e09cf6f5e6f3e895b901
a371576de001   123456789012.dkr.ecr.ap-northeast-1.amazonaws.com/go-hello   "/bin/sh -c /bin/go-…"   29 seconds ago   Up 26 seconds             0.0.0.0:49154->8080/tcp, :::49154->8080/tcp   ecs-go-hello-1-go-hello-d08094a6fb85aefc1400
8ec251e819a2   amazon/amazon-ecs-agent:latest                               "/agent"                 14 minutes ago   Up 14 minutes (healthy)                                                 ecs-agent
# curl localhost:49153
Hello, World! My name is 5a6dc5f7a402
# curl localhost:49154
Hello, World! My name is a371576de001
   #+end_src
   
*** 所感
    想像以上に簡単にオンプレ側にコンテナインスタンスを作成することができた。

    ただ、ロードバランサーが無いのでAWS側に用意したものと連携する手段が無いように見える。
    AWS上以外で実行したいユースケースとしては、例えばコンプライアンス上の理由から、
    DBをクラウドに持っていけずローカルに置くしか無い場合に、必要以上の情報がAWS側に流れるのを防ぐために
    APIゲートウェイ的に使うとか、そういったことが考えられると思うが、
    AWS上のワークロードからの通信手段が無いと微妙と思った。
    
    まあ、Direct ConnectやVPNで繋いでれば確保できるし、普通はそうやるのだろうけど。

    あとはバッチ処理してS3に上げたいとかそういうことはできるのかな。

    タスクへのインバウンド通信の手段が無いといろいろ不便な気がするので、
    その辺は今後に期待なのかと思う。
** DONE EKSのPodのSecurityGroupを試す                               :aws:eks:
   CLOSED: [2021-07-26 Mon 23:50]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2021\" \"2021-07\")
:EXPORT_FILE_NAME: 202107250958
:END:

EKSは、3年前ぐらいに少しさわったぐらいで、あまり深くいじれてなかったので、再入門してみる。
今回は、2020年の9月ごろに利用できるようになった、Podへのセキュリティグループの割当てを試してみる。
*** セットアップ

    ドキュメントを参考にセットアップする。
    
 https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/security-groups-for-pods.html

 やることは、eksクラスタのIAMロールに =AmazonEKSVPCResourceController= マネージドポリシーを追加することと
 以下のコマンドで =aws-node= デーモンセットの環境変数でPodへのENI割当てを有効化するぐらい。

#+begin_src bash
   $ kubectl set env daemonset aws-node -n kube-system ENABLE_POD_ENI=true
#+end_src
*** 試してみる

    実際にPodに適用して試してみる。

    あまり複雑にしてもしょうがないので、nginxを3種類動かす。

    #+begin_src bash
      $ kubectl create deployment nginx --image nginx
      $ kubectl expose deployment nginx --port 80
      $ kubectl create deployment nginx2 --image nginx
      $ kubectl expose deployment nginx2 --port 80
      $ kubectl create deployment nginx3 --image nginx
      $ kubectl expose deployment nginx3 --port 80

      $ kubectl get pod --show-labels
      NAME                      READY   STATUS    RESTARTS   AGE   LABELS
      nginx-596bdcb889-qbvsf    1/1     Running   0          30s   app=nginx,pod-template-hash=596bdcb889
      nginx2-5fc4444698-srd9l   1/1     Running   0          20h   app=nginx2,pod-template-hash=5fc4444698
      nginx3-5465449d99-7jfww   1/1     Running   0          34m   app=nginx3,pod-template-hash=5465449d99
#+end_src

まずは、何もセキュリティグループを設定していない状態なので、
各Pod間で通信できることを確認する。

#+begin_src bash
      $ kubectl exec nginx-596bdcb889-qbvsf  -- curl nginx2 -s --head
      HTTP/1.1 200 OK

      $ kubectl exec nginx-596bdcb889-qbvsf  -- curl nginx3 -s --head
      HTTP/1.1 200 OK

      $ kubectl exec nginx2-5fc4444698-srd9l -- curl nginx -s --head
      HTTP/1.1 200 OK

      $ kubectl exec nginx2-5fc4444698-srd9l -- curl nginx3 -s --head
      HTTP/1.1 200 OK

      $ kubectl exec nginx3-5465449d99-7jfww -- curl nginx -s --head
      HTTP/1.1 200 OK

      $ kubectl exec nginx3-5465449d99-7jfww -- curl nginx2 -s --head
      HTTP/1.1 200 OK
#+end_src

同じセキュリティグループからのTCP80を許可するセキュリティグループを作成し、内容を確認する。

#+begin_src bash
  $ aws ec2 describe-security-groups --group-ids sg-0ee9cf2b805966c5a
  {
      "SecurityGroups": [
          {
              "Description": "Allow access to 80",
              "GroupName": "eks-sg-policy-nginx",
              "IpPermissions": [
                  {
                      "FromPort": 80,
                      "IpProtocol": "tcp",
                      "IpRanges": [],
                      "Ipv6Ranges": [],
                      "PrefixListIds": [],
                      "ToPort": 80,
                      "UserIdGroupPairs": [
                          {
                              "GroupId": "sg-0ee9cf2b805966c5a",
                              "UserId": "123456789012"
                          }
                      ]
                  }
              ],
              "OwnerId": "123456789012",
              "GroupId": "sg-0ee9cf2b805966c5a",
              "IpPermissionsEgress": [
                  {
                      "IpProtocol": "-1",
                      "IpRanges": [
                          {
                              "CidrIp": "0.0.0.0/0"
                          }
                      ],
                      "Ipv6Ranges": [],
                      "PrefixListIds": [],
                      "UserIdGroupPairs": []
                  }
              ],
              "VpcId": "vpc-0d0d85fab71542349"
          }
      ]
  }

#+end_src
次に =nginx= と =nginx2= にだけセキュリティグループが割当てられるようにラベルをつける。
#+begin_src bash
  $ kubectl label pod nginx-596bdcb889-qbvsf sg=true
  pod/nginx-596bdcb889-qbvsf labeled
  $ kubectl label pod nginx2-5fc4444698-srd9l sg=true
  pod/nginx2-5fc4444698-srd9l labeled
  $ kubectl label pod nginx3-5465449d99-7jfww sg=false
  pod/nginx3-5465449d99-7jfww labeled

  $ kubectl get pod --show-labels
  NAME                      READY   STATUS    RESTARTS   AGE     LABELS
  nginx-596bdcb889-qbvsf    1/1     Running   0          3m58s   app=nginx,pod-template-hash=596bdcb889,sg=true
  nginx2-5fc4444698-srd9l   1/1     Running   0          20h     app=nginx2,pod-template-hash=5fc4444698,sg=true
  nginx3-5465449d99-7jfww   1/1     Running   0          38m     app=nginx3,pod-template-hash=5465449d99,sg=false
#+end_src
最後に、 =SecurityGroupPolicy= を作成して、 =sg=true= のラベルがついたPodにのみ、
先ほど作成したTCP80を許可するセキュリティグループが割り当てられるように設定する。
#+begin_src bash
  $ cat <<EOF | kubectl apply -f -
  apiVersion: vpcresources.k8s.aws/v1beta1
  kind: SecurityGroupPolicy
  metadata:
    name: sgp-eks-sg-policy-nginx
  spec:
    podSelector: 
      matchLabels:
        sg: "true"
    securityGroups:
      groupIds:
      - sg-0ee9cf2b805966c5a
  EOF
#+end_src
これで  =nginx= と =nginx2= 間は通信できて、 =nginx3= からは通信できなくなる、はずだったのだが、通信できてしまった。
#+begin_src bash
  $ kubectl exec nginx3-5465449d99-7jfww -- curl nginx -s --head
  HTTP/1.1 200 OK
#+end_src

どうやら、 =SecurityGroupPolicy= を作成すると、該当するPodにENIが割り当てられることで、
セキュリティグループを利用できるようになるのだが、その動作はPod起動時にしか反映されないらしい。

しょうがないので、Deploymentの設定をいじって、Podを再作成する。
Podの再作成後、再度ラベルが付与されていることを確認する。

#+begin_src bash
  $ kubectl get pod --show-labels
  NAME                      READY   STATUS    RESTARTS   AGE   LABELS
  nginx-8667fff857-wsdsd    1/1     Running   0          49s   app=nginx,pod-template-hash=8667fff857,sg=true
  nginx2-7bf6674f46-2nb2v   1/1     Running   0          31s   app=nginx2,pod-template-hash=7bf6674f46,sg=true
  nginx3-5b6ffc5664-wl8k5   1/1     Running   0          18s   app=nginx3,pod-template-hash=5b6ffc5664,sg=false
#+end_src

ラベルが付与されていることを確認できたので、再度通信テストしてみる。

今度は期待通り、 =nginx3= からの通信はおこなえず、それ以外の通信は許可されていることがわかる。
終了コードも7なので、ホストに接続できなかったことを示している。

#+begin_src bash
  $ kubectl exec nginx-8667fff857-wsdsd  -- curl nginx2 -s --head
  HTTP/1.1 200 OK
  $ kubectl exec nginx-8667fff857-wsdsd  -- curl nginx3 -s --head
  HTTP/1.1 200 OK
  $ kubectl exec nginx2-7bf6674f46-2nb2v  -- curl nginx -s --head
  HTTP/1.1 200 OK
  $ kubectl exec nginx2-7bf6674f46-2nb2v  -- curl nginx3 -s --head
  HTTP/1.1 200 OK
  $ kubectl exec nginx3-5b6ffc5664-wl8k5 -- curl nginx -s --head
  command terminated with exit code 7
  $ kubectl exec nginx3-5b6ffc5664-wl8k5 -- curl nginx2 -s --head
  command terminated with exit code 7
#+end_src

EC2でしか利用できずFargateでは利用できないとか、EC2に割り当てられるENIの上限がそのままノードで起動できるPod数の上限になるとか、
使い勝手が必ずしもよいとは言えないが、NetworkPolicyよりも手軽には使えるな、という印象。
*** Podの違い

=kubectl describe= で見てみると、セキュリティグループを割り当てたPodには
割り当てていないPodよりもいくつか設定が追加されていることが確認できた。
**** セキュリティグループを割り当てないPod
     いたって普通の設定内容。
     
     #+begin_src bash
   $ kubectl describe pod nginx3-5b6ffc5664-wl8k5
   Name:         nginx3-5b6ffc5664-wl8k5
   Namespace:    default
   Priority:     0
   Node:         ip-10-0-0-161.ap-northeast-1.compute.internal/10.0.0.161
   Start Time:   Sun, 25 Jul 2021 01:40:25 +0000
   Labels:       app=nginx3
                 pod-template-hash=5b6ffc5664
                 sg=false
   Annotations:  kubernetes.io/psp: eks.privileged
   Status:       Running
   IP:           10.0.0.180
   IPs:
     IP:           10.0.0.180
   Controlled By:  ReplicaSet/nginx3-5b6ffc5664
   Containers:
     nginx:
       Container ID:   docker://bee90795077ee863e602f81950d34c6572c1018b9bea99d3e0167f8957ab9ed9
       Image:          nginx
       Image ID:       docker-pullable://nginx@sha256:8f335768880da6baf72b70c701002b45f4932acae8d574dedfddaf967fc3ac90
       Port:           <none>
       Host Port:      <none>
       State:          Running
         Started:      Sun, 25 Jul 2021 01:40:29 +0000
       Ready:          True
   Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                    node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
   Events:
     Type    Reason     Age    From               Message
     ----    ------     ----   ----               -------
     Normal  Scheduled  4m38s  default-scheduler  Successfully assigned default/nginx3-5b6ffc5664-wl8k5 to ip-10-0-0-161.ap-northeast-1.compute.internal
     Normal  Pulling    4m37s  kubelet            Pulling image "nginx"
     Normal  Pulled     4m35s  kubelet            Successfully pulled image "nginx" in 2.441671981s
     Normal  Created    4m35s  kubelet            Created container nginx
     Normal  Started    4m34s  kubelet            Started container nginx
#+end_src

**** セキュリティグループを割り当てたPod
アタッチしたENIに関するアノテーションや、Tolerationsが追加されていることがわかる。
また、起動時のイベントを見てみても、最初はENIがアタッチされておらず起動できなかったのが、
     vpc-resource-controllerが動いてENIをアタッチすることで起動できたことがわかる。

 #+begin_src bash
   $ kubectl describe pod nginx-8667fff857-wsdsd
   Name:         nginx-8667fff857-wsdsd
   Namespace:    default
   Priority:     0
   Node:         ip-10-0-0-161.ap-northeast-1.compute.internal/10.0.0.161
   Start Time:   Sun, 25 Jul 2021 01:39:54 +0000
   Labels:       app=nginx
                 pod-template-hash=8667fff857
                 sg=true
   Annotations:  kubectl.kubernetes.io/restartedAt: 2021-07-25T01:31:55Z
                 kubernetes.io/psp: eks.privileged
                 vpc.amazonaws.com/pod-eni:
                   [{"eniId":"eni-0e89d1162f64567cf","ifAddress":"06:e3:5d:e6:08:31","privateIp":"10.0.0.48","vlanId":1,"subnetCidr":"10.0.0.0/24"}]
   Status:       Running
   IP:           10.0.0.48
   IPs:
     IP:           10.0.0.48
   Controlled By:  ReplicaSet/nginx-8667fff857
   Containers:
     nginx:
       Container ID:   docker://8b8288860b60c8f5212992706a1b7ee07acc3bfab95f2c386b21e7a8158fc9d4
       Image:          nginx
       Image ID:       docker-pullable://nginx@sha256:8f335768880da6baf72b70c701002b45f4932acae8d574dedfddaf967fc3ac90
       Port:           <none>
       Host Port:      <none>
       State:          Running
         Started:      Sun, 25 Jul 2021 01:39:58 +0000
       Ready:          True
       Limits:
         vpc.amazonaws.com/pod-eni:  1
       Requests:
         vpc.amazonaws.com/pod-eni:  1
       Environment:                  <none>
   Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                    node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
                    vpc.amazonaws.com/pod-eni:NoSchedule op=Exists
   Events:
     Type     Reason                  Age   From                     Message
     ----     ------                  ----  ----                     -------
     Normal   Scheduled               99s   default-scheduler        Successfully assigned default/nginx-8667fff857-wsdsd to ip-10-0-0-161.ap-northeast-1.compute.internal
     Normal   SecurityGroupRequested  99s   vpc-resource-controller  Pod will get the following Security Groups [sg-0ee9cf2b805966c5a]
     Warning  FailedCreatePodSandBox  99s   kubelet                  Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container "863c69846507747844c18042d7ad2505b6316bcafd9395ca33f9480578599203" network for pod "nginx-8667fff857-wsdsd": networkPlugin cni failed to set up pod "nginx-8667fff857-wsdsd_default" network: add cmd: failed to assign an IP address to container
     Normal   ResourceAllocated       98s   vpc-resource-controller  Allocated [{"eniId":"eni-0e89d1162f64567cf","ifAddress":"06:e3:5d:e6:08:31","privateIp":"10.0.0.48","vlanId":1,"subnetCidr":"10.0.0.0/24"}] to the pod
     Normal   SandboxChanged          98s   kubelet                  Pod sandbox changed, it will be killed and re-created.
     Normal   Pulling                 97s   kubelet                  Pulling image "nginx"
     Normal   Pulled                  95s   kubelet                  Successfully pulled image "nginx" in 2.423182635s
     Normal   Created                 95s   kubelet                  Created container nginx
     Normal   Started                 95s   kubelet                  Started container nginx
     #+end_src

*** Podのセキュリティグループが利用できないワーカーノードを使った場合
Podのセキュリティグループは、利用できるインスタンスタイプが限定されている。
たとえばT3インスタンスのワーカーノードしか存在しない状態で起動しようとすると、
条件を満たすノードがみつからずスケジューリングできずに起動に失敗する。

#+begin_src bash
  $ kubectl describe pod nginx-8667fff857-qrxzj
(中略)
  Events:
    Type     Reason            Age                 From               Message
    ----     ------            ----                ----               -------
    Warning  FailedScheduling  1s (x3 over 46s)    default-scheduler  0/3 nodes are available: 1 Insufficient vpc.amazonaws.com/pod-eni, 2 node(s) were unschedulable
 #+end_src
* 2022
** DONE bitboardアルゴリズムでN-Queen問題を解く                   :algorithm:go:
CLOSED: [2022-01-18 Tue 23:43]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2022\" \"2022-01\")
:EXPORT_FILE_NAME: 202201142319
:END:
*** はじめに

探索アルゴリズムを組むにあたって、配列を使うと効率が良くないので
ビット列を使って節約することが知られている。
自分もオセロのCPUアルゴリズムを組んでいるときにビットボードの存在を知った。
ビットボードは書いたことがなく面白そうだったので、比較的簡単なアルゴリズムで書けるN-Queenをbitboardで解いてみることにした。
*** N-Queen問題
その名の通りだが、NxNのチェス版に、N個のクイーンを互いに取られない位置に配置する組み合わせを求める問題である。
クイーンは将棋の飛車と角行を組み合わせた動き、つまり縦横斜めに移動できるので、
その一直線上に複数のクイーンが配置されないようにする必要がある。
*** 作ったもの

[[https://github.com/grugrut/n-queen]]

こちらで作成した。
言語はとくにこだわりなかったのでgolangで。

配列で素直に解くナイーブな解法と、bitboardを使う解法の二種類で書いている。
bitboardの場合はuint64で表現している都合上、変数1つでは8x8の盤面までしか表現できないので、
変数4つ使って16x16まで表現できるものも実装してみた。
*** 時間計測
bitboardは省メモリももちろんだが高速化のために使われるので、
どれほど速度があがるかを見てみた。

| 盤面サイズ(NxN) | ナイーブ | bitboard |
|-----------------+----------+----------|
|               4 |    0.002 |    0.002 |
|               5 |    0.002 |    0.002 |
|               6 |    0.003 |    0.003 |
|               7 |     0.01 |    0.008 |
|               8 |    0.065 |    0.035 |
|               9 |    0.632 |    0.239 |
|              10 |    6.592 |    1.876 |
|              11 |   75.598 |   16.512 |

8x8ぐらいの盤面までは、ほとんど大差ないが、9x9ぐらいから目に見えて時間が違ってきた。
11x11になると、とりうるクイーンの配置パターンが1200兆通りぐらいあるはずなので、
そこから効率的に探索ができているということがわかる。
*** bitboardのしくみ
ビットボードでは、それぞれのセルを1bitで示し、
操作もビットの論理演算とシフト演算の組み合わせで実現する。

クイーンがいるセルのbitを1、空のセルを0としたbit列を用意しておくと、
例えば左方向については、1ビットずつビットシフトしたものの論理和を求めることで
左方向にクイーンが効いているセルをまとめて求めることができる。
ただし、単純にビットシフトしていくだけだと左端のものをビットシフトしたときに
繰り上がり(といってよいかわからないが)により、1つ上の行の右端に移動してしまうので、
0b011111110111111011111101111110111111011111101111110111111
との論理積をビットシフトしていくことで、左端のクイーンは0になり繰り上がりを防止することができる。

こうすることで、通常の配列だと1つずつループさせないといけないのに対してビットシフトなら
複数のクイーンについてまとめて計算できるので効率よく求めることができるというわけだ。

これを八方向に対して計算することで、今のクイーンが効いているセルを求めることができる。
この状態では、1はクイーンが配置できないセル、0はクイーンが配置できるセルであるので、
0のセルに配置しては効いてるセルを求め、結果N個のクイーンが配置できれば成功となる。

候補の探索をする際には、特定のビット列 bに対して、 =(b & -b)= を求めることで
bの一番下の1になっているビットだけが抽出できるため、
先ほどのビット列の否定(この場合は、1がクイーンが配置できるセルになるので、ここから1になっているところを抽出できれば、それが探索候補になる)に対して、
上記の計算をすれば、探索候補を順に求めていくことができる。

細かいところでは探索済のビット列を別に持っていて、何度も同じ探索をしないように、とかのロジックも入れているが
基本的な仕組みは以上である。
*** まとめ
uint64でビット演算を考えると意外と思い違いなどもあり、多少苦戦したが
思ったよりも簡単にN-Queenのビットボードは実装できた。

もともとの目的だったオセロのアルゴリズムについても今後ビットボードアルゴリズムで書き直して
短時間で強い探索ができるオセロAIを作成していきたい。

** DONE ECS上のSpringBootのPrometheusメトリクスをCloudWatchで収集する :aws:ecs:springboot:prometheus:cloudwatch:
CLOSED: [2022-05-01 Sun 13:07]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2022\" \"2022-04\")
:EXPORT_FILE_NAME: 202204241336
:END:
[[*ECS上のSpring Boot ActuatorのメトリクスをCloudWatchに送信する][ECS上のSpring Boot ActuatorのメトリクスをCloudWatchに送信する]]
の後、CloudWatchエージェント自体にPrometheusメトリクスを収集する機能が追加されていたが、
試せてなかったので、今回試してみた。

ほとんど公式ドキュメントに書いてあることをやってみただけだが、あちこちにちらばって記載されていて、ちょっとわかりにくい。
*** アプリケーション側の準備
まずは情報の取得元となるJavaアプリケーションを用意する。
SpringBootであれば、SpringBoot ActuatorがPrometheusエンドポイントを用意しているので、簡単にJavaアプリケーションの情報(ヒープやgcなど)をPrometheusのメトリクスとして公開することができる。

pom.xmlに以下の依存関係を設定する。actuatorはSpringBoot Starterで設定してしまうほうが多いので、 =micrometer-registry-prometheus= だけ追加するほうが多い気はする。
#+begin_src xml
  <dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-actuator</artifactId>
  </dependency>
  <dependency>
    <groupId>io.micrometer</groupId>
    <artifactId>micrometer-registry-prometheus</artifactId>
  </dependency>
#+end_src

そして、 =application.properties= に公開するエンドポイントを設定してあげればよい。
#+begin_src plain
management.endpoints.web.exposure.include=health,prometheus
#+end_src

これだけで、起動したアプリケーションに対して =/actuator/prometheus= のパスにアクセスすればPrometheusメトリクス形式で情報を取ることができる。超簡単。
*** アプリケーションのデプロイ

今回はECS Fargateで動かす。
Prometheusメトリクス取得するCloudWatch Agentはタスクの情報を取得して、ネットワーク経由でエンドポイントにアクセスして情報を収集してくれるので、
EC2のホストに導入しておく必要もなく、サイドカーにする必要もない。

そのため、メトリクスにアクセスできるようにSecurity Groupなどを設定しておけばよい。

公開するときは、せめてALBのパスルーティング設定などで =/actuator/= 配下に対してはインターネット経由ではアクセスできず、
VPC内部のプライベートIPアドレスレンジからしかアクセスできないようにしておくのがよい。
外部経由でいろいろ操作できると攻撃の足掛りにされてしまう危険があるので。

まじめにやる場合はもっと対策しましょう。
*** CloudWatch Agentのデプロイ
公式でCloudFormationが用意されているので、それを使えば簡単にデプロイできるが、
それほど複雑なことはやっていない。

以下のようなタスク定義を作成して、デプロイすればOK。

#+begin_src json
  {
  "executionRoleArn": "arn:aws:iam::0123456789012:role/EcsExecutionRole",
  "containerDefinitions": [
    {
      "dnsSearchDomains": null,
      "environmentFiles": null,
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/cwagent-prometheus-fargate",
          "awslogs-region": "ap-northeast-1",
          "awslogs-stream-prefix": "ecs"
        }
      },
      "portMappings": [],
      "cpu": 0,
      "environment": [],
      "secrets": [
        {
          "valueFrom": "CloudWatch-Prometheus-Config",
          "name": "PROMETHEUS_CONFIG_CONTENT"
        },
        {
          "valueFrom": "CloudWatch-Agent-Config",
          "name": "CW_CONFIG_CONTENT"
        }
      ],
      "memory": null,
      "memoryReservation": null,
      "stopTimeout": null,
      "image": "amazon/cloudwatch-agent",
      "startTimeout": null,
      "privileged": null,
      "name": "cloudwatch-agent"
    }
  ],
  "memory": "1024",
  "taskRoleArn": "arn:aws:iam::0123456789012:role/EcsTaskRole",
  "compatibilities": [
    "EC2",
    "FARGATE"
  ],
  "family": "cwagent-prometheus-fargate",
  "requiresCompatibilities": [
    "FARGATE"
  ],
  "networkMode": "awsvpc",
  "runtimePlatform": {
    "operatingSystemFamily": "LINUX",
    "cpuArchitecture": null
  },
  "cpu": "512",
  "revision": 1,
}
#+end_src

Dockerイメージは =amazon/cloudwatch-agent= が公開されているのでそれを利用すればよい。
肝となるのはシークレットの設定で、 =PROMETHEUS_CONFIG_CONTENT= の環境変数名でPrometheusの設定を、
=CW_CONFIG_CONTENT= の環境変数名でCloudWatch Agentの設定を投入することになる。

いろいろやりかたはあると思うが、 SSM ParameterStoreを使って外部から注入するのが一番お手軽でしょう。
*** Prometheusの設定
=PROMETHEUS_CONFIG_CONTENT= は、いわゆるPrometheusの設定をいれるので特別なことはない。
例えば1分間隔で取得するなら、以下のようなyamlを書けばよい。

#+begin_src yaml
  global:
    scrape_interval: 1m
    scrape_timeout: 10s
  scrape_configs:
    - job_name: cwagent-ecs-file-sd-config
      sample_limit: 10000
      file_sd_configs:
        - files: ["/tmp/cwagent_ecs_auto_sd.yaml"]
#+end_src

scrape_configsが肝にはなり、収集先のURLなどは CloudWatch Agentがファイル出力したのを参照するので、
そのパスを記述することになる。
*** CloudWatch Agentの設定
CloudWatch AgentもいわゆるCWAgentの設定なので、json形式で書く。
どうでもいいが、一つの情報取得のためにyamlやjsonとフォーマットが違うものを複数書こうとすると、結構混乱する。

#+begin_src json
{
  "logs": {
    "metrics_collected": {
      "prometheus": {
        "prometheus_config_path": "env:PROMETHEUS_CONFIG_CONTENT",
        "ecs_service_discovery": {
          "sd_frequency": "1m",
          "sd_result_file": "/tmp/cwagent_ecs_auto_sd.yaml",
          "docker_label": {},
          "task_definition_list": [
            {
              "sd_job_name": "text-changer",
              "sd_metrics_ports": "8080",
              "sd_task_definition_arn_pattern": ".*:task-definition/.*text-changer.*:*",
              "sd_metrics_path": "/actuator/prometheus"
            }
          ]
        },
        "emf_processor": {
          "metric_declaration": [
            {
              "source_labels": ["container_name"],
              "label_matcher": "^text-changer$",
              "dimensions": [["ClusterName","TaskDefinitionFamily"]],
              "metric_selectors": [
                "^process_files_max_files$",
                "^process_cpu_usage$",
                "^jvm_classes_unloaded_classes_total$"
              ]
            },
            {
              "source_labels": ["container_name"],
              "label_matcher": "^text-changer$",
              "dimensions": [["ClusterName","TaskDefinitionFamily","id"],
                             ["ClusterName","TaskDefinitionFamily","area"]],
              "metric_selectors": [
                "^jvm_memory_(used|committed|max)_bytes$"
              ]
            }
          ]
        }
      }
    },
    "force_flush_interval": 5
  }
}
#+end_src

=sd_result_file= に先ほどPrometheus側の設定にも記述したファイルパスを記述する。
=task_definition_list= に、収集したいメトリクスを指定するために、タスク定義のパターンマッチや
ポート、メトリクスのパスを指定する。

そして、実際に収集したいメトリクスは =emf_processor= 配下に指定することになる。

見ればだいたいどうやって設定するのかはわかると思うが、ソースとなるラベルとパターンマッチを指定して、
そのマッチしたタスクに対して、 =metric_selectors= で指定したメトリクスを収集することができる。

また、結果はCloudWatchのメトリクスとして指定されるので、その際のディメンションをどうするかも指定する。
配列になってるので、ひとつのメトリクスを複数のディメンションで切ることも可能。

そして、ドキュメント上、どういうディメンションが使えるのかよくわからなかったのだが、
CloudWatch Logsの方を見てみると、以下のように収集したメトリクスがログとして記録されている。
これがそれぞれディメンションに設定することができるので、例えばjvm_heapに対して heap領域かnon-heap領域かで分けて収集したいと思ったら、
=area= をディメンションに指定すれば分離できるんだな、ということがわかる。

#+begin_src json
{
    "ClusterName": "ecs-sandbox",
    "LaunchType": "FARGATE",
    "StartedBy": "ecs-svc/0926471211139162459",
    "TaskClusterName": "ecs-sandbox",
    "TaskDefinitionFamily": "text-changer",
    "TaskGroup": "service:text-changer",
    "TaskId": "c2411978f30e4d7e946572418de2fc15",
    "TaskRevision": "1",
    "Timestamp": "1651216730390",
    "Version": "0",
    "area": "nonheap",
    "container_name": "text-changer",
    "id": "Metaspace",
    "instance": "10.0.0.50:8080",
    "job": "text-changer",
    "jvm_memory_committed_bytes": 47972352,
    "jvm_memory_max_bytes": -1,
    "jvm_memory_used_bytes": 46019512,
    "prom_metric_type": "gauge"
}  
#+end_src
*** 収集結果とまとめ
これらを設定してCloudWatch Agentのタスクを起動すると、以下のように情報を取得し、
CloudWatch Agentで監視することができた。

[[file:images/20220429-cloudwatch-prometheus.png]]

設定してみての所感だが、CloudWatchだけでメトリクスを収集できるのは非常に便利。
ただ、いちいちどのメトリクスを収集するか、とか、収集の時点でディメンションを気にしないといけないといった
面倒さはあるので、Prometheusでそのまま収集してしまったほうが楽なのではないか、と思った。
** DONE ECSでfirelensを利用したログ収集で、標準出力と標準エラー出力の転送先を分ける :aws:ecs:firelens:
CLOSED: [2022-05-07 Sat 13:14]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2022\" \"2022-05\")
:EXPORT_FILE_NAME: 202205071017
:END:

ちょっと前に調べて、全然ネット上に情報がみあたらず結構困ったので、今さらだけど備忘として残しておく。

ECSでは、ログ収集に使うログドライバーをいくつかの種類から選択することができるが、
デフォルトでは =awslogs= が利用される。
これは、CloudWatch Logsに標準出力と標準エラー出力をそれぞれ転送するもので、手軽にログ収集の設定をするなら簡単に使える。
ただし、以下のように標準出力と標準エラー出力が区別なく混ざって出力されてしまうため、きちんとログとして見ようとするとわかりづらい。

#+begin_src text
2022-05-07T10:55:54.885+09:00	AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.17.0.2. Set the 'ServerName' directive globally to suppress this message
2022-05-07T10:55:54.887+09:00	AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.17.0.2. Set the 'ServerName' directive globally to suppress this message
2022-05-07T10:55:54.895+09:00	[Sat May 07 01:55:54.890732 2022] [mpm_event:notice] [pid 1:tid 139741816851776] AH00489: Apache/2.4.53 (Unix) configured -- resuming normal operations
2022-05-07T10:55:54.895+09:00	[Sat May 07 01:55:54.890848 2022] [core:notice] [pid 1:tid 139741816851776] AH00094: Command line: 'httpd -D FOREGROUND'
2022-05-07T10:56:14.880+09:00	10.0.1.128 - - [07/May/2022:01:56:14 +0000] "GET / HTTP/1.1" 200 45
2022-05-07T10:56:14.897+09:00	10.0.0.29 - - [07/May/2022:01:56:14 +0000] "GET / HTTP/1.1" 200 45
2022-05-07T10:56:16.747+09:00	10.0.0.29 - - [07/May/2022:01:56:16 +0000] "GET /httpd HTTP/1.1" 404 196
2022-05-07T10:56:22.181+09:00	10.0.0.29 - - [07/May/2022:01:56:22 +0000] "GET /httpd/ HTTP/1.1" 404 196
#+end_src
*** Firelens

ECSでは、標準のawslogs以外に、さらに高度なログ収集をするためにfirelensログドライバーが用意されている。
firelensは、実体はfluentd/fluentbitでそれをawsで使いやすいようにデフォルトでいくつかプラグイン導入したり、
いくつか初期設定がいれられている便利なものだ。

firelensもデフォルトでは標準出力、標準エラー出力を拾って処理するようになっている。
導入は簡単で、タスク定義の設定画面で、 =ログルーターの結合= という項目があるので、その中の
=Firelensの統合を有効にする= にチェックをつけてあげればよい。

そうすると、実体をfluentbitにするかfulentdにするかと、コンテナイメージの入力ボックスがある。
fluentbitの方が軽量なので基本的にはfluentbitを選ぶと良いだろう。
昔はfluentdの方が実績とプラグインが豊富という話も聞いたが、最近fluentdじゃないとできないことがあったってのは自分は聞いたことがない。

コンテナイメージも、AWSで用意しているものがあるので、特にカスタマイズ不要であればそれを選択すればよい。
カスタマイズについては後述するが、やはりちょっと複雑なことをしようと思うと、
AWSが用意しているのをベースに独自のイメージビルドが必要になる。

そして、適用をクリックすると、自動でサイドカーコンテナとして、タスクに =log_router= という名前のコンテナが追加される。
その後、元のメインコンテナの設定に戻ってログドライバーの設定を =awsfirelens= にする。

#+begin_export html
<span><i class="fas fa-exclamation-triangle"></i>&nbsp;注意事項</span>
#+end_export
ここで始めてfirelensを使う人がほぼハマるトラップがある。
ログオプションにKeyが =Name= のオプションが自動で生成されていると思うが、
これが値が空だとタスクの起動に失敗する。

=Name= に転送先(cloudwatchとかfirehoseとか)を設定し、それぞれに必要な追加のオプションもいれることで
それらにログ転送することができる。
例えば以下のように設定する。

|-------------------+---------------------|
| Key               | value               |
|-------------------+---------------------|
| Name              | cloudwatch          |
| region            | ap-northeast-1      |
| log_group_name    | /ecs/httpd-firelens |
| log_stream_prefix | ecs/                |
| auto_create_group | true                |
|-------------------+---------------------|

そして、そのログの出力結果が以下のような感じ。json形式でcloudwatch logsに出力される。
#+begin_src json
{
    "container_id": "1829176055cff356eb14e075e5590066701dcc430a4f5dad1a784030xxxxxxxx",
    "container_name": "/ecs-httpd-firelens-5-httpd-d6f5ed9ea1e4xxxxxxxx",
    "ec2_instance_id": "i-089c43b20xxxxxxxx",
    "ecs_cluster": "sandbox",
    "ecs_task_arn": "arn:aws:ecs:ap-northeast-1:123456789012:task/ecs-sandbox/949973cbc3b54734aeb1ab6exxxxxxxx",
    "ecs_task_definition": "httpd-firelens:5",
    "log": "[Sat May 07 02:37:28.480419 2022] [mpm_event:notice] [pid 1:tid 140549171129664] AH00489: Apache/2.4.53 (Unix) configured -- resuming normal operations",
    "source": "stderr"
}
{
    "container_id": "1829176055cff356eb14e075e5590066701dcc430a4f5dad1a784030xxxxxxxx",
    "container_name": "/ecs-httpd-firelens-5-httpd-d6f5ed9ea1e4xxxxxxxx",
    "ec2_instance_id": "i-089c43b20xxxxxxxx",
    "ecs_cluster": "sandbox",
    "ecs_task_arn": "arn:aws:ecs:ap-northeast-1:123456789012:task/ecs-sandbox/949973cbc3b54734aeb1ab6exxxxxxxx",
    "ecs_task_definition": "httpd-firelens:5",
    "log": "[Sat May 07 02:37:28.480544 2022] [core:notice] [pid 1:tid 140549171129664] AH00094: Command line: 'httpd -D FOREGROUND'",
    "source": "stderr"
}
#+end_src

先ほどのawslogsとcloudwatchに標準出力も標準エラー出力もどちらも出力されるが、 =source= がjson情報の中に含まれるので、それを元に必要な情報だけを取り出すことが多少はやりやすくなる。

*** Firelensの動作をカスタマイズする
さて、やっと本題であるが、もちろん全部のログをCloudWatchに送ってそれで問題なければよいのだが、
おそらく必ずしも全部のログをCloudWatchに送りたいかというと、そうでないケースも多々あるだろう。
CloudWatch LogsのPutLogEventの料金けっこうエグいし……。

特に監査用のログなどS3に保管しておけばよくてCloudWatchに送る必要がないものを振り分けたくなるはずだ。
他にも標準出力、標準エラー出力だけでなく、ログファイルを拾いたい場合もあると思う。

そのような場合は、firelensの設定をカスタマイズしてあげる必要がある。

カスタマイズの方法は以下の2パターンがある。
1. firelens.confをS3に配置して、タスク起動時にそこから読みこむ
2. 公式のfirelensコンテナイメージをベースイメージとして、カスタマイズしたものをビルドしてそれをサイドカーコンテナにする

ただし、データプレーンにFargateを採用している場合には、1のS3を利用するパターンは使えない。
おそらくセキュリティのためにホストのディレクトリマウントを制限しているからであろう。
そのため、Fargateを利用する場合は、2のイメージビルドが必須となる。

また、データプレーンにEC2を利用している場合でもfirelens.confで設定できるfluentbitの設定は、 =[INPUT]= 、 =[OUTPUT]= 、 =[FILTER]= ぐらいなので、たとえばfluentbitのストリーム処理を使ってより柔軟なログ収集がしたい場合などは、結局イメージビルドをする必要がある。

さて、今回の用途ではfirelens.confの設定だけで十分なので、S3にファイルを配置する方式で設定する。

firelens.confをS3から取得するための設定だが、なんとWeb画面のGUI上からは設定できず、
タスク定義のJSONをいじる必要がある。
なんで、この状況が放置されているのかは正直よくわからないので、そういうものだとあきらめる。

タスク定義の設定画面の下の方に =JSONによる設定= というボタンがあるので、それをクリックするとjsonの編集画面が表示される。
その中から、firelensコンテナ側の設定から以下の設定をみつける。

#+begin_src json
(前略)  
            "firelensConfiguration": {
                "type": "fluentbit",
                "options": null
            },
(中略)
            "name": "log_router"
(後略)
#+end_src
このfirelensConfiguration配下のoptionsを以下のように設定する。
もしかしたら、optionsの項目が存在しない場合もあるかもしれないが、その場合もoptionsごと追加すればよい。
以下は、ecs-configバケットにfirelens.confを配置している場合の例だ。

#+begin_src json
      "firelensConfiguration": {
        "type": "fluentbit",
        "options": {
          "config-file-type": "s3",
          "config-file-value": "arn:aws:s3:::ecs-config/firelens.conf"
        }
      },
#+end_src

firelens.confには例えば以下のような設定をいれる。
#+begin_src text
[OUTPUT]
	Name cloudwatch
	Match *
	region ap-northeast-1
	log_group_name /ecs/logs/httpd-access
	log_stream_name ecs/logs/httpd-$(tag)
	auto_create_group true
#+end_src

これはcloudwatch logsにすべてのタグにマッチするログを送信する設定になっている。

**** 標準出力・標準エラー出力の出力先を変える。
ここまでは公式情報も多く、比較的順調に進んだが、結局もともとの課題であった標準出力・標準エラー出力を振り分ける方法が情報がまるで無く結構大変だった。

通常であればソースが違えば付与されるタグが違うので、 =Match= ルールでそのタグにあわせて設定してあげれば簡単にログの出力先を変えることはできる。
だが残念ながら、firelensで標準出力・標準エラー出力を拾う場合、どちらも同じ =<container name>-firelens-<task ID>= というタグ名になってしまい、 =Match= ではこれが標準出力なのか標準エラー出力なのか、区別することができない。

もちろん、アプリ側のコンテナをいじって標準出力・標準エラー出力をファイルにリダイレクトしてあげて、
それを読みこめばできるのだが、それはそれでコンテナの基本原則に反しているようで極力やりたくない。

同じことやってる人がいないか探すと、AWSのブログでおなじみのclassmethodの記事がすぐにひっかかる。

https://dev.classmethod.jp/articles/storing-error-logs-and-all-logs-separately-in-firelens/

残念ながら、この記事でやっているのは標準エラー出力に対して何かをやっているのではなく、
1つのログの中からエラーのログだけを処理するということをやっている。

だが、この記事が大きなヒントになった。
タグが同じならタグをつけかえてしまえばよいのだ。

firelensの =[FILTER]= では、 =rewrite_tag= という、タグを書き換えるフィルタが存在するし、上記の記事でもこれを利用している。
タグ書き換えでは対象のログをしぼりこむRuleが存在する。

そして、このRuleはログの本文だけでなく、ログに存在する全てのjsonキーを利用することができる。
つまり、以下のログのキーが全て対象にできるということだ。

#+begin_src json
{
    "container_id": "1829176055cff356eb14e075e5590066701dcc430a4f5dad1a784030xxxxxxxx",
    "container_name": "/ecs-httpd-firelens-5-httpd-d6f5ed9ea1e4xxxxxxxx",
    "ec2_instance_id": "i-089c43b20xxxxxxxx",
    "ecs_cluster": "sandbox",
    "ecs_task_arn": "arn:aws:ecs:ap-northeast-1:123456789012:task/ecs-sandbox/949973cbc3b54734aeb1ab6exxxxxxxx",
    "ecs_task_definition": "httpd-firelens:5",
    "log": "[Sat May 07 02:37:28.480544 2022] [core:notice] [pid 1:tid 140549171129664] AH00094: Command line: 'httpd -D FOREGROUND'",
    "source": "stderr"
}
#+end_src

そのため、このjsonの中には、 "source" というキーが含まれていて、ここにstderr, stdoutが書かれているのでこれをrewrite_tagのルールにしてしまえばよい。

というわけでまとめると、以下のように設定することで標準出力だけをCloudWatch Logsに出力することができる。

#+begin_src text
[FILTER]
	Name rewrite_tag
	Match *-firelens-*
	Rule $source stdout log-stdout false

[OUTPUT]
	Name cloudwatch
	Match log-stdout
	region ap-northeast-1
	log_group_name /ecs/logs/httpd-access
	log_stream_name ecs/logs/httpd-$(tag)
	auto_create_group true
#+end_src

これを応用すれば、標準エラー出力をS3に送るなども類似の設定で実現することができる。

*** まとめ

ECSで柔軟にログ収集をするのにはfirelensを使うと非常に便利であるが、
標準出力・標準エラー出力を振り分けるといった、よくありそうなユースケースがなかなか情報がみつからないのは意外だった。
英語のサイトも含めて検索したが、単語が一般的すぎて、検索にうまくひっかけられないというのもあったとは思う。

ECSを使いこなすなら以下の本は、結構こまかく書かれてあり、よいと思う。
今回は省略したfirelensのストリーム設定を含めた使い方などECSを対象に広く深く書かれているので、
ECSを使いこなそうという人にはオススメの一冊だ。

#+begin_export html
<div class="card" style="width: 18rem; border: 2px solid #999999; padding: 8px;">
    <a href="https://www.amazon.co.jp/AWS%E3%82%B3%E3%83%B3%E3%83%86%E3%83%8A%E8%A8%AD%E8%A8%88%E3%83%BB%E6%A7%8B%E7%AF%89-%E6%9C%AC%E6%A0%BC-%E5%85%A5%E9%96%80-%E6%A0%AA%E5%BC%8F%E4%BC%9A%E7%A4%BE%E9%87%8E%E6%9D%91%E7%B7%8F%E5%90%88%E7%A0%94%E7%A9%B6%E6%89%80/dp/4815607656?_encoding=UTF8&qid=1651889384&sr=8-1&linkCode=li3&tag=grugrut-22&linkId=c789bf9f4eef8fdd4730bcd11c060a0b&language=ja_JP&ref_=as_li_ss_il" target="_blank"><img style="width: 177px;" border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&ASIN=4815607656&Format=_SL250_&ID=AsinImage&MarketPlace=JP&ServiceVersion=20070822&WS=1&tag=grugrut-22&language=ja_JP" ></a><img src="https://ir-jp.amazon-adsystem.com/e/ir?t=grugrut-22&language=ja_JP&l=li3&o=9&a=4815607656" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />
  <div class="card-body">
    <h5 class="card-title"><a href="https://amzn.to/3w9oxaY">AWSコンテナ設計・構築[本格]入門</a></h5>
    <p class="card-text"><a href="https://amzn.to/3w9oxaY">2021/10/21<br>株式会社野村総合研究所 (著), 新井雅也  (著), 馬勝淳史 (著), NRIネットコム株式会社 (監修), 佐々木拓郎 (監修)</a></p>
  </div>
</div>    
#+end_export

** DONE AWSの資格を全制覇した                                           :aws:certification:
CLOSED: [2022-11-19 Sat 12:22]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(2022 2022-11)
:EXPORT_FILE_NAME: 202211191217
:END:

タイトルの通りだけど、ようやく全資格(12冠)を取得したので振り返ってみる。

*** 取得した資格と取得順序
- Solution Architect Associate
- Solutions Architect Professional
- DevOps Engineer Professional
- Security Specialty
- Advanced Networking Specialty
- Database Specialty
- Data Analytics Specialty
- Cloud Practioner
- Developer Associate
- Machine Learning Specialty
- SysOps Administrator Associate
- SAP on AWS Specialty

の12個。Alexa SkillBuilderは全制覇のモチベが無いときに、終わってしまったので取ってない。
最初は業務で利用するサービスまわりのものだったり、設計してて難しいなって思ったまわりを試験勉強の中で身に付けようってのを
モチベーションとして受験していたが、Cloud Practionerを取りだした辺りで、ここまで来たら全部取るかってのがモチベーションに変わっていった。

*** 勉強方法
基本的に、試験ガイドを読んで範囲を確認し、模擬問題を解いて弱そうなところをBlackBeltの資料や
公式ドキュメントを読んで理解するという、あまり特別なことはしていない。

特に最後に受験したSAP on AWSは、SAP自体を触った経験が皆無で、SAPってよく聞くけど何ができるの？って知識から不足していたが、
AWS SkillBuildersで教材が提供されはじめたりして、SAPの用語の理解とあわせて学習を進めた。

最初のころはいわゆる黒本・白本と呼ばれる書籍を買って読んだりしてたが、
Solution Architect以降は、本でやらんでもいいかなってなっていたので
基本はWeb上のドキュメント等しか使ってない。

#+begin_export html
<div class="card" style="width: 18rem; border: 2px solid #999999; padding: 8px;">
    <a href="https://amzn.to/3giId8q" target="_blank"><img style="width: 177px;" border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&ASIN=4295010650&Format=_SL160_&ID=AsinImage&MarketPlace=JP&ServiceVersion=20070822&WS=1&tag=grugrut-22&language=ja_JP" ></a><img src="https://ir-jp.amazon-adsystem.com/e/ir?t=grugrut-22&language=ja_JP&l=li2&o=9&a=4295010650" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />
  <div class="card-body">
    <h5 class="card-title"><a href="https://amzn.to/3giId8q">(模擬問題付き)改訂新版 徹底攻略 AWS認定 ソリューションアーキテクト − アソシエイト教科書[SAA-C02]対応</a></h5>
    <p class="card-text"><a href="https://amzn.to/3giId8q">2021/1/8<br>鳥谷部 昭寛  (著), 宮口 光平 (著), 菖蒲 淳司 (著), 株式会社ソキウス・ジャパン (編集)</a></p>
  </div>
</div>    
#+end_export

基本的には、AWSのサービスについてなんとなく知っていれば、だいたい問題なかったと思うが、前述のSAP on AWSはSAPのことを知らないと何を聞かれてるのか理解できないことと、
Machine Learning SpecialtyはAWSのドキュメントにも記載はあるとはいえ、機械学習の基礎とかアルゴリズムについて問われがちで
AWS以外のことを知らないといけないという点で人によっては事前準備が必要かな、と思った。

*** まとめ
無事にAWSの資格の全制覇を達成した。
最近はGoogle Cloudの資格もちょくちょく取ってるので、その辺も記録に残していきたい。

** DONE Google Cloud Database EngineerをはじめGoogle Cloudの資格をいくつか取得しました :gcp:certification:
CLOSED: [2022-11-22 Tue 22:45]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(2022 2022-11)
:EXPORT_FILE_NAME: 202211222244
:END:

AWSの資格を全部取得してしまったので、Google Cloudの資格もいくつか取ってみるか、ということで取ってみました。

せっかくなので、それぞれの所感を簡単にまとめておきます。

*** Professional Cloud Architect
正確には最近取った資格ではなく、2019年に取得しました。
一度取った資格を、延長のために再受験することにモチベーションを保てず気付いたら失効させてしまっていました。
まあ、一度取っていれば自身のスキルを示すという観点では十分なんじゃないでしょうか。

とはいえ本資格は、自身のクラウドエンジニアとしてのキャリア上、一番始めに取得した資格(この後にAWS SAAとか取りました)なので、思い出深い資格ではあります。

このあと何回か触れるでしょうが、Google Cloudの資格は、結構Google Cloudのことを知らなくてもクラウドネイティブ設計とは、ってことがわかれば答えられるので受験難易度は低いと思います。
その分、PCA持ってるぜい!って設計とかレビューとかしようとするとGoogle Cloudのこと何も知らないので苦労することに……

*** Professional Database Engineer
2022年にGAになったばかりの比較的新しい資格で、まだ英語版の問題しか提供されていません。
最近、業務でも英語を使わざるを得なくなってきたものの、なんとなく苦手意識を感じていたのでテコ入れのために受験しました。

50問を2時間で解くということで問題文の解釈に多少は神経を使ったものの、
問題文と回答文なので簡潔にわかりやすく書かれており、問われる範囲も狭いのでそこまで難易度は高くないという印象でした。

基本的には、Cloud SQL、BigTable、Firebase、SpannerといったGoogle Cloudの代表的なデータベースサービスの概要とユースケースがわかり、
CloudSQLについてはサイジングとデータ移行、高可用性設計がわかればそんなに苦労はしないかと思います。

*** Professional Cloud Developer
今はクラウドエンジニアですが、以前はアプリケーション開発のキャリアだったので、
いつかは挑戦しようと思ってずるずるとここまで1年ぐらい来てしまっていたのですが、
試験範囲改訂されるということを知り駆け込みであわてて受験しました。
新範囲のgrpcなどは概要しか知らず使ったことがなかったので自身がなかったのじゃ…

これも、権限周り(GKEのサービスアカウントとGoogle CloudのIAMの連携とか)以外は、
Google Cloudに限らない一般的な開発知識が問われるので、駆け込みでも十分間に合ったなという印象です。

*** Professional Data Engineer
Database Engineerと名前が近く混同しがちですが、こちらはBigQueryとかその周辺のData Fusionなどが主体で問われます。
業務で最近BigQueryを使うことがあったので取ってみました。

これはさすがにGoogle Cloudのことをちゃんと問われますが、そこまで難易度は高くないかと思いました。

*** まとめ
AWSの資格と比べ、学習教材の量だと圧倒的に少ないGoogle Cloudの資格ですが、
難易度は良い意味で全体的に低め(変なことを聞かれないので、知るべきことを知ってれば対策不要)というのが特徴かと思います。

めぼしい資格は取ってしまったのでしばらくお休みかなと思いますが、おもしろそうなのがあれば取りにいきたいですね。
これで失効までの期間が2年ではなくもっと伸びてくれれば最高なのですが、なんとかならないですかね。

** DONE Emacs 29で導入される予定のおもしろい機能の紹介                :emacs:
CLOSED: [2022-11-24 Thu 23:03]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(2022 2022-11)
:EXPORT_FILE_NAME: 202211242303
:END:

気付いたらEmacsのバージョンも次は29というところまで進んできました。
最近あまりいじれていなかったのですが、NEWSを見てみるといろいろとこんな機能が入ったのかと思うのが多々あったのでご紹介します。

現時点では、Emacs 29がリリースされる日はわかりませんが、楽しみに待ちたいと思います。

というわけで、本稿はHEADのetc/NEWSからピックアップしてのご紹介となります。

https://emba.gnu.org/emacs/emacs/-/blob/master/etc/NEWS

*** フォントサイズの変更(グローバル反映)
=C-x C-M-+= 、 =C-x C-M--= で全てのバッファのフォントサイズが変更できるようになりました。
これまで、 =C-x C-+= =C-x C--= でカレントバッファのフォントサイズは変更できましたが、
それのグローバル版になります。

使い道が思いつくようなつかないようなという感じですが、プレゼンする時などにはよさそうですね。

*** コマンド実行 M-xとM-X
これまであったインタラクティブなコマンドを実行する =M-x (executive-extended-command)= がありましたが、
類似のコマンドとして =M-X= が追加されました。
これは =M-x= が全てのコマンドを実行できるのに対して、 =M-X= はカレントバッファに関係あるコマンドだけが候補になります。

Scratchバッファでそれぞれ実行してみると、 =M-X= のほうがコマンドの候補がはじめから絞られてることがわかります。

file:images/20221124-M-x-M-X.png

正直、絞り込みだったらIvyなりVerticoなり、その他絞り込みインタフェースが充実してるのでそちらを使うかな。
絞りこむには多少はコマンドを覚えてないといけないので、どうしても思いだせないときは役に立つかも。

*** 行の複製 duplicate-line/duplicate-dwim
=duplicate-line= =duplicate-dwim= が追加されます。
その名の通り行の複製や、選択箇所をいいかんじに複製してくれます。

emacs lispの手習いで書いたりすると思いますが、まさか公式採用されるとは思わなかったですね。
あまり頭を使わずに編集する場合に複製して差分だけいじりたいって思うことはありつつ、
自分はコマンドを自作してこなかったので結構うれしいです。

*** Emacsの再起動 restart-emacs
=restart-emacs= は、その名の通りEmacsを再起動するコマンドです。
再起動時は同じコマンドライン引数で起動してくれるようです。

パッケージいじってるときは結構再起動させて反映チェックすることが多いので、
嬉しいといえば嬉しいのかな……？

*** パッケージの更新 package-update

(追記) こちらリリース前にコマンドが修正されて、 =package-upgrade= になったようです。(追記ここまで)

これまでpackage.elでインストールしたパッケージを更新したい場合は、 =M-x list-packages= して
アップデートしたいものを選んで実行する必要がありました。
これが =package-update= コマンドで実現できるようになります。
=M-x package-update= すると、アップデートできるパッケージが候補に出てくるので、
そこから更新したいものを選んで実行すると簡単に更新できます。

すべての更新可能なパッケージを更新する =package-update-all= もあります。

これまでパッケージの更新は、 =package-refresh-contents= して、 =list-packages= して
=U= で一括選択して =x= で実行となんだかんだ面倒だったので、
これが =package-refresh-contents= =package-update-all= で済むのは多少楽になりそうです。
未確認なのは、list-packagesだとparadoxが使えて並列アップデートができているはずなので
=package-update-all= でもそれが利くのかどうか。
モードラインにはその表示がでなかったので数が多いと、もしかしたらlist-packagesの方が良いってこともありそうです。

*** (追記) 何度でも蘇えるさ scratch-buffer
Emacs起動時に生成され、Lispのちょっとした動作確認などにも使える =*scratch*= バッファですが、
誤って消してしまって途方にくれたことある方もいるのではないでしょうか？ 私はあります。

なんとそんな場合でも安心。 scratchバッファを再生成してくれる =scratch-buffer= コマンドが追加されました。
そもそもバッファ名が重要なので、 =*scratch*= という名前でバッファを作ればよいということが
Emacs-JP Slackで流れててそれ自体も目が点だったのですが、これで気軽にスクラッチできますね。

*** まとめ
今回は、Emacs 29の新機能から、本筋ではないちょっと面白系の更新をご紹介しました。
実はEmacs 29は、linumが撤廃されるとか結構アグレッシブな変更もあるようなので、
その辺も別途確認していきたいと思います。
** DONE ECS Service Connectを試す                                       :aws:ecs:
CLOSED: [2022-12-11 Sun 15:48]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(2022 2022-12)
:EXPORT_FILE_NAME: 202212111548
:END:

AWS ECS(Elastic Container Service)では、コンテナ間通信の方法がいくつか用意されています。
- ELBを使う方式
- Cloud Mapを使う方式
- AppMeshを使う方式

今回、re:Invent 2022にあわせて、新しい方式である ECS Service Connectが発表されたものの、
いまいちどういう動きをしているのかがドキュメント上だとわからなかったので試してみました。

*** ECS Service Connectとは
https://docs.aws.amazon.com/ja_jp/AmazonECS/latest/developerguide/service-connect.html

ドキュメントによると、サービスディスカバリとサービスメッシュの両方を構築できるとかかれています。
re:Inventでもセッションがありましたが、そこでもCloud Mapよりも機能性がよく、
AppMeshよりもシンプルな方式とかかれており、今後のデファクトにしたそうです。

これの執筆時点ではB/Gデプロイメントには対応していませんが、
re:InventのECSの今後について語るセッションで、今後ネイティブ対応でB/Gにも対応するという
説明もあったので、きっと2023年中にはELBを使わなくてもB/Gができるのではないか、と想像しています。

Cloud Mapの方式に似ているということで、それぞれ試してみました。

*** Cloud Mapによるサービスディスカバリとコンテナ間通信
実験用の構成として、nginxのサービスとubuntuのサービスの2種類を起動しました。
ubuntuの方は接続元として利用するだけなので、メインはnginxのほうが重要です。

**** 設定

nginxのServiceを作成するときに、 =サービスの検出の統合の有効化= にチェックをいれると
=Cloud Map= によるサービスディスカバリを設定することができます。
なお、新しいECSコンソールでは、この設定がどこにあるかみつからなかったので、
もしかすると今後はService Connectを使ってほしいということなのかもしれません。

[[file:images/20221211-ecs-servicediscovery.png]]

Cloud Mapによるサービスディスカバリでは、Route53にレコードが追加されていることが確認できます。

[[file:images/20221211-ecs-route53.png]]

**** 動作確認

実際に動いているコンテナが以下の感じ。

#+begin_src plain
# docker ps
CONTAINER ID   IMAGE                                    COMMAND                  CREATED             STATUS                       PORTS     NAMES
72e94b84444a   grugrut/my-nginx                         "nginx -g 'daemon of…"   2 minutes ago       Up 2 minutes                           ecs-nginx-2-nginx-84adfedfabc68ad00b00
38d5e35ec3ac   grugrut/my-nginx                         "nginx -g 'daemon of…"   2 minutes ago       Up 2 minutes                           ecs-nginx-2-nginx-ace884f199adade51800
08f47ec37110   amazon/amazon-ecs-pause:0.1.0            "/pause"                 2 minutes ago       Up 2 minutes                           ecs-nginx-2-internalecspause-a4b5b493eb84ced89a01
e0f55bea0d38   amazon/amazon-ecs-pause:0.1.0            "/pause"                 2 minutes ago       Up 2 minutes                           ecs-nginx-2-internalecspause-96b0c2a2a0a39d906400
80558e0dfbf8   grugrut/my-ubuntu                        "bash -c 'sleep 3600'"   9 minutes ago       Up 9 minutes                           ecs-ubuntu-6-ubuntu-84d8ec899dbdfec73500
58b91c46f1b3   amazon/amazon-ecs-pause:0.1.0            "/pause"                 9 minutes ago       Up 9 minutes                           ecs-ubuntu-6-internalecspause-bab982db83c5b6c42800
84c7988e86fa   ecs-service-connect-agent:interface-v1   "/usr/bin/agent"         About an hour ago   Up About an hour (healthy)             ecs---instance-service-connect-relay-929096c3f1de99a36300
561af0bc5a06   amazon/amazon-ecs-agent:latest           "/agent"                 2 hours ago         Up 2 hours (healthy)                   ecs-agent
#+end_src

コンテナの中に入って名前解決すると、先ほど名前をつけた =nginx.local= が解決できることがわかります
実装にもよりますが、ラウンドロビンでアクセスし、それぞれに対して接続ができていることも確認できました。
なお、わかりやすさのためにそれぞれのnginxのindex.htmlを変更し、どちらにアクセスしたかわかるように手を入れています。

#+begin_src plain
# nslookup nginx.local
Server:         10.0.0.2
Address:        10.0.0.2#53

Non-authoritative answer:
Name:   nginx.local
Address: 10.0.14.132
Name:   nginx.local
Address: 10.0.3.218

# curl nginx.local
contaienr 1
# curl nginx.local
container 2
#+end_src

*** Service Connectによるコンテナ間通信

続いてService Connectのほうを試してみます。
こちらは新しいECSコンソールでないと設定できないので注意しましょう。

**** 設定

若干コンソールの表示がおかしいもののService Connectというオプションが増えているので、
そこで設定をします。

[[file:images/20221211-ecs-serviceconnect.png]]

Service Connectの設定には、 =クライアント側のみ= と =クライアントとサーバー= の
2種類の設定があります。
リクエストを受ける側は =クライアントとサーバー= を選択し、
リクエストする側は =クライアント側のみ= を選択すればよさそうです。

**** 動作の確認

クライアントとサーバーの設定をするだけだと、
名前解決および通信をおこなうことができません。

#+begin_src plain
# nslookup web
Server:         10.0.0.2
Address:        10.0.0.2#53
 server can't find web NXDOMAIN

# curl web.local
curl: (6) Could not resolve host: web
#+end_src

これに対して、クライアント側の設定もしてあげると、通信ができるようになります。
ただし、名前解決をしてコンテナのIPアドレスに通信をするわけではないところが
Cloud Mapを使ったサービスディスカバリとは動作が大きく異なりますね。

接続にいっているIPアドレスが、 127.255.0.1 ということで、コンテナのIPアドレスに直接アクセスに
いっているわけでないことがわかります。

#+begin_src plain
# nslookup web
Server:         10.0.0.2
Address:        10.0.0.2#53

server can't find web: NXDOMAIN

# curl web
container 2
# curl web
container 1
# curl web -v
 *   Trying 127.255.0.1:80...
 * Connected to web (127.255.0.1) port 80 (#0)
> GET / HTTP/1.1
> Host: web
> User-Agent: curl/7.81.0
> Accept: */*
>
 * Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< server: envoy
< date: Sun, 11 Dec 2022 11:44:35 GMT
< content-type: text/html
< content-length: 12
< last-modified: Sun, 11 Dec 2022 11:43:29 GMT
< etag: "6395c261-c"
< accept-ranges: bytes
< x-envoy-upstream-service-time: 2
<
container 2
#+end_src

起動しているコンテナを見ると以下のような感じです。
Cloud Mapを使ったときと比べると =ecs-service-connect-agent= のコンテナが
Service Connectを設定しているコンテナ分起動していることがわかります。

追加でリソースは消費するとはいえ、微々たるものなので、
そこまで気にしなくてもよいと思います。


#+begin_src plain
# docker ps
CONTAINER ID   IMAGE                                    COMMAND                  CREATED         STATUS                   PORTS     NAMES
9ca56498aaeb   grugrut/my-ubuntu                        "bash -c 'sleep 3600'"   8 minutes ago   Up 8 minutes                       ecs-ubuntu-6-ubuntu-eadc82a8ba8adc4d0000
beaba3ee4bab   ecs-service-connect-agent:interface-v1   "/usr/bin/agent"         9 minutes ago   Up 9 minutes (healthy)             ecs-ubuntu-6-ecs-service-connect-l1ujXbD-b08e87eb81d6eaaf0f00
20e2ae8e64ae   amazon/amazon-ecs-pause:0.1.0            "/pause"                 9 minutes ago   Up 9 minutes                       ecs-ubuntu-6-internalecspause-e48fb1f5f09fc7c4f701
f94289122ab4   grugrut/my-nginx                         "nginx -g 'daemon of…"   4 hours ago     Up 4 hours                         ecs-nginx-2-nginx-bea4d392f4b0a69c4a00
25a2b44a0452   grugrut/my-nginx                         "nginx -g 'daemon of…"   4 hours ago     Up 4 hours                         ecs-nginx-2-nginx-9aa990f493bbe7c01800
e3c69a975123   ecs-service-connect-agent:interface-v1   "/usr/bin/agent"         4 hours ago     Up 4 hours (healthy)               ecs-nginx-2-ecs-service-connect-PZNlT0-96a7c5d8e7a0d6a95700
e723950f738f   ecs-service-connect-agent:interface-v1   "/usr/bin/agent"         4 hours ago     Up 4 hours (healthy)               ecs-nginx-2-ecs-service-connect-PZNlT0-dcd5bccb9884a9e18901
43a6a1c79ff9   amazon/amazon-ecs-pause:0.1.0            "/pause"                 4 hours ago     Up 4 hours                         ecs-nginx-2-internalecspause-eeabc7d4a984af932700
096800f48572   amazon/amazon-ecs-pause:0.1.0            "/pause"                 4 hours ago     Up 4 hours                         ecs-nginx-2-internalecspause-ecddc5d882f6c4aa8d01
84c7988e86fa   ecs-service-connect-agent:interface-v1   "/usr/bin/agent"         5 hours ago     Up 5 hours (healthy)               ecs---instance-service-connect-relay-929096c3f1de99a36300
561af0bc5a06   amazon/amazon-ecs-agent:latest           "/agent"                 6 hours ago     Up 6 hours (healthy)               ecs-agent
#+end_src

とはいえ、docker topで見てみると、Service Connect Agentのほうがメモリ消費しているようにも見えますね。
どちらかというと凪のときにみたので、nginxが使ってないという見方もできそうですが。

#+begin_src plain
# docker top f9 aux
USER                PID                 %CPU                %MEM                VSZ                 RSS                 TTY                 STAT                START               TIME                COMMAND
root                695                 0.0                 0.3                 55196               12216               ?                   Ss                  08:04               0:00                nginx: master process nginx -g daemon off;
33                  740                 0.0                 0.1                 55828               5588                ?                   S                   08:04               0:00                nginx: worker process
33                  741                 0.0                 0.1                 55828               5588                ?                   S                   08:04               0:00                nginx: worker process
# docker top be aux
USER                PID                 %CPU                %MEM                VSZ                 RSS                 TTY                 STAT                START               TIME                COMMAND
20000               1812                0.0                 0.6                 732180              23584               ?                   Ssl                 11:37               0:01                /usr/bin/agent
20000               1851                0.1                 1.8                 46363700            71852               ?                   Sl                  11:37               0:02                /usr/bin/envoy -c /tmp/envoy-config-2006183657.yaml -l info --concurrency 2 --drain-time-s 20
#+end_src

また、docker topしてみると、ECS Service Connect AgentはEnvoyが動いていることが確認できますね。
これは、冒頭のre:Inventの動画でも言っていました。
Cloud Mapの設定を見てもAPIコールのみのサービスが作成されていたので、
これをもとにEnvoyがうまく仲介しているのだと思います。

[[file:images/20221211-ecs-serviceconnect-cloudmap.png]]

そして、Service Connectを利用するもうひとつの利点が
通信のメトリクスを取得できることです。

[[file:images/20221211-ecs-serviceconnect-cloudwatch.png]]

これは受信側しか見てないですが、もちろん送信側もレスポンス内容がどうだったか
メトリクスを取得することが可能です。

*** まとめ

最近発表されたECS Service Connectを動かしてみて、簡単にコンテナ間通信が実現できることを確認しました。
ヘルスチェック周りなど、細かいところの挙動までみれてないので、
もう少しその辺を掘り下げてみていきたいと思います。

** DONE FISからChaosMeshを呼び出しEKSのカオスエンジニアリングをおこなう :aws:eks:fis:chaosmesh:
CLOSED: [2022-12-20 Tue 00:00]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(2022 2022-12)
:EXPORT_FILE_NAME: 202212190851
:END:

この記事は、[[https://qiita.com/advent-calendar/2022/aws][AWSアドベントカレンダー2022]]の20日目の記事です。

AWS Fault Injection Simulator(FIS)は、
障害を挿入する実験をおこない、アプリケーションの回復性や信頼性を
確認、改善するためのマネージドサービスです。
いわゆるカオスエンジニアリングの実験をおこなうことができます。

2021年の3月にGAになったFISは、少しずつ対応する障害注入アクションの種類を増やしてきました。
私も今年の6月ぐらいにちょっといじっていたのですが、当時はKubernetesに対する障害にはほとんど対応していませんでした。
その直後の、7月に待望のEKSに対する障害注入アクションが追加されたのですが、
その実現方式は、k8sのカオスエンジニアリングツールとして有名な =ChaosMesh= と =Litmus= を呼び出すことができるというものでした。
正直、当時は「そう来たか〜」と思ったものですが、実際に動かしてみる機会がなかったので今回実際に触ってみようと思います。

*** 環境のセットアップ

今回は、EKSに対してFISとChaosMeshを連携させて障害の注入を試してみます。
カオスエンジニアリングを試してみるには、アプリケーションが必要です。
マイクロサービスのデモアプリケーションとして有名なものの一つである[[https://microservices-demo.github.io/][Sock Shop]]を使います。
Sock Shopは主にWeaveworksがメンテナンスしているマイクロサービスのデモアプリで、
アプリケーションだけでなくモニタリングや負荷がけの設定も用意されているので簡単に試すことが可能です。

**** EKSのセットアップ

EKSは普通にセットアップするので詳細は割愛します。
ただし普段NetworkPolicyが欲しいケースがあるので、デフォルトのCNIである =aws-cni= ではなくて =Calico= を使いました。
あまりEKS + Calicoは相性がそこまで良いとも言えず、k8sのwebhook用のPodに対して =.spec.hostNetwork: true= の設定を入れる必要があったり、
なにかと面倒なので無理にやらなくてもよいと思います。

ただし、aws-vpcをデフォルトのまま利用すると、1ノードあたりで起動できるPodの数が非常に少なく、
今回のようにいろいろなものをデプロイしたい場合に非常に多くのノードが必要になってしまうので、以下の設定を入れることをお勧めします。

https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/cni-increase-ip-addresses.html

EKSにCalicoを入れるには、Calicoの公式ドキュメントを参照してください。
AWS側のドキュメントの手順でやっても、(少なくとも日本語版は)うまく設定できなかったのでCalico側を見ることをお勧めします。

https://projectcalico.docs.tigera.io/getting-started/kubernetes/managed-public-cloud/eks

**** Sock Shopのデプロイ

次に、Sock Shopをデプロイします。

なお、Sock ShopのアプリケーションのDeployment定義において、
mongo dbが新しくなりすぎていて、アプリケーション側(DBクライアント側)と互換性が無くなり、エラーが出てしまいます。
下記issueの通り、デプロイ前に手動で修正して、バージョンを指定してあげましょう。
具体的には、イメージ指定の箇所で、 =image: mongo= とlatest指定になっているのを =image: mongo:5.0.11= とバージョン指定します。

https://github.com/microservices-demo/microservices-demo/issues/900

#+begin_src bash
  $ git clone https://github.com/microservices-demo/microservices-demo
  $ cd microservices-demo/deploy/kubernetes

  # 上記mongoのバージョン設定をおこなう
  $ vi manifests/03-carts-db-dep.yaml
  $ vi manifests/13-orders-db-dep.yaml

  # アプリケーションのデプロイ
  $ kubectl apply -f manifests
#+end_src

デプロイが完了したら、NodePortでサービスが公開されているので、アクセスできることを確認しましょう。
front-endがTCP30001ポートで公開されているので、EC2インスタンスの30001ポートにアクセスすると、
Sock Shopのアプリケーションが動作しているのを確認できると思います。

#+begin_src bash
  $ kubectl get svc -n sock-shop
  NAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
  carts          ClusterIP   172.20.216.10    <none>        80/TCP              37h
  carts-db       ClusterIP   172.20.124.238   <none>        27017/TCP           37h
  catalogue      ClusterIP   172.20.97.161    <none>        80/TCP              37h
  catalogue-db   ClusterIP   172.20.160.217   <none>        3306/TCP            37h
  front-end      NodePort    172.20.143.5     <none>        80:30001/TCP        37h
  orders         ClusterIP   172.20.240.155   <none>        80/TCP              37h
  orders-db      ClusterIP   172.20.35.101    <none>        27017/TCP           37h
  payment        ClusterIP   172.20.117.186   <none>        80/TCP              37h
  queue-master   ClusterIP   172.20.29.104    <none>        80/TCP              37h
  rabbitmq       ClusterIP   172.20.174.80    <none>        5672/TCP,9090/TCP   37h
  session-db     ClusterIP   172.20.6.130     <none>        6379/TCP            37h
  shipping       ClusterIP   172.20.199.13    <none>        80/TCP              37h
  user           ClusterIP   172.20.195.229   <none>        80/TCP              37h
  user-db        ClusterIP   172.20.38.144    <none>        27017/TCP           37h
#+end_src

***** 監視と負荷ツールのデプロイ

動いたことを確認したら、続けて監視用の設定と負荷掛け用の設定を投入します。
監視用の設定は、Grafanaのダッシュボード設定用のyamlファイルがGrafanaがデプロイされていることを前提として作られているので、
まとめてデプロイすることができません。同梱されているREADME.mdを見ながらひとつずつデプロイしていきます。

#+begin_src bash
  $ pwd
  path/to/microservices-demo/deploy/kubernetes/
  $ cd manifests-monitoring/

  # 監視設定のデプロイ
  $ kubectl create -f 00-monitoring-ns.yaml
  $ kubectl apply $(ls *-prometheus-*.yaml | awk ' { print " -f " $1 } ')
  $ kubectl apply $(ls *-grafana-*.yaml | awk ' { print " -f " $1 }'  | grep -v grafana-import)
  $ kubectl apply -f 23-grafana-import-dash-batch.yaml

  # 負荷掛け設定のデプロイ
  $ cd ..
  $ kubectl apply -f manifests-loadtest/loadtest-dep.yaml
#+end_src

負荷掛けのアプリケーションは、 =Locust= を使った負荷設定がされており、デプロイするだけで継続的に負荷を掛けることができます。
ログを見ればその内容も確認することができます。

#+begin_src bash
  $ kubectl -n loadtest get pod
  (前略)
  NAME                        READY   STATUS    RESTARTS   AGE
  load-test-7b8dbc789-sd2jt   1/1     Running   0          4m
  load-test-7b8dbc789-tncrp   1/1     Running   0          3m59s
  $ kubectl -n loadtest logs load-test-7b8dbc789-sd2jt
  [2022-12-18 01:45:29,751] load-test-7b8dbc789-ndm4d/INFO/locust.main: Starting Locust 0.7.5
  [2022-12-18 01:45:29,752] load-test-7b8dbc789-ndm4d/INFO/locust.runners: Hatching and swarming 5 clients at the rate 5 clients/s...
   Name                                                          # reqs      # fails     Avg     Min     Max  |  Median   req/s
  --------------------------------------------------------------------------------------------------------------------------------------------
  --------------------------------------------------------------------------------------------------------------------------------------------
   Total                                                              0     0(0.00%)                                       0.00

  [2022-12-18 01:45:30,758] load-test-7b8dbc789-ndm4d/INFO/locust.runners: All locusts hatched: Web: 5
  [2022-12-18 01:45:30,758] load-test-7b8dbc789-ndm4d/INFO/locust.runners: Resetting stats

   Name                                                          # reqs      # fails     Avg     Min     Max  |  Median   req/s
  --------------------------------------------------------------------------------------------------------------------------------------------
   GET /                                                             10     0(0.00%)      39       7      91  |      33    0.00
   GET /basket.html                                                  11     0(0.00%)      12       4      53  |       8    0.00
   DELETE /cart
  (後略)
#+end_src

また、監視についてもGrafanaがやはりNodePortで公開されているので、
ポートを確認してアクセスします。

#+begin_src bash
  $ kubectl -n monitoring get svc
  NAME                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
  grafana              NodePort    172.20.254.180   <none>        80:31300/TCP        36h
  kube-state-metrics   ClusterIP   None             <none>        8080/TCP,8081/TCP   36h
  node-exporter        ClusterIP   None             <none>        9100/TCP            36h
  prometheus           NodePort    172.20.34.79     <none>        9090:31090/TCP      36h
#+end_src

TCP31300にアクセスすると、Grafanaにアクセスすることができます。
ユーザ認証がありますが、デフォルトのユーザ/パスワードは =admin/admin= でログインすることができます。
GrafanaでPodのリソース使用状況や、各サービスの処理能力、レイテンシなどを確認することが可能です。

[[file:images/20221218-grafana-before.png]]

なかなか長くなりましたがアプリケーションの環境セットアップは以上となります。

*** ChaosMeshのセットアップ
つづけて、ChaosMeshをセットアップしていきます。

**** ChaosMeshのインストール
まずはEKSにChaosMeshをインストールします。
インストール方法として、シェルスクリプトを実行するだけのクイックスタートとHelmの方法の2つが用意されていますが、
クイックスタートはコンテナランタイムがDockerであることを前提としています。
現代のEKSはコンテナランタイムが =containerd= になっており、いざ実験開始してみたら動かず再インストールするはめにあうので、
はじめからhelmを使ってインストールしましょう(一敗)。

https://chaos-mesh.org/docs/production-installation-using-helm/

基本的に手順通りやっていけばインストールは完了します。

#+begin_src bash
  $ helm repo add chaos-mesh https://charts.chaos-mesh.org
  $ helm search repo chaos-mesh
  $ kubectl create ns chaos-mesh
  $ helm install chaos-mesh chaos-mesh/chaos-mesh -n=chaos-mesh --set chaosDaemon.runtime=containerd --set chaosDaemon.socketPath=/run/containerd/containerd.sock --set controllerManager.hostNetwork=true --version 2.5.0
#+end_src

なお、最後のhelmコマンドの引数に含まれる =--set controllerManager.hostNetwork=true= ですが、
これはCNIをCalicoにしているために追加しています。
aws-cniを利用している場合は特にこの引数は不要です。

ChaosMeshにもダッシュボードがありますが、今回は利用しないので省略します。

**** ChaosMeshを動かしてみる
ChaosMeshでは障害ごとに =CRD(Custom Resource Definision)= が定義されており、それらのカスタムリソースを作成することで障害を注入することができます。

たとえば、以下のようなyaml定義をpod-kill.yamlという名前で作成し、
これをデプロイするとSelectorにマッチするPodを終了させることができます。

#+begin_src yaml
  apiVersion: chaos-mesh.org/v1alpha1  
  kind: PodChaos  
  metadata:  
    name: pod-kill
  spec:  
    action: pod-kill  
    mode: one  
    selector:  
      namespaces:  
      - sock-shop  
      labelSelectors:  
        name: 'carts'
#+end_src

これをデプロイすると、PodChaosのカスタムリソースが作成されると同時に、
=sock-shop= 名前空間の、 =name=carts= のラベルを持つPodが終了して再起動したことが確認できます。
確認が終わったらPodChaosのカスタムリソースは削除しておきましょう。

#+begin_src bash
  $ kubectl get PodChaos
  No resources found in default namespace.
  $ kubectl apply -f pod-kill.yaml
  podchaos.chaos-mesh.org/pod-kill created
  $ kubectl get pod -n sock-shop
  NAME                            READY   STATUS    RESTARTS  AGE   LABELS
  carts-7bbf9dc945-h6p9d          1/1     Running   0         5s    name=carts
  carts-db-5dfd9c6594-6gc7h       1/1     Running   0         48m   name=carts-db
  catalogue-6479dbb5bd-qrd6b      1/1     Running   0         33h   name=catalogue
  catalogue-db-6b55d8cdb7-mbbxf   1/1     Running   0         48m   name=catalogue-db
  front-end-7f5c844b4c-kqzvx      1/1     Running   0         48m   name=front-end
  orders-74f65597c5-hcq4p         1/1     Running   0         33h   name=orders
  orders-db-74698dffd-t8qt7       1/1     Running   0         48m   name=orders-db
  payment-c7df5b49-5qnxh          1/1     Running   0         33h   name=payment
  queue-master-9fc44d68d-wdxl5    1/1     Running   0         33h   name=queue-master
  rabbitmq-6576689cc9-q2cjh       2/2     Running   0         48m   name=rabbitmq
  session-db-695f7fd48f-lqgxb     1/1     Running   0         48m   name=session-db
  shipping-79c568cddc-9kpzs       1/1     Running   0         48m   name=shipping
  user-79dddf5cc9-8vhg9           1/1     Running   0         33h   name=user
  user-db-b8dfb847c-2xgsv         1/1     Running   0         48m   name=user-db
  $ kubectl get PodChaos
  NAME       AGE
  pod-kill   11s
  $ kubectl delete PodChaos pod-kill
  podchaos.chaos-mesh.org "pod-kill" deleted
  $ 
#+end_src

**** IAMロール、ClusterRoleの設定
次に、FISの実験実行用のIAMロールを作成し、そのIAMロールでChaosMeshの障害を注入できるようにします。
この後出てきますが、FISでChaosMeshと連携する際は、その実験用のロールがkubernetesのAPIを呼ぶだけです。
そのため、kubernetesのAPIを呼び出せるようにIAMロールとClusterRoleの連携の設定をおこないます。

***** IAMロールの作成
まずは、FISの連携用のロールを作ります。マネジメントコンソールでうまく作れなかったので、
ドキュメントに従い、aws cliを使って作成します。

https://docs.aws.amazon.com/ja_jp/fis/latest/userguide/getting-started-iam-service-role.html

#+begin_src bash
  $ echo "{
    "Version": "2012-10-17",
    "Statement": [
       {
         "Effect": "Allow",
           "Principal": {
              "Service": [
                "fis.amazonaws.com"
              ]
           },
           "Action": "sts:AssumeRole"
        }
     ]
  }" > fis-role-trust-policy.json
  $ aws iam create-role --role-name my-fis-role --assume-role-policy-document file://fis-role-trust-policy.json
#+end_src

また、FISの実験ロールにはCloudWatch LogsのCreateLogDeliveryのアクションが必要なので、それを許可します。
私はVisual Editorを使って以下のようなインラインポリシーを作成してアタッチしています。

#+begin_src json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": "logs:CreateLogDelivery",
            "Resource": "*"
        }
    ]
}  
#+end_src

また、S3にログも保管したかったので、 =AmazonS3FullAccess= のポリシーもアタッチしています。

注意事項: 今回は簡略化のために権限は緩めに設定しています。
実際に利用する際は必要でない権限はつけないようにできるだけ絞りましょう。

***** ClusterRoleを作成する
次に、実験の際にカスタムリソースを作成する用のkubernetes側のRoleを作成します。

今回は、ドキュメントにあるChaosMesh管理者用の設定を元に定義を作りました。
こちらもIAMロールの権限同様に、実際に利用する際は強すぎる権限を与え過ぎないようによく検討しましょう。

以下のようにClusterRole、ClusterRoleBindingのyaml定義を作成し、デプロイすると
ChaosMeshの各種カスタムリソースを作成できるClusterRoleが作成され、
=chaosmesh:manager= グループに対してそれが割り当てられます。

#+begin_src yaml
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: chaosmesh-manager
  rules:  
  - apiGroups: [""]  
    resources: ["pods", "namespaces"]  
    verbs: ["get", "watch", "list"]  
  - apiGroups:  
    - chaos-mesh.org  
    resources: [ "*" ]  
    verbs: ["get", "list", "watch", "create", "delete", "patch", "update"]
  ---
  apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    creationTimestamp: null
    name: chaosmesh-manager-binding
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: chaosmesh-manager
  subjects:
  - kind: Group
    name: chaosmesh:manager
#+end_src

***** IAMロールとClusterRoleを連携する
最後に、IAMロールとClusterRoleを連携し、
先ほど作成した IAMロール =my-fis-role= がkubernetesのAPIを呼び出してChaosMeshのカスタムリソースを操作できるようにします。

IAMロールがkubernetesの権限を得るためには、名前空間 =kube-system= にある =ConfigMap= のひとつ =aws-auth= を設定します。
=kubectl -n kube-system edit configmap aws-auth= を実行するとエディタが開かれるので、
例えば以下のように設定します。

#+begin_src yaml
apiVersion: v1
data:
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::123456789012:role/eks-sandbox-worker-node-role
      username: system:node:{{EC2PrivateDNSName}}
    - groups:
      - chaosmesh:manager
      rolearn: arn:aws:iam::123456789012:role/my-fis-role
      username: chaosmesh-manager
kind: ConfigMap
metadata:
  creationTimestamp: "2022-12-17T23:15:29Z"
  name: aws-auth
  namespace: kube-system
  resourceVersion: "xxxxx"
  uid: a0b0c0e0-abcd-1234-5678-1234567890ab
#+end_src

追加するのは以下の部分です。

#+begin_src yaml
    - groups:
      - chaosmesh:manager
      rolearn: arn:aws:iam::123456789012:role/my-fis-role
      username: chaosmesh-manager
#+end_src

groupのところにClusterRoleBindingsで入れたグループ名を、rolearnに作成したIAMロールのarnを入れます。
usernameは適当にいれてよいはずです。

これを保存すれば、FISからChaosMeshを呼び出す準備は完了です。

*** FISのセットアップ

最後にFISの実験テンプレートを作成します。

実験テンプレートの作成では実験の説明(名前)、1つ以上のアクション、アクションに対応したターゲット、
実験を実行するIAMロール、停止条件、ログを設定します。

今回はネットワークの遅延をシミュレートする =NetworkChaos= の注入だけをおこなう実験を作成します。

アクションの設定では以下のように設定をおこないます。

[[file:images/20221218-fis-network-delay.png]]

今回ChaosMeshとの連携をやってみて、一番信じがたかったのがこのアクションパラメータの部分でした。

なんと、カスタムリソースの設定を分解しながら入力ボックスに入れていくのです(しかもspec部分はjson形式)。
たしかに他のアクションもSSMドキュメントのパラメータを入力支援無しでjsonで入れるので
予想はつきましたが、ここはもうちょっと親切になって欲しいところです。

せめてIAMポリシーのエディタのようにjsonの入力支援がつくとか、それぐらいは実現してほしい。
1行のインプットボックスに対してjsonを設定するのは、なかなか大変でした。

さて、脱線しましたが、アクションパラメータにはそれぞれ以下のものを入力します。
- Kubernetes API Version
  - カスタムリソースのAPIバージョンを入れます。
  - ChaosMeshであれば今は、 =chaos-mesh.org/v1alpha1= です。
- Kubernetes Kind
  - 作成するカスタムリソースを入れます。今回は =NetworkChaos= を作成します。
- Kubernetes Namespace
  - カスタムリソースを作成する名前空間を指定します。今回は =default= にしました。
    先の例でもあったように、これはアプリケーションの名前空間と一致している必要はありません。
- Kubernetes Spec
  - 先述の通り、カスタムリソースのSpec部分をJSON形式で入れます。

今回は以下のようなspecで定義しました。
これは、 =sock-shop= 名前空間の =name:carts= のラベルのPodに対して
180秒間の間、90msのネットワーク遅延をおこします。

これにより特定のサービスで処理遅延が発生した場合に、
どのサービスに影響があるかを実験することができます。

#+begin_src json
{
    "action": "delay",
    "delay": {
        "latency": "90ms"
    },
    "duration": "180s",
    "mode": "one",
    "selector": {
       "labelSelectors": {
            "name": "carts"
        },
       "namespaces": [
            "sock-shop"
        ]
    }
}
#+end_src

ちなみに、普段yamlで定義を書いている人にとっては、いきなりjsonで書けといわれてもちょっと面倒かと思います。
その場合は、yamlを用意しておけば、 =kubectl= の =dry-run= 機能を使うことでyamlをjsonに変換することが可能です。

#+begin_src bash
$ cat <<EOF | kubectl create --dry-run=client -o json -f -
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: network-delay
spec:
  action: delay
  mode: one
  selector:
    namespaces:
      - sock-shop
    labelSelectors:
      name: carts
  delay:
    latency: 90ms
  duration: 10s
EOF

{
    "apiVersion": "chaos-mesh.org/v1alpha1",
    "kind": "NetworkChaos",
    "metadata": {
        "name": "network-delay",
        "namespace": "default"
    },
    "spec": {
        "action": "delay",
        "delay": {
            "latency": "90ms"
        },
        "duration": "10s",
        "mode": "one",
        "selector": {
            "labelSelectors": {
                "name": "carts"
            },
            "namespaces": [
                "sock-shop"
            ]
        }
    }
}
#+end_src

残りの実験テンプレートの設定項目については、
ここまで準備していればプルダウンでEKSクラスタを設定したり、
IAMロールの設定で =my-fis-role= の設定ができると思います。

また、FISの最大の特徴としてCloudWatchと連携して、
実験により想定外の事象が発生した場合には実験を停止させることが可能ですが、
今回は本設定は省略します。

*** 実験をおこなう

ここまで非常に長くセットアップをおこなってきましたが、
ついに実験をおこなうことができます。

実験をおこなう際には、まずは今回の実験に対してどのように振る舞うか仮説を立てましょう。
そして、実験を通して実際の動きと仮説のギャップをみて、
システムの改善ポイントを発見していきます。

今回はデモアプリケーションで、あまり中身の動きはわかっていないのですが、
例えば「cartsアプリケーションにネットワーク遅延が発生することで、
当該サービスは当然レイテンシが大きくなってしまうが、他のサービスには影響は出ないはず」
という仮説を立てたとします。

それではFISの画面から実験を開始してみましょう。

実験を開始すると、うまく動いていれば先ほど定義したとおり、 =default= 名前空間に対して
=NetworkChaos= のカスタムリソースが作成されていることが確認できます。
詳細を見てみても、定義したとおりの障害が注入されていることが確認できますね。

#+begin_src bash
  $ kubectl get networkchaos
  NAME                                                           ACTION   DURATION
  fis-6ph6ccpl6or3ce1l65i36ohj68sjcdr16co3adb474oj6ohl6ssiqthi   delay    180s
  $ kubectl describe networkchaos
  Name:         fis-6ph6ccpl6or3ce1l65i36ohj68sjcdr16co3adb474oj6ohl6ssiqthi
  Namespace:    default
  Labels:       <none>
  Annotations:  <none>
  API Version:  chaos-mesh.org/v1alpha1
  Kind:         NetworkChaos
  Metadata:
  (中略)
  Spec:
    Action:  delay
    Delay:
      Correlation:  0
      Jitter:       0ms
      Latency:      90ms
    Direction:      to
    Duration:       180s
    Mode:           one
    Selector:
      Label Selectors:
        Name:  carts
      Namespaces:
        sock-shop
  Status:
    Conditions:
      Status:  True
      Type:    Selected
      Status:  True
      Type:    AllInjected
      Status:  False
      Type:    AllRecovered
      Status:  False
      Type:    Paused
    Experiment:
      Container Records:
        Events:
          Operation:      Apply
          Timestamp:      2022-12-18T04:13:50Z
          Type:           Succeeded
        Id:               sock-shop/carts-7bbf9dc945-lb4vp
        Injected Count:   1
        Phase:            Injected
        Recovered Count:  0
        Selector Key:     .
      Desired Phase:      Run
    Instances:
      sock-shop/carts-7bbf9dc945-lb4vp:  1
  Events:
    Type    Reason           Age   From            Message
    ----    ------           ----  ----            -------
    Normal  FinalizerInited  27s   initFinalizers  Finalizer has been inited
    Normal  Updated          27s   initFinalizers  Successfully update finalizer of resource
    Normal  Started          27s   desiredphase    Experiment has started
    Normal  Updated          27s   desiredphase    Successfully update desiredPhase of resource
    Normal  Updated          27s   records         Successfully update records of resource
    Normal  Applied          27s   records         Successfully apply chaos for sock-shop/carts-7bbf9dc945-lb4vp
    Normal  Updated          27s   records         Successfully update records of resource
#+end_src

そして、実験が終わると自動的にカスタムリソースが削除されて実験前の状態に戻ったことがイベントからも確認できます。

#+begin_src bash
  $ kubectl get event
  (前略)
  4m31s       Normal    Started           networkchaos/fis-6ph6ccpl6or3ce1l65i36ohj68sjcdr16co3adb474oj6ohl6ssiqthi   Experiment has started
  4m31s       Normal    Updated           networkchaos/fis-6ph6ccpl6or3ce1l65i36ohj68sjcdr16co3adb474oj6ohl6ssiqthi   Successfully update desiredPhase of resource
  4m31s       Normal    Updated           networkchaos/fis-6ph6ccpl6or3ce1l65i36ohj68sjcdr16co3adb474oj6ohl6ssiqthi   Successfully update records of resource
  4m31s       Normal    Applied           networkchaos/fis-6ph6ccpl6or3ce1l65i36ohj68sjcdr16co3adb474oj6ohl6ssiqthi   Successfully apply chaos for sock-shop/carts-7bbf9dc945-lb4vp
  4m31s       Normal    Updated           networkchaos/fis-6ph6ccpl6or3ce1l65i36ohj68sjcdr16co3adb474oj6ohl6ssiqthi   Successfully update records of resource
  91s         Normal    TimeUp            networkchaos/fis-6ph6ccpl6or3ce1l65i36ohj68sjcdr16co3adb474oj6ohl6ssiqthi   Time up according to the duration
  91s         Normal    Updated           networkchaos/fis-6ph6ccpl6or3ce1l65i36ohj68sjcdr16co3adb474oj6ohl6ssiqthi   Successfully update desiredPhase of resource
  91s         Normal    Updated           networkchaos/fis-6ph6ccpl6or3ce1l65i36ohj68sjcdr16co3adb474oj6ohl6ssiqthi   Successfully update records of resource
  91s         Normal    Recovered         networkchaos/fis-6ph6ccpl6or3ce1l65i36ohj68sjcdr16co3adb474oj6ohl6ssiqthi   Successfully recover chaos for sock-shop/carts-7bbf9dc945-lb4vp
  91s         Normal    Updated           networkchaos/fis-6ph6ccpl6or3ce1l65i36ohj68sjcdr16co3adb474oj6ohl6ssiqthi   Successfully update records of resource
  72s         Normal    FinalizerInited   networkchaos/fis-6ph6ccpl6or3ce1l65i36ohj68sjcdr16co3adb474oj6ohl6ssiqthi   Finalizer has been removed
  72s         Normal    Updated           networkchaos/fis-6ph6ccpl6or3ce1l65i36ohj68sjcdr16co3adb474oj6ohl6ssiqthi   Successfully update finalizer of resource
  #+end_src

さて、実験の結果はどうだったでしょうか。
改めてGrafanaの画面を見てみます。
  
[[file:images/20221218-grafana-during.png]]

Cartのレイテンシ(2行目右側)のグラフから実験中はレイテンシが非常に大きくなっていることが確認できます。
さらに、Orders(3行目右側)についてもレイテンシが同様に大きくなったことが確認できました。
その一方で、CatalogueやPaymentについては、そこまで影響は受けていないように見えます。

このように仮説と実際の挙動が異なる箇所については、万が一障害が発生した場合に
予期せぬ事象につながったり、修復に時間がかかったりとサービス影響が出やすいところと言えます。

カオスエンジニアリングを実施することで、この仮説と異なる部分を見つけだし改善したり、
逆に仮説通りであることを確認することで、システムの信頼性を高めることができます。

*** まとめ
今回は、FISとChaosMeshの連携を実際に試してみました。
正直な感想としては、セットアップに時間がかかり、FISでの設定もあまり親切でないところから
わざわざFISから呼び出さなくてもChaosMesh単体で使えばよいかな、と思いましたが、
FISにはCloudWatchと連携して実験が予期しない動作をした場合に止めるなど優れているところも多くあります。

今は、まずはFISで実施できるアクションを増やしてみた、という段階だと思いますので、
これからよりマネージドに便利に扱えるようになることに期待したいと思います。


* 2023
** DONE キーボードをMoonLanderに変更した                         :moonlander:keyboard:
CLOSED: [2023-08-04 Fri 11:42]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2023\" \"2023-08\")
:EXPORT_FILE_NAME: 202308041041
:END:

[[*ergodox ezを購入した][ergodox ezを購入した]]

[[*Ergodox EZをJIS配列で利用して5ヶ月経過したので振り返り][Ergodox EZをJIS配列で利用して5ヶ月経過したので振り返り]]

の通り、これまで2020年に購入したErgodox EZを愛用していたのですが、
出社もぼちぼちでてくる中で、家でも会社でも持ち歩きながら使っていたら、ちょうど3年で故障してしまいました。

特定の一画のキーが動作しなくなってしまう感じ。分解してみところ回路がかなり汚れていて、掃除したら一瞬復活したのですがすぐにまた元に戻ってしまったので、いったんあきらめることにしました。

ということで代替機が必要になり、同じものを買ってもよかったのですが、どうせならということで前から気になっていた [[https://www.zsa.io/moonlander/][MoonLander]] を購入してみました。
製造元はこれまで使っていたErgodox EZと同じZSAであり、見た感じErgodox EZにキー配列も良く似ていて移行しやすそうだったので。

というわけで購入したのがこちら。

[[file:images/20230804_moonlander_0.jpg]]

Ergodox EZのときはWhiteを使っていましたが、結構汚れが目立つときになったので今回はBlackにしてみました。
黒は黒で皮脂などが気になるので小まめに拭くようにはしています。

半導体需給とかいろいろ気になったのですが、意外にもすぐに発送され、注文から1週間ちょいで受けとることができました。

*** キー配列や焼き込み

Ergodox EZと同じく、qmk firmwareが使えるので、基本的な配列はそのまま移行しました。
今のキー配列はこんな感じにしています。

https://github.com/grugrut/qmk_firmware/blob/a333bb91b44f32fb563a950e7cf716f3663bf4d9/keyboards/moonlander/keymaps/grugrut/keymap.c

Ergodox EZのときは、親指部分が充実していたのでそこにPage Up/Page Down/Home/Endを配置していましたが、その部分のキーが使えなくなったので、レイヤ切替後のホームポジションにいくようにしています。
もともと遠い親指のキーは使えてなかったので、そんなに影響はないでしょう。

ビルドしたファームウェアを書き込むには、Ergodox EZのときは、[[https://www.pjrc.com/teensy/loader.html][teensy]]用のアプリを使っていたのですが、
ベースのものが変わったからかMoonlander向けには使えなくなっていたので、[[https://www.zsa.io/wally/][Wally]]を使いました。

Moonlanderは元からQK_BOOTが配置されているので、リセットボタンを押さなくても書き込めるのが地味に嬉しいところでした。

*** 良かったところ

ここからは、Ergodox EZからMoonlanderに乗り換えてみて良かったところを書いていきます。

**** 乗り換えやすい

上にも書きましたが、キーの配置がほぼ同じなので、移行コストはほぼ0でした。

**** キー入力の音が静か

Ergodox EZはスイッチのカチカチって音というより底打ち音がカチャカチャ結構響く感じで、
会社で使うときには勢いにのってくると音が気になるって感じったのですが、
Moonlanderのほうが底打ち音があまりしない気がします。

**** バックライトLEDがテンション上がる

購入当初は、こんなゲーミング要素いらないんだけどって思い、
届くまでの間に、QMKでバックライトを制御する方法を調べてたぐらいでした。
が、いざ使ってみると意外や意外。七色に光るの結構テンション上がるのでそのまま使っています。

*** 良くなかったところ

**** Ergodoxとの細かい差異(すぐ慣れる)

乗り換え特有の話ですが、概ね同じなだけに細かい差が最初は戸惑いました。

***** 一番端のキーのサイズが1U
左手側の一番左の列、右手側の一番右の列が、Ergodox EZでは1.25Uあるのですが、Moonlanderは全部同じ1Uです。
この僅かな幅が最初慣れず、スカッてしまったり一個隣の列を叩いてしまったりしました。

一週間も使ってれば慣れます。

***** 親指キーの並び
親指向けにキーの出島(上の写真で赤いキーがあるところ)があるのですが、
若干Ergodox EZと角度が違っていて最初戸惑いました。

一週間も使ってれば慣れます。

***** キープロファイル

Ergodox EZは無刻印の場合はDCSプロファイルなのですが、Moonlanderの場合は無刻印でもDSAプロファイルです。
(Erogdox EZも印字ありの場合はDSAらしい)

キーごとに傾きが違うのが打ちやすかったのでちょっと苦戦してます。
一週間使っても慣れずミスタイプが多めになっています。

ただ、キーキャップを外して掃除すると無刻印 & DCSプロファイルだと、もとに戻すのがかなり大変だったので掃除はしやすくなりますね。

**** 端のキーをうっかり押してしまう

コンパクトになったのはとても良いのですが、キーボードの土台に余白が少ないので、キーボードの端を触れてしまったときに意図せず指がひっかかってキーを押下してしまうことがあります。
そんなにクリティカルなキーを配置してないので実害はないですが、おもわぬ暴発はちょっとびっくりします。

*** まとめ

Ergodoxが故障してしまったので、Moonlanderに乗り換えてみました。
気になるところは多少ありますが、慣れの問題ですし、これから使いこなしていきたいと思います。
** DONE AWSの新しい資格 Data Engineer Associateのベータ版を受験してきた :aws:certification:
CLOSED: [2023-12-03 Sun 09:21]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2023\" \"2023-12\")
:EXPORT_FILE_NAME: 202312030921
:END:

[[*AWSの資格を全制覇した][AWSの資格を全制覇した]] の通り、私はAWSの資格を全部取得済なのですが、
今回新しく AWS Certified Data Engineer - Associate (DEA)とデータ活用系の資格が出るということでベータ版を受講してきました。

ちなみに、去年は全資格制覇ということで、2023 Japan AWS All Certifications Engineersにも選んでもらい、
今年度も全部維持できてるから更新できるぞ、と思っていた矢先の新資格登場でしたが、2024のクライテリアには影響ないようです。よかった。

*** Data Engineer Associateは
https://aws.amazon.com/jp/certification/certified-data-engineer-associate/
の公式の説明によると、
#+begin_quote
AWS Certified Data Engineer - Associate は、コアデータ関連の AWS サービスのスキルと知識や、データパイプラインの実装、問題のモニタリングとトラブルシューティング、ベストプラクティスに従ってコストとパフォーマンスを最適化する能力を検証します。
#+end_quote
ということで、データエンジニア向けの資格ですね。

データエンジニア向けの資格というと、これまで Data Analytics Specialty (DAS)があり、
棲み分けどうなるのかなと思ってましたが、なんとこちらは2024/4/9に廃しされるということです。

FAQにも
#+begin_quote
AWS Certified Data Engineer - Associate は、AWS Certified Data Analytics - Specialty ほど深くも複雑でもなく、プログラミングの概念の適用を含み (AWS Certified Data Analytics - Specialty の範囲外)、AWS Certified Data Analytics - Specialty よりもデータ運用/サポートおよびセキュリティ要件の実装に重点を置いています。
#+end_quote
と書かれているように、DASのほうはいかにデータ分析をするかが問われていたと思いますが、
DAEのほうが、よりデータの準備やセキュリティの方が重視されるようです。

*** 試験について
当然、どんな問題が出たかなどは書けないですが、試験の感想です。

長かったし、疲れた。

この一言に尽きます。
試験時間が170分で85問、しかも言語は英語ということで気を休めることなく解き続ける必要があったので、
オフラインの試験会場で受験したのですが、試験後はかなりぐったりしていました。

Solution Architect Professionalが180分で75問、Specialtyは180分で65問ということで、かなり密度が濃いです。
Associateなので問題のレベルはそれなりとは言え、普段は雰囲気で英語を読んでたり機械翻訳も併用したりとたるんでいたので
自力で全部英文を読まないといけないことから集中力をかなり使いました。
あと、ベータ版だからか判断に迷う問題もでてきたり。

普段は問題文の和訳が微妙すぎてかえって難しかったので英文を読んだほうが楽だわー、と毒づいてたりしましたが、
いざ英語だけになると苦戦しますね。Google Cloud Database Engineerも英語だったけど、そっちはそこまで大変じゃなかったのにな。

普段は1時間ぐらいで完了して出てしまうのですが、今回は最後まで解ききった時点で残り時間30分とちょっとヒヤヒヤする時間配分になりました。
私は使わなかったですが、母語でない言語で試験を受ける場合は試験時間30分延長の特別措置を受けられるので、申し込むだけ申し込んでおいたほうが良いと思います。結果早めに終わったらそこで退出するだけなので。

結果は90日後ということなので、気長に待ちたいと思います。
おかげさまで、周りにも後を追ってAll Certifications Engineerを目指す人たちが増えてきてくれているので、その人たちにも宣伝していこうと思います。

*** 振り返り
今回英語のベータ版を受験しましたが、日本語ならそんなに難しくないと思うので、正式版が出たらそこまで苦労せず取れるのではないでしょうか。

あと、余談ですが直前にre:Invent振り返りを見たのは失敗でした。
試験ガイドのとおりデータの準備、取り回しみたいな問題もありましたが、何度「zero-ETLってやつでなんとかして」と思ったことか
(該当の問題がzero-ETLで対応してるかどうか知らないですが)。

#+begin_export html
<div class="card" style="width: 18rem; border: 2px solid #999999; padding: 8px;">
<a href="https://www.amazon.co.jp/%E8%A6%81%E7%82%B9%E6%95%B4%E7%90%86%E3%81%8B%E3%82%89%E6%94%BB%E7%95%A5%E3%81%99%E3%82%8B%E3%80%8EAWS%E8%AA%8D%E5%AE%9A-%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90-%E5%B0%82%E9%96%80%E7%9F%A5%E8%AD%98%E3%80%8F-Compass-Books%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-NRI%E3%83%8D%E3%83%83%E3%83%88%E3%82%B3%E3%83%A0%E6%A0%AA%E5%BC%8F%E4%BC%9A%E7%A4%BE/dp/4839978174?_encoding=UTF8&qid=1701565221&sr=8-1&linkCode=li2&tag=grugrut-22&linkId=55e75d6ada479cc9cbe7fd69a94ec539&language=ja_JP&ref_=as_li_ss_il" target="_blank"><img border="0" src="//ws-fe.amazon-adsystem.com/widgets/q?_encoding=UTF8&ASIN=4839978174&Format=_SL160_&ID=AsinImage&MarketPlace=JP&ServiceVersion=20070822&WS=1&tag=grugrut-22&language=ja_JP" ></a><img src="https://ir-jp.amazon-adsystem.com/e/ir?t=grugrut-22&language=ja_JP&l=li2&o=9&a=4839978174" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />
  <div class="card-body">
    <h5 class="card-title"><a href="https://amzn.to/46P44sw">要点整理から攻略する『AWS認定 データ分析-専門知識』 (Compass Booksシリーズ)</a></h5>
    <p class="card-text"><a href="https://amzn.to/46P44sw">2023/6/26<br>
    NRIネットコム株式会社 (著), 佐々木拓郎 (著), 喜早彬 (著), 小西秀和 (著), 望月拓矢 (著), 和田将利 (著)</a></p>
  </div>
</div>    
#+end_export
** DONE ゾーンシフトも出現したしクロスゾーン負荷分散を再考察する        :aws:route53:elb:
CLOSED: [2023-12-19 Tue 22:25]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2023\" \"2023-12\")
:EXPORT_FILE_NAME: 202312192224
:END:

先日まで開催されていたAWSの一大イベント Re:Invent の期間中に、
単一AZ障害時の自動対処をおこなうための機能がリリースされていました。

それが自動ゾーンシフトです。
https://docs.aws.amazon.com/ja_jp/r53recovery/latest/dg/arc-zonal-autoshift.html

ゾーンシフトは、Route 53 Application Recovery Controller(ARC)に付随する機能で、
これまで手動でのみ特定のAZを切り離すことができましたが、
今回のアップデートで自動で障害を検知して切り離してくれるようになりました。

ただ、ゾーンシフトの大前提として「ELBのクロスゾーン負荷分散を無効化しておくこと」というのがあり、
チームメンバともこれ無効化するの微妙だよねという話を手動ゾーンシフトが出たときもしていました。
そして今回の自動ゾーンシフトの発表ということで、あらためてクロスゾーン負荷分散の無効化の是非について考えてみようと思います。
*** クロスゾーン負荷分散って？

そもそもクロスゾーン負荷分散についておさらいしておきたいと思います。
クロスゾーン負荷分散はその名のとおり、AZをクロスして負荷分散するELBの設定です。
ELBは作成するときは1リソースでも、実際には設定した各AZにそれぞれインスタンスが配置されます。
クロスゾーン負荷分散が無効の場合は自身と同じAZのターゲットグループにだけ転送しますが、有効にするとゾーンをまたいだものにも転送されるというわけです。

[[file:images/20231219-alb-zone.png]]

クロスゾーン負荷分散の有効/無効で下記の表のように違いがありました。

| 考慮事項                     | クロスゾーン負荷分散 有効                      | クロスゾーン負荷分散 無効                    |
|------------------------------+------------------------------------------------+----------------------------------------------|
| スティッキーセッション       | サポートされる                                 | サポートされない                             |
| バックエンドの偏りの負荷影響 | 全インスタンス均一に負荷分散される             | インスタンス数が少ないゾーンでは負荷が上がる |
| AZのインスタンス全損時の挙動 | ELBのエンドポイントはそのまま。分散先が変わる  | ELBのエンドポイント自体が削除される          |
| 性能面の考慮事項             | AZまたぎの通信になるので多少レイテンシが上がる | -                                            |
| コスト面の考慮事項           | AZまたぎの通信になるので多少コストが上がる     | -                                            |

コスト・性能の問題はあれど、そのレベルを気にするならそもそもパブクラを選ぶか？ということもあり、
クロスゾーン負荷分散は基本的に有効にするものだと思ってきて育ってきたわけですから、
今回のゾーンシフト機能でクロスゾーン負荷分散をわざわざ無効化することという前提があったのはとても驚きでした。
*** どんなワークロードなら無効化してもよいのか

正直、今のところわざわざクロスゾーン負荷分散を無効化してまでゾーンシフトを利用したほうがよいケースが思いうかびません。

強いて言うなら、EKSがバックエンドであれば、もともとKubernetesはコンピュートノード間で仮想ネットワークを張って、
そちらでクロスゾーンで通信がおこなわれるのでELBのレイヤでもクロスゾーンに分散する必要はないのかもと思っていますが、どうなんでしょうか？

AZのインスタンスが全損したときの復旧が、クロスゾーン負荷分散が無効のほうが仕組み上わずかに時間がかかる(503エラーになる確率が上がる)ので、それを気にするよりも大規模AZ障害がおきたときにも、手を加えずにビジネス継続したいという考えに持っていければ使えるのかもしれないですね。
どうしても目先の切替時間の秒数に目が行きがちですが。
*** なぜゾーンシフトという機能がでてきたか
ここからは完全にエスパーですが、そもそもなぜクロスゾーン負荷分散を無効化してまでゾーンシフトという機能がでてきたのでしょうか。

私は最近AWSがさかんに言うようになった「静的安定性」の考え方にもとづくものと考えています。
今年のAWS Summit Tokyoでもセッションがありましたが、静的安定性は一言で言えば、
「障害が発生しても、何も設定変更をしなくても良い状態」を指すようです。

https://pages.awscloud.com/rs/112-TZM-766/images/AWS-43_AWS-Summit-2023_Resilience.pdf

クロスゾーン負荷分散は、自身のAZの中でしか分散されないので、
AZ単体で想定される負荷をさばく、十分なキャパシティが求められます。
これはまさに静的安定性の考え方と一致しています。

当然通常運用時は余剰キャパシティが多く、正直無駄なコストにも見えてしまいますが、
そのコストを払ってでもビジネス継続性が求められる超重要なシステムに対して信頼性を提供する機能としてゾーンシフトが出てきたのではないでしょうか。

なので、やはりあらゆるシステムでとりあえず入れておこうというものではなく、
費用と費用対効果と十分そろばんを弾いて、導入する機能なのかなと思います。

* 2024
** DONE EKSでのMountpoint for Amazon S3を試す                             :aws:eks:s3:
CLOSED: [2024-01-08 Mon 13:34]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2024\" \"2024-01\")
:EXPORT_FILE_NAME: 202401081308
:END:

この前のRe:Invent中に発表された Mountpoint for Amazon S3 Container Storage Interfaceは、
EKS向けのCSIでS3をあたかもファイルストレージであるかのようにマウントすることができます。

今回はこれに興味をもったので試してみました。

https://aws.amazon.com/jp/about-aws/whats-new/2023/11/mountpoint-amazon-s3-csi-driver/

*** セットアップ

公式ドキュメントにセットアップ手順があり、これに沿って進めればわりと詰まらずに進めることができるので、細かな手順は省略します。

https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/s3-csi.html

一点だけ。EKSをセットアップするときに、eksctlを使わない場合はIAM OIDCプロバイダの作成の手順を忘れずにやりましょう。
私はこれをうっかりスルーしていて30分ぐらい悩みました。

https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/enable-iam-roles-for-service-accounts.html

*** 導入後の状態を見てみる

#+begin_src plain
$ kubectl get all -n kube-system
NAME                               READY   STATUS    RESTARTS   AGE
pod/aws-node-qqkfd                 2/2     Running   0          20h
pod/coredns-58488c5db-25rdz        1/1     Running   0          21h
pod/coredns-58488c5db-75fd9        1/1     Running   0          21h
pod/eks-pod-identity-agent-bwmhq   1/1     Running   0          20h
pod/kube-proxy-pdbzf               1/1     Running   0          20h
pod/s3-csi-node-mjcmq              3/3     Running   0          2m59s

NAME               TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)         AGE
service/kube-dns   ClusterIP   172.20.0.10   <none>        53/UDP,53/TCP   21h

NAME                                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/aws-node                 1         1         1       1            1           <none>                   21h
daemonset.apps/eks-pod-identity-agent   1         1         1       1            1           <none>                   20h
daemonset.apps/kube-proxy               1         1         1       1            1           <none>                   21h
daemonset.apps/s3-csi-node              1         1         1       1            1           kubernetes.io/os=linux   2m59s

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/coredns   2/2     2            2           21h

NAME                                DESIRED   CURRENT   READY   AGE
replicaset.apps/coredns-58488c5db   2         2         2       21h
#+end_src

=kube-system= 名前空間に、s3-csi-nodeというpodが起動していることが確認できます。

#+begin_src plain
  $ kubectl describe sa -n kube-system s3-csi-driver-sa
Name:                s3-csi-driver-sa
Namespace:           kube-system
Labels:              app.kubernetes.io/component=csi-driver
                     app.kubernetes.io/instance=aws-mountpoint-s3-csi-driver
                     app.kubernetes.io/managed-by=EKS
                     app.kubernetes.io/name=aws-mountpoint-s3-csi-driver
Annotations:         eks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/AmazonEKS_S3_CSI_DriverRole
Image pull secrets:  <none>
Mountable secrets:   <none>
Tokens:              <none>
Events:              <none>
#+end_src

先ほど作成したIAMロールとリンクしたServiceAccountが作成されていることも確認できます。

#+begin_src plain
  $ kubectl get sc
NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
gp2 (default)   kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  21h

#+end_src
特にStorageClassが作成されるわけではないようです。

*** 動作確認

では、S3バケットをマウントするPodを起動してみようと思います。
はじめに以下のようなPV, PVCを用意します。
#+begin_src yaml
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: s3-pv
  spec:
    capacity:
      storage: 1Mi
    accessModes:
      - ReadWriteMany
    mountOptions:
      - allow-delete
      - region us-east-1
    csi:
      driver: s3.csi.aws.com
      volumeHandle: s3-csi-driver-volume
      volumeAttributes:
        bucketName: eks-mount-bucket
  ---
  apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: s3-claim
  spec:
    accessModes:
      - ReadWriteMany
    storageClassName: ""
    resources:
      requests:
        storage: 1Mi
    volumeName: s3-pv
#+end_src

pvc, pvそれぞれにあるストレージサイズの設定は値は設定していますが、実際は無視されるようです。
今回は、1Miバイトで設定しているので、後程大きいファイルを作成して試してみようと思います。

また、PVCの =spec.storageClassName= は空文字が設定されています。これはk8sのバリデーションでひっかかってしまうので設定があること自体が重要ということですね。

これらのyamlを適用すると、pv, pvcが作成されたことが確認できます。
なお、事前の権限設定でミスがあってもPVの作成まではうまくいってしまうようなので、要注意です。

#+begin_src plain
  $ kubectl get pv,pvc -o wide
  NAME                     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REASON   AGE   VOLUMEMODE
  persistentvolume/s3-pv   1Mi        RWX            Retain           Bound    default/s3-claim                           13s   Filesystem

  NAME                             STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE
  persistentvolumeclaim/s3-claim   Bound    s3-pv    1Mi        RWX                           13s   Filesystem
#+end_src

次に作成したPVをマウントするPodをデプロイします。
Pod側は特に通常のPVをマウントするpodとやりかたは変わりません。今回は、適当にAmazonLinuxベースで作成しました。

#+begin_src yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec:
  containers:
  - image: amazonlinux
    name: amazonlinux
    command: ["sh", "-c", "sleep 3600"]
    volumeMounts:
      - name: s3
        mountPath: /data
  volumes:
    - name: s3
      persistentVolumeClaim:
        claimName: s3-claim  
#+end_src

**** Podが起動しない場合は？

ここまでの設定(特に権限周り)でミスがあった場合、このPodを起動する時に初めてAWSのAPIがコールされS3をマウントするため、このPod起動のタイミングでエラーとなります。

Podが起動中のまま進まない場合は、k8sのイベントを見てみましょう。
私もOIDC周りで設定に誤りがあり以下のイベントが出続ける状態でした。

#+begin_src plain
  2s          Warning   FailedMount   pod/pod-1   MountVolume.SetUp failed for volume "s3-pv" : rpc error: code = Internal desc = Could not mount "eks-mount-bucket" at "/var/lib/kubelet/pods/c954d12a-838c-43d5-91e9-54d4ee773aa2/volumes/kubernetes.io~csi/s3-pv/mount": Mount failed: Failed to launch systemd service mount-s3-1.3.1-5bc63583-08b3-4860-9c80-8b663c98cf17.service output: Error: Failed to create S3 client...
#+end_src

エラーが出て動かない場合は、これまでの手順を見直して設定を飛ばしてたり、記載内容に誤りがないか落ち着いて確認しましょう。

**** PodからS3にアクセスする

マウントしたら通常のファイルシステム同様にアクセスすることが可能です。
ファイルの作成をすることもできます。

#+begin_src plain
  $ kubectl exec -it pod-1 -- /bin/bash
  bash-5.2# echo "Hello, world" >> /data/hello.txt
  bash-5.2# exit
  $ aws s3 ls eks-mount-bucket
  2024-01-08 02:56:59         13 hello.txt
  $ aws s3 cp s3://eks-mount-bucket/hello.txt hello.txt
  download: s3://eks-mount-bucket/hello.txt to ./hello.txt 
  $ cat hello.txt 
  Hello, world
  $
#+end_src

作成したファイルは特に時間差を感じることなくS3にアップされていることも確認できました。
また、今回アクセスモードを =ReadWriteMany= で作成していますが、複数のPodで同時にマウントして、他のPodが作成したファイルをシームレスに参照できることも確認できました。

また、上記で書いたとおりPVの設定でストレージサイズは設定するものの、実際には上限がかかるわけではないので、
設定値(今回は1MiBにしていた)以上のファイルも作成することが可能です。

#+begin_src plain
    # dd if=/dev/zero of=/data/ddtest bs=1M count=100 
    100+0 records in
    100+0 records out
    104857600 bytes (105 MB, 100 MiB) copied, 0.902866 s, 116 MB/s
    bash-5.2# ls -l /data/
    total 102401
    -rw-r--r-- 1 root root 104857600 Jan  8 03:52 ddtest
    -rw-r--r-- 1 root root        13 Jan  8 02:56 hello.txt
    $ aws s3 ls eks-mount-bucket
    2024-01-08 03:52:47  104857600 ddtest
    2024-01-08 02:56:59         13 hello.txt
#+end_src

**** 注意点

本来ブロックストレージであるS3をファイルシステムストレージのようにマウントしているので、
ちょっと触ってみても、これできないのか、という操作がいくつかありました。

***** ファイルに追記できない

同時作成の排他制御はどうなるのかテストしてみようと思ったのですが、
そもそもファイルは新規作成のみで追記はできないようです。
以下のように権限エラーが出てしまいます。

#+begin_src plain
  # echo "hello" >> /data/hello.txt 
  bash: /data/hello.txt: Operation not permitted
#+end_src

S3 マウントポイントドライバの説明にも"既存のファイルの上書きは不可、削除して再作成が必要"と書いてありますね。これはユースケースを考える上で注意が必要そうです。

https://github.com/awslabs/mountpoint-s3/blob/main/doc/CONFIGURATION.md#file-modifications-and-deletions

#+begin_quote
You cannot currently use Mountpoint to overwrite existing objects. However, if you use the --allow-delete flag, you can first delete the object and then create it again.
#+end_quote

***** 空のディレクトリ作成は反映されない

AWSコンソールのほうでは空のディレクトリを作成して、それをPodから見ることはできたのですが、
Podの方で空のディレクトリを作ってもS3には反映されていませんでした。
ディレクトリの中にファイルを作成するとすぐに反映されたので、PutObject API的に、ファイルを作成することが重要ということですね。

まあ、実際のケースでこれで困ることは少ないと思います。

**** Mountpoint for S3 CSIの挙動を変えてみる

PVのyamlの中に、 =spec.mountOptions= があります。これが実際のドライバの引数になるようで、ここの記述を変えると挙動を変えることができました。

#+begin_src yaml
  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: s3-pv
  spec:
    capacity:
      storage: 1Mi
    accessModes:
      - ReadWriteMany
    mountOptions:
      - allow-delete # この行を削除
      - region us-east-1
    csi:
      driver: s3.csi.aws.com
      volumeHandle: s3-csi-driver-volume
      volumeAttributes:
        bucketName: grugrut-s3-eks-mount-test
#+end_src

たとえばこのように =allow-delete= を削除すると、以下のようにファイルの作成はできるが削除が許可されないことが確認できました。

#+begin_src plain
# rm /data/ddtest 
rm: cannot remove '/data/ddtest': Operation not permitted
bash-5.2# touch /data/new-file
bash-5.2# ls /data/
ddtest  hello.txt  new-file
bash-5.2#   
#+end_src

他のオプションも使えるのかまでは試してないですが、これ以外のところはuidなどを除けばそこまで制御するものは無さそうです。

https://github.com/awslabs/mountpoint-s3/blob/main/doc/CONFIGURATION.md#file-system-configuration

*** 考察

**** 他のストレージとの違い

これまでは、エフェメラルストレージを除くと、EKSでファイルを永続化したい場合、
AWSのマネージドサービスを利用する場合だと、 =EBS= と =EFS= が選択肢としてありました。

今回、新しくS3が追加されてEFSは今後不要になるのでは、とも思ってたのですが実際に触ってみると使いどころが違いますね。

主観がけっこう入っていますが、それぞれ以下のような違いがあります。

|                                | EBS | EFS | S3 |
|--------------------------------+-----+-----+----|
| 複数Podでのマウント            | ×   | ○   | ○  |
| マルチAZでの利用               | ×   | ○   | ○  |
| ファイルの追記・編集           | ○   | ○   | ×  |
| バックアップ・リストアの利便性 | ×   | ○   | ○  |
| 利用する上での手軽さ           | ○   | ×   | ○  |
| EKS外とのファイル連携          | ×   | ×   | ○  |

これまでだと、基本的にはEBSから考えるもののAZが固定される辛さやリストアの面倒くささから、
EFSをマウントするか、アプリ側でAPIをたたいてS3にアップするか、などを考える必要があったかと思います。

今回、S3が新たに加わったことでファイルのシステム間連携などには良い選択肢になったと思います。
その反面、ファイルの追記はできないのでアプリによっては引き続きEFSを考えるか、というところでしょうか。

**** 想定ユースケース

***** システム間ファイル連携

一番、一般的なユースケースかと思います。

システム同士を疎結合にするために直接やりとりするのではなく、データをファイル連携で渡す際にはS3がよく用いられます。
これを作りこみなくマウントしたフォルダに配置するだけで簡潔するということで、どんなアプリであってもS3が扱いやすくなったと言えるでしょう。

***** 頻繁に更新が入るファイルの外部配置

コンテナに埋めこんでおくと更新のたびにビルドが必要で面倒なのでファイルを外部に配置しておきたい、という場合、
k8sの機能ではConfigMapを利用することができます。
しかし、ConfigMapでは1MBまでしか対応できなかったり大量ファイルだと取り回しが面倒というケースもあります。

これまではそういったケースでEFSを用いて実現することがありましたが、EFSってマネコンからファイルの操作ができないのでリリースや中身の確認が面倒なんですよね。

こういったケースではS3は非常によい選択肢かと思います。

他にも今後多くのユースケースがでてきそうですね。

*** まとめ

今回は、Re:Inventで登場した Mountpoint for Amazon S3 CSIを実際に動かしてみて、
使いどころについてまとめました。

新しい機能がどんどん増えていくので便利なものは活用していきたいですね。

** DONE AWSの新たな資格 Data Engineer Associate に合格した :aws:certification:
CLOSED: [2024-02-14 Wed 23:39]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2024\" \"2024-02\")
:EXPORT_FILE_NAME: 202402142338
:END:

[[*AWSの新しい資格 Data Engineer Associateのベータ版を受験してきた][AWSの新しい資格 Data Engineer Associateのベータ版を受験してきた]]

の通り、昨年の11月にベータ試験が始まった Data Engineer Associateを受験していました。

公表されていたスケジュールでは、試験終了後90日後ということだったので3月入ってからだなとのんびり構えていたのですが、
想定よりもずっと早い2024/2/14に結果が届きました。

結果は **合格** 。嬉しいバレンタインの贈り物でした。

とは言え、試験レポートを見てみると720点で合格のところスコアは722点と超ギリギリ。
分野ごとの結果を見ても、もっとがんばりましょうの分野が多く、Associateということでちょっと甘く見てたかなと反省です。

試験直後の振り返りの方でも触れましたが、ベータ版がゆえに問題がこなれていない、
英語で全部読むところがハンディキャップになるというところも多分にあったと思います。

標準バージョンの試験はもっとやりやすくなってると思いますので、興味がある人は是非受験してみると良いと思います。
公式のサイトを見ても標準版の試験は以前のアナウンスから変わらず、2024年4月からのようです。

#+begin_quote
ベータバージョン後の標準バージョンの試験の登録は 2024 年 3 月に開始され、2024 年 4 月から受験できるようになります。
#+end_quote
https://aws.amazon.com/jp/certification/certified-data-engineer-associate/

今出ている情報からは、標準バージョンでは最初から日本語対応しているのかはわからないですが、もし英語で受ける場合は、
私の反省ポイントのひとつである、母語でない場合の30分試験時間延長の特別措置を申しこんでおくことを強くお勧めします!!

#+begin_quote
英語がネイティブでない受講者が英語で試験を受ける場合は、リクエストに応じて 30 分間の試験の延長が利用できます。特別な配慮の「ESL +30」は、試験の登録前に 1 度のみリクエストする必要があります。
#+end_quote
https://aws.amazon.com/jp/certification/policies/before-testing/

というわけで、世界準速Data Engineer Associate合格体験記でした。
** DONE AWS CloudShellを利用するときのつらみは全部tmuxが解決してくれる :aws:cloudshell:
CLOSED: [2024-02-22 Thu 00:32]
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :archives '(\"2024\" \"2024-02\")
:EXPORT_FILE_NAME: 202402211945
:END:

AWSでちょっとした操作をおこないたい場合、CloudShell上で作業することもあるかと思います。

ちゃんとした運用用途であればガバナンス等の目的のためにCLI実行サーバをEC2インスタンスとして用意するかなどを検討するべきですが、
個人の検証や簡単な作業用途であればAWS CLIがすでに導入されていたり便利ですよね。

ですが、CloudShellも万能ではなく、例えば以下のような点で不便さを感じていました。

- セッションがひとつしか同時に存在できない。複数の作業(ログをtailで流しながらコマンドを実行したい等)をするために複数タブでCloudShellを開こうとしても、ひとつのタブでしか動作せず、他も同じ画面になってしまう
- ちょっとした通信の途切れなどでページのリロードが要求される。リロードすると過去のログをページスクロールして見ることができない

あまりこれまでのCloudShellの活用の記事などでも見たことがなかったのですが、実はCloudShellのセッションはtmux上で動いています。
これによって、上記の悩みは簡単に解決します。

ちなみに、ちゃんと公式ドキュメントにはtmuxが入っていることは書かれています。さらりと書かれすぎててこれまで気付いてませんでした。。。

#+begin_quote
With the shell that's created for AWS CloudShell sessions, you can switch seamlessly between your preferred command line shells. More specifically, you can switch between Bash, PowerShell, and Z shell. You also have access to pre-installed tools and utilities. These include git, make, pip, sudo, tar, *tmux* , vim, wget, and zip.
#+end_quote
https://docs.aws.amazon.com/cloudshell/latest/userguide/cloudshell-features.html

*** tmuxとは
=tmux= (読みはティーマックス)はターミナルマルチプレクサの略であり、通常のターミナルの上で動作しターミナルよりも強化されたユーザ体験を提供してくれます。
例えば、セッションを複数作成して複数ターミナルが動いているように切り替えられたり、セッションを複数のペインに分割したり、ということができます。

https://github.com/tmux/tmux/wiki

人によっては =screen= の方が馴染みがあるかもしれません。かく言う私も「screen使えればこんな悩み解決するんだよなあ……」と思って調べてたら「ってかtmux使えるじゃん！ というかすでにtmux上のセッションじゃん、これ！！」ということで気付きました。

tmuxはプリフィックスキー (デフォルトでは ~Ctrl + b~ 、以下説明では ~<prefix>~ と表記します )に続けてキーを入力することで、tmuxのコマンドを実行することができます。

他にもいろいろな使い道はありますが、今回は私の冒頭の悩みがtmuxでどう解決できるのかを紹介します。

*** 複数画面にして別の作業をしたい

個人的な好みとしては、基本的には別ウィンドウで開いてそれぞれで別の作業をしたいのですが、できないものはしかたがないです。

tmuxではひとつの画面をペインと呼ばれる画面内の独立した区画に区切ることができ、以下の図のように、擬似的に複数ウィンドウを用意することができます。

[[file:images/20240221-cloudshell-1.png]]

CloudShell上で実行できるペイン操作のコマンドの例です。

| 操作内容                               | コマンド           |
| ペインを上下に二分割                   | ~<prefix> \"~      |
| ペインを左右に二分割                   | ~<prefix> %~       |
| 指定した方向のペインをアクティブにする | ~<prefix> <矢印>~  |
| 次のペインをアクティブにする           | ~<prefix> o~       |
| ペインのレイアウトを変更する           | ~<prefix> <Space>~ |
| アクティブなペインを閉じる             | ~<prefix> x~       |
| アクティブなペイン以外をすべて閉じる   | ~<prefix> !~       |

ターミナルソフトで実行している場合は、マウスでペインをクリックすることでアクティブなペインを切り替えたり、
ペインのサイズを ~<prefix> <Ctrl>+<矢印>~ で変更できたりするのですが、
CloudShellはブラウザ上で動いているためか、動作しませんでした。

あと、ちょっと反応がにぶいのは気になります。

とはいえ、ペイン分割できるのは便利ですね。
ペイン分割した際の内容をコピーしたい場合は、マウス操作であればAltキーを押しながらドラッグすることで矩形選択することができます。

*** 過去の表示内容を見たい

CloudShellは、ブラウザで表示されるためブラウザのスクロールで過去のコンソール出力内容も見ることができます。

しかし、CloudShellは通信が途切れた場合や一定時間放置すると、すぐにセッションタイムアウトとなってしまいブラウザのページリロードが要求されます。
ページをリロードすると当然過去の出力内容をスクロールして見ることはできなくなります。

しかし、tmuxは自身の機能として過去のコンソール出力結果を見ることができるので、
ページをリロードした後でも出力を確認することができます。

~<prefix> [~ により、コピーモードに切り替わるのですが、このときに ~<Page Up>~ ~<Page Down~ によってスクロールして過去の出力内容も確認することができます。
コピーモードの終了方法は ~q~ です。

なお、これも本来は ~<Ctrl>+u~ ~<Ctrl>+d~ でもスクロールすることができるはずなのですが、CloudShellでは動作しなかったので注意してください。

また、この機能で過去の出力内容を見ることができるのはあくまでtmuxのセッションが残っている間だけなので、1週間前の作業内容を見たい、と思っても遡れないので過信は禁物です。

また、tmuxの機能としてファイルにターミナル出力ログを保存する機能もあるので、興味がある人は調べてみてください。

*** まとめ

CloudShellでデフォルトで入っているtmuxを使って、ちょっとした不便さを解消する方法についてご紹介しました。
リモートワークが中心になると、ふとしたこういうテクニックが伝承されにくかったりすると思います。

他にも役に立ちそうなTipsがあれば、また紹介していきたいと思います。
